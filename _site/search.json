[
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-Class_Ex06.html",
    "href": "In-class_Ex/In-class_Ex06/In-Class_Ex06.html",
    "title": "In Class Exercise 6",
    "section": "",
    "text": "Spatial autocorrelation: term used to describe the presence of systematic spatial variation in a variable.\nMore negative correlation = more outliers (with a checkboard pattern)\nMoran’s I (z value): Describe how features differ from the values in the study area as a whole\n\nPositive value: Clustered, observations tend to be similar\nNegative value: Dispersed, observations tend to be dissimilar\nApprox. zero: Observations are arranged randomly over space\n\nGeary c (z value): Describe how features different from their immediate neighbours\n\nPositive value: Dispersed, observations tend to be dissimilar\nNegative value: Clustered, observations tend to be similar\nc=1: observations are arranged randomly over space\n\nConfidence interval: value in which represents how confident you are, recommended to be 95%"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-Class_Ex06.html#overview",
    "href": "In-class_Ex/In-class_Ex06/In-Class_Ex06.html#overview",
    "title": "In Class Exercise 6",
    "section": "",
    "text": "Spatial autocorrelation: term used to describe the presence of systematic spatial variation in a variable.\nMore negative correlation = more outliers (with a checkboard pattern)\nMoran’s I (z value): Describe how features differ from the values in the study area as a whole\n\nPositive value: Clustered, observations tend to be similar\nNegative value: Dispersed, observations tend to be dissimilar\nApprox. zero: Observations are arranged randomly over space\n\nGeary c (z value): Describe how features different from their immediate neighbours\n\nPositive value: Dispersed, observations tend to be dissimilar\nNegative value: Clustered, observations tend to be similar\nc=1: observations are arranged randomly over space\n\nConfidence interval: value in which represents how confident you are, recommended to be 95%"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-Class_Ex06.html#installing-packages",
    "href": "In-class_Ex/In-class_Ex06/In-Class_Ex06.html#installing-packages",
    "title": "In Class Exercise 6",
    "section": "6.1 Installing Packages",
    "text": "6.1 Installing Packages\nIn this in class exercise, we will be using the following packages:\n\npacman::p_load(sf, sfdep, tmap, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-Class_Ex06.html#the-data",
    "href": "In-class_Ex/In-class_Ex06/In-Class_Ex06.html#the-data",
    "title": "In Class Exercise 6",
    "section": "6.2 The Data",
    "text": "6.2 The Data\n\n6.2.1 Import shapefile into r environment\nThe code chunk below uses st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/georgiaxng/georgiaxng/is415-handson/In-class_Ex/In-class_Ex06/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n6.2.2 Import csv file into r environment\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package. The output is R dataframe class.\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n6.2.3 Performing relational join\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\nhunan_GDPPC &lt;- left_join(hunan,hunan2012)%&gt;%\n  select(1:4,7,15)\n\n\n\n6.2.4 Plotting Chloropeth Map\n\ntmap_mode(\"plot\")\ntm_shape(hunan_GDPPC) +\n  tm_fill(\"GDPPC\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"GDPPC\") +\n  tm_layout(main.title = \"Distribution of GDP per capita by county, Hunan Province\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-Class_Ex06.html#global-measures-of-spatial-association",
    "href": "In-class_Ex/In-class_Ex06/In-Class_Ex06.html#global-measures-of-spatial-association",
    "title": "In Class Exercise 6",
    "section": "6.3 Global Measures of Spatial Association",
    "text": "6.3 Global Measures of Spatial Association\n\n6.3.1 Deriving contiguity weights: Queen’s method\nNote: nb refers to the neighbours\n\nwm_q &lt;- hunan_GDPPC %&gt;% mutate(nb = st_contiguity(geometry), \n                               wt = st_weights(nb, style = \"W\"), \n                               .before = 1)\n\n\n\n6.3.2 Computing Global Moran’s I\n\nmoranI&lt;- global_moran(wm_q$GDPPC,\n                      wm_q$nb,\n                      wm_q$wt)\nglimpse(moranI)\n\nList of 2\n $ I: num 0.301\n $ K: num 7.64\n\n\n\n\n\n\n\n\nTip\n\n\n\nK refers to average neighbours they have\n\n\n\n\n6.3.3 Performing Global Moran’s I Test\n\nglobal_moran_test(wm_q$GDPPC,\n                      wm_q$nb,\n                      wm_q$wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\n\n6.3.4 Performing Global Moran’s I permutation test\nIn practice, monte carlo simulation should be used to perform the statistical test. For sfdep, it is supported by globel_moran_perm()\nIt is alway a good practice to use set.seed() before performing simulation. This is to ensure that the computation is reproducible.\n\nset.seed(1234)\n\nNext, global_moran_perm() is used to perform Monte Carlo simulation.\n\nglobal_moran_perm(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\nThe statistical report above show that the p-value is smaller than alpha value of 0.05. Hence, we have enough statistical evidence to reject the null hypothesis that the spatial distribution of GPD per capita are resemble random distribution (i.e. independent from spatial). Because the Moran’s I statistics is greater than 0. We can infer that the spatial distribution shows sign of clustering.\n\n\n\n\n\n\nTip\n\n\n\nThe numbers of simulation is alway equal to nsim + 1. This mean in nsim = 99. This mean 100 simulation will be performed."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-Class_Ex06.html#computing-local-morans-i-visualisations",
    "href": "In-class_Ex/In-class_Ex06/In-Class_Ex06.html#computing-local-morans-i-visualisations",
    "title": "In Class Exercise 6",
    "section": "6.4 Computing Local Moran’s I & Visualisations",
    "text": "6.4 Computing Local Moran’s I & Visualisations\nIn this section, we will learn how to compute Local Moran’s I of GDPPC at county level by using local_moran() of sfdep package.\n\nlisa &lt;- wm_q %&gt;% \n  mutate(local_moran = local_moran(\n    GDPPC, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\n\n\n\n\n\nNote\n\n\n\n\nunnest is to put it back to one single tibble table\nTo ensure consistency, stay with one p value (either p_ii_sim or p_folded sim(k4))\nIf skewness is close to 0, use mean, else, use median.\n\nThe output of local_moran() is a sf data.frame containing the columns ii, eii, var_ii, z_ii, p_ii, p_ii_sim, and p_folded_sim.\n\nii: local moran statistic\neii: expectation of local moran statistic; for localmoran_permthe permutation sample means\nvar_ii: variance of local moran statistic; for localmoran_permthe permutation sample standard deviations\nz_ii: standard deviate of local moran statistic; for localmoran_perm based on permutation sample means and standard deviations p_ii: p-value of local moran statistic using pnorm(); for localmoran_perm using standard deviatse based on permutation sample means and standard deviations p_ii_sim: For localmoran_perm(), rank() and punif() of observed statistic rank for [0, 1] p-values using alternative= -p_folded_sim: the simulation folded [0, 0.5] range ranked p-value (based on https://github.com/pysal/esda/blob/4a63e0b5df1e754b17b5f1205b cadcbecc5e061/esda/crand.py#L211-L213)\nskewness: For localmoran_perm, the output of e1071::skewness() for the permutation samples underlying the standard deviates\nkurtosis: For localmoran_perm, the output of e1071::kurtosis() for the permutation samples underlying the standard deviates.\n\n\n\n\n6.4.1 Visualising local Moran’s I\nIn this code chunk below, tmap functions are used prepare a choropleth map by using value in the ii field.\n\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"local Moran's I of GDPPC\",\n            main.title.size = 0.8)\n\n\n\n\n\n\n\n\n\n\n6.4.2 Visualising p-value of local Moran’s I\nIn the code chunk below, tmap functions are used prepare a choropleth map by using value in the p_ii_sim field.\n\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_fill(\"p_ii_sim\") + \n  tm_borders(alpha = 0.5) +\n   tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 0.8)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nFor p-values, the appropriate classification should be 0.001, 0.01, 0.05 and not significant instead of using default classification scheme.\n\n\n\n\n6.4.3 Visualising local Moran’s I and p-value\nFor effective comparison, it will be better for us to plot both maps next to each other as shown below.\n\n\nShow the code\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(lisa) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"local Moran's I of GDPPC\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(lisa) +\n  tm_fill(\"p_ii\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n6.4.4 Visualising LISA map\nLISA map is a categorical map showing outliers and clusters. There are two types of outliers namely: High-Low and Low-High outliers. Likewise, there are two type of clusters namely: High-High and Low-Low cluaters. In fact, LISA map is an interpreted map by combining local Moran’s I of geographical areas and their respective p-values.\nIn lisa sf data.frame, we can find three fields contain the LISA categories. They are mean, median and pysal. In general, classification in mean will be used as shown in the code chunk below.\n\n\nlisa_sig &lt;- lisa  %&gt;%\n  filter(p_ii &lt; 0.05)\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-Class_Ex06.html#hot-spot-and-cold-spot-area-analysis-hcsa",
    "href": "In-class_Ex/In-class_Ex06/In-Class_Ex06.html#hot-spot-and-cold-spot-area-analysis-hcsa",
    "title": "In Class Exercise 6",
    "section": "6.5 Hot Spot and Cold Spot Area Analysis (HCSA)",
    "text": "6.5 Hot Spot and Cold Spot Area Analysis (HCSA)\nHCSA uses spatial weights to identify locations of statistically significant hot spots and cold spots in an spatially weighted attribute that are in proximity to one another based on a calculated distance. The analysis groups features when similar high (hot) or low (cold) values are found in a cluster. The polygon features usually represent administration boundaries or a custom grid structure."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-Class_Ex06.html#computing-local-gi-statistics",
    "href": "In-class_Ex/In-class_Ex06/In-Class_Ex06.html#computing-local-gi-statistics",
    "title": "In Class Exercise 6",
    "section": "6.6 Computing local Gi* statistics",
    "text": "6.6 Computing local Gi* statistics\nAs usual, we will need to derive a spatial weight matrix before we can compute local Gi* statistics. Code chunk below will be used to derive a spatial weight matrix by using sfdep functions and tidyverse approach.\n\nwm_idw &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n\n\n\n\n\n\nNote\n\n\n\nGi* and local Gi* are distance-based spatial statistics. Hence, distance methods instead of contiguity methods should be used to derive the spatial weight matrix.\n\n\nNow, we will compute the local Gi* by using the code chunk below.\n\nHCSA &lt;- wm_idw %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    GDPPC, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\nHCSA\n\nSimple feature collection with 88 features and 18 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n# A tibble: 88 × 19\n   gi_star cluster   e_gi     var_gi std_dev p_value p_sim p_folded_sim skewness\n     &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1  0.0416 Low     0.0114 0.00000641  0.0493 9.61e-1  0.7          0.35    0.875\n 2 -0.333  Low     0.0106 0.00000384 -0.0941 9.25e-1  1            0.5     0.661\n 3  0.281  High    0.0126 0.00000751 -0.151  8.80e-1  0.9          0.45    0.640\n 4  0.411  High    0.0118 0.00000922  0.264  7.92e-1  0.6          0.3     0.853\n 5  0.387  High    0.0115 0.00000956  0.339  7.34e-1  0.62         0.31    1.07 \n 6 -0.368  High    0.0118 0.00000591 -0.583  5.60e-1  0.72         0.36    0.594\n 7  3.56   High    0.0151 0.00000731  2.61   9.01e-3  0.06         0.03    1.09 \n 8  2.52   High    0.0136 0.00000614  1.49   1.35e-1  0.2          0.1     1.12 \n 9  4.56   High    0.0144 0.00000584  3.53   4.17e-4  0.04         0.02    1.23 \n10  1.16   Low     0.0104 0.00000370  1.82   6.86e-2  0.12         0.06    0.416\n# ℹ 78 more rows\n# ℹ 10 more variables: kurtosis &lt;dbl&gt;, nb &lt;nb&gt;, wts &lt;list&gt;, NAME_2 &lt;chr&gt;,\n#   ID_3 &lt;int&gt;, NAME_3 &lt;chr&gt;, ENGTYPE_3 &lt;chr&gt;, County &lt;chr&gt;, GDPPC &lt;dbl&gt;,\n#   geometry &lt;POLYGON [°]&gt;\n\n\n\n6.6.1 Visualising Gi*\n\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8))\n\n\n\n\n\n\n\n\n\n\n6.6.2 Visualising p-value of HCSA\n\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_fill(\"p_sim\") + \n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n6.6.3 Visualising local HCSA\nFor effective comparison, you can plot both maps next to each other as shown below.\n\n\nShow the code\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(HCSA) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"Gi* of GDPPC\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(HCSA) +\n  tm_fill(\"p_value\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of Gi*\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-Class_Ex06.html#visualising-hot-spot-and-cold-spot-areas",
    "href": "In-class_Ex/In-class_Ex06/In-Class_Ex06.html#visualising-hot-spot-and-cold-spot-areas",
    "title": "In Class Exercise 6",
    "section": "7 Visualising hot spot and cold spot areas",
    "text": "7 Visualising hot spot and cold spot areas\nNow, we are ready to plot the significant (i.e. p-values less than 0.05) hot spot and cold spot areas by using appropriate tmap functions as shown below.\n\nHCSA_sig &lt;- HCSA  %&gt;%\n  filter(p_sim &lt; 0.05)\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(HCSA_sig) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.4)\n\n\n\n\n\n\n\n\nFigure above reveals that there is one hot spot area and two cold spot areas. Interestingly, the hot spot areas coincide with the High-high cluster identifies by using local Moran’s I method in the earlier sub-section."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html",
    "title": "In Class Exercise 2",
    "section": "",
    "text": "In this exercise, we learn the various practices of importing data,\nBefore we start the exercise, we will need to import necessary R packages first. We will use the following packages sf and tidyverse.\n\npacman::p_load(sf,tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#overview",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#overview",
    "title": "In Class Exercise 2",
    "section": "",
    "text": "In this exercise, we learn the various practices of importing data,\nBefore we start the exercise, we will need to import necessary R packages first. We will use the following packages sf and tidyverse.\n\npacman::p_load(sf,tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#importing-data",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#importing-data",
    "title": "In Class Exercise 2",
    "section": "2.1 Importing data",
    "text": "2.1 Importing data\n\n2.1.1 Dataset\nWe will be using the below datasets for this exercise.\n\nMaster Plan 2014 Subzone Boundary (Web) from data.gov.sg\nMaster Plan 2019 Subzone Boundary (Web) from data.gov.sg\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2023 from singstat.gov.sg\n\n\n\n2.1.2 Master Plan 2014 Subzone Boundary\nThis code chunk imports in shapefile.\n\nmpsz14_shp &lt;- st_read(dsn = \"data/MPSZ2014/MasterPlan2014SubzoneBoundaryWebSHP/\", \n                  layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/georgiaxng/georgiaxng/is415-handson/In-class_Ex/In-class_Ex02/data/MPSZ2014/MasterPlan2014SubzoneBoundaryWebSHP' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nConverting the Master Plan 2014 Subzone Boundary shapefile to a kml file.\n\n#! output: false\nmpsz14_kml = st_write(mpsz14_shp,\"data/MPSZ2014/MasterPlan2014SubzoneBoundary_WEB_PL.kml\",delete_dsn = TRUE)\n\nDeleting source `data/MPSZ2014/MasterPlan2014SubzoneBoundary_WEB_PL.kml' using driver `KML'\nWriting layer `MasterPlan2014SubzoneBoundary_WEB_PL' to data source \n  `data/MPSZ2014/MasterPlan2014SubzoneBoundary_WEB_PL.kml' using driver `KML'\nWriting 323 features with 15 fields and geometry type Multi Polygon.\n\n\n\n\n2.1.3 Master Plan 2019 Subzone Boundary\nThe below chunk of code is used to import Master Plan 2019 shapefile and also project it to the 3414 crs system:\n\nmpsz19_shp &lt;- st_read(dsn = \"data/MPSZ2019\", \n                  layer = \"MPSZ-2019\") %&gt;%\nst_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `/Users/georgiaxng/georgiaxng/is415-handson/In-class_Ex/In-class_Ex02/data/MPSZ2019' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\nRefer to https://epsg.io/ for the crs code when you need to reproject, if the coordinates are in geographic coordinate system, it may be necessary to convert it to the projected coordinate system and vice versa. It will depend on the usecase so it is important to check it.\nst_crs() can be used to check the crs currently used. eg. st_crs(mpsz19_shp)\n\nImporting Master Plan 2019 in kml format:\n\nmpsz19_kml &lt;- st_read(\"data/MPSZ2019/MasterPlan2019SubzoneBoundaryNoSeaKML.kml\")\n\nReading layer `URA_MP19_SUBZONE_NO_SEA_PL' from data source \n  `/Users/georgiaxng/georgiaxng/is415-handson/In-class_Ex/In-class_Ex02/data/MPSZ2019/MasterPlan2019SubzoneBoundaryNoSeaKML.kml' \n  using driver `KML'\nSimple feature collection with 332 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY, XYZ\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\n\n2.1.4 Population Data\nThe below code imports the population data.\n\npopdata &lt;- read_csv(\"data/respopagesextod2023/respopagesextod2023.csv\")\n\nAggregating the data and grouping them by area, subzone and age group.\n\npopdata2023 &lt;- popdata %&gt;%\n  group_by(PA,SZ,AG) %&gt;%\n  summarize(`POP`=sum(`Pop`))%&gt;%\n  ungroup()%&gt;%\n  pivot_wider(names_from = AG,\n              values_from = POP)\ncolnames(popdata2023)\n\n [1] \"PA\"          \"SZ\"          \"0_to_4\"      \"10_to_14\"    \"15_to_19\"   \n [6] \"20_to_24\"    \"25_to_29\"    \"30_to_34\"    \"35_to_39\"    \"40_to_44\"   \n[11] \"45_to_49\"    \"50_to_54\"    \"55_to_59\"    \"5_to_9\"      \"60_to_64\"   \n[16] \"65_to_69\"    \"70_to_74\"    \"75_to_79\"    \"80_to_84\"    \"85_to_89\"   \n[21] \"90_and_Over\"\n\n\n\npopdata2023 &lt;- popdata2023 %&gt;%\nmutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[14])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:13])+\nrowSums(.[15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#joining-popdata2023-and-mpsz19_shp",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#joining-popdata2023-and-mpsz19_shp",
    "title": "In Class Exercise 2",
    "section": "2.2 Joining popdata2023 and mpsz19_shp",
    "text": "2.2 Joining popdata2023 and mpsz19_shp\n\npopdata2023 &lt;- popdata2023 %&gt;% mutate_at(.vars = vars(PA,SZ), .funs = list(toupper))\n\n\ntoupper is used to convert all text to uppercases so that the data is uniform for comparison, filtering, or joining with other datasets.\n\n\nmpsz_pop2023 &lt;- left_join(mpsz19_shp, popdata2023, by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\npopdata2023_mpsz &lt;- left_join(popdata2023, mpsz19_shp, by = c(\"SZ\" = \"SUBZONE_N\"))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#installing-the-required-packages",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#installing-the-required-packages",
    "title": "In-class Exercise 3",
    "section": "3.2 Installing the Required Packages",
    "text": "3.2 Installing the Required Packages\nSince maptools is retired and binary is removed from CRAN, we will be downloading it from the posit public package manager snapshots.\n\nNote: It is important to add eval:false in the code chunk as shown below after installation is complete to avoid it being executed every time the quarto document is being rendered.\n\ninstall.packages(\"maptools\",repos = \"https://packagemanager.posit.co/cran/2023-10-13\")\n\npacman::p_load(sf,tmap,tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#the-data",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#the-data",
    "title": "In-class Exercise 3",
    "section": "3.3 The Data",
    "text": "3.3 The Data\n\nmpsz_sf &lt;- st_read(dsn=\"data/MasterPlan2014SubzoneBoundaryWebSHP/\", \n                   layer=\"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/georgiaxng/georgiaxng/is415-handson/In-class_Ex/In-class_Ex03/data/MasterPlan2014SubzoneBoundaryWebSHP' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nThe code chunk below, st_union() is used to derive the coastal outline sf tibble data.frame.\n\nsg_sf &lt;-mpsz_sf %&gt;%\n  st_union()\n\n\nplot(sg_sf)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#viewing",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#viewing",
    "title": "In-class Exercise 3",
    "section": "3.4 Viewing",
    "text": "3.4 Viewing\nThe below chunk of code imports the ACLED Myanmar data, converts it into a spatial format, changes the coordinate system, and formats the event dates into a standard date format.\n\nacled_sf &lt;- read_csv(\"data/ACLED_Myanmar.csv\") %&gt;% \n  st_as_sf(coords = c(\n    \"longitude\", \"latitude\"), crs = 4326) %&gt;% \n  st_transform(crs= 32647)%&gt;%\n  mutate(event_date = dmy(event_date))\n\nThis code produces an interactive map displaying dots for events in 2023 or classified as “Political violence.\n\ntmap_mode('view')\nacled_sf %&gt;%\n  filter(year == 2023 |\n           event_type == \"Political violence\") %&gt;%\n  tm_shape()+\n  tm_dots()\ntmap_mode('plot')"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "Drug abuse is associated with significant negative health, financial and social consequences. Yet, illicit drug consumption remains highly prevalent and continues to be a growing problem worldwide. In 2021, 1 in 17 people aged 15–64 in the world had used a drug in the past 12 months. Notwithstanding population growth, the estimated number of drug users grew from 240 million in 2011 to 296 million in 2021.\nThe geopolitics of Thailand which is near the Golden Triangle of Indochina, the largest drug production site in Asia, and the constant transportation infrastructure development made Thailand became market and transit routes for drug trafficking to the third countries.\nIn Thailand, drug abuse is one of the major social issue. There are about 2.7 million youths using drugs in Thailand. Among youths aged between 15 and 19 years, there are about 300,000 who have needs for drug treatment. Most of Thai youths involved with drugs are vocational-school students, which nearly doubles in number compared to secondary-school students.\nIn particular, we will only be focusing on drug use cases since my analysis aims to provide insights into the spatial distribution and trends of drug abuse across Thailand.\n\n\n\nIn this exercise, we are going to achieve the following tasks:\n\nUsing appropriate function of sf and tidyverse, preparing the following geospatial data layer:\n\na study area layer in sf polygon features. It must be at province level (including Bangkok) of Thailand.\na drug abuse indicators layer within the study area in sf polygon features.\n\nUsing the extracted data, perform global spatial autocorrelation analysis by using sfdep methods.\nUsing the extracted data, perform local spatial autocorrelation analysis by using sfdep methods.\nDescribe the spatial patterns revealed by the analysis above.\n\n\n\n\nBefore we start off, we will have to import the necessary packages required for us to conduct our analysis.\nWe will be using the following packages:\n\nsf: Manages spatial vector data, enabling operations like spatial joins, buffering, and transformations for points, lines, and polygons.\ntmap: Creates and customizes thematic maps for spatial data visualization, including static and interactive maps with various map elements.\ntidyverse: A suite of packages for data manipulation (dplyr), visualization (ggplot2), and tidying (tidyr), facilitating a streamlined workflow for data analysis.\nggplot2: A powerful data visualization package within the tidyverse, allowing the creation of complex and customizable graphs. It supports a wide range of geospatial plotting when combined with sf objects, enabling seamless integration of spatial data in layered plots like points, polygons, and lines.\nsfdep: Facilitates spatial dependence analysis by providing tools to compute spatial weights, autocorrelation statistics, and other spatial econometric measures. It integrates with the sf package, allowing users to easily conduct spatial analysis on spatial data frames.\nRColorBrewer: Provides a collection of color palettes specifically designed for data visualization.\n\n\npacman::p_load(sf, tmap, tidyverse, sfdep, ggplot2,RColorBrewer)\n\n\n\n\nFor the purpose of this take-home exercise, two data sets shall be used, they are:\n\nThailand Drug Offenses [2017-2022] at Kaggle.\nThailand - Subnational Administrative Boundaries at HDX."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#introduction",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#introduction",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "Drug abuse is associated with significant negative health, financial and social consequences. Yet, illicit drug consumption remains highly prevalent and continues to be a growing problem worldwide. In 2021, 1 in 17 people aged 15–64 in the world had used a drug in the past 12 months. Notwithstanding population growth, the estimated number of drug users grew from 240 million in 2011 to 296 million in 2021.\nThe geopolitics of Thailand which is near the Golden Triangle of Indochina, the largest drug production site in Asia, and the constant transportation infrastructure development made Thailand became market and transit routes for drug trafficking to the third countries.\nIn Thailand, drug abuse is one of the major social issue. There are about 2.7 million youths using drugs in Thailand. Among youths aged between 15 and 19 years, there are about 300,000 who have needs for drug treatment. Most of Thai youths involved with drugs are vocational-school students, which nearly doubles in number compared to secondary-school students.\nIn particular, we will only be focusing on drug use cases since my analysis aims to provide insights into the spatial distribution and trends of drug abuse across Thailand."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#goal",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#goal",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "In this exercise, we are going to achieve the following tasks:\n\nUsing appropriate function of sf and tidyverse, preparing the following geospatial data layer:\n\na study area layer in sf polygon features. It must be at province level (including Bangkok) of Thailand.\na drug abuse indicators layer within the study area in sf polygon features.\n\nUsing the extracted data, perform global spatial autocorrelation analysis by using sfdep methods.\nUsing the extracted data, perform local spatial autocorrelation analysis by using sfdep methods.\nDescribe the spatial patterns revealed by the analysis above."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#importing-of-packages",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#importing-of-packages",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "Before we start off, we will have to import the necessary packages required for us to conduct our analysis.\nWe will be using the following packages:\n\nsf: Manages spatial vector data, enabling operations like spatial joins, buffering, and transformations for points, lines, and polygons.\ntmap: Creates and customizes thematic maps for spatial data visualization, including static and interactive maps with various map elements.\ntidyverse: A suite of packages for data manipulation (dplyr), visualization (ggplot2), and tidying (tidyr), facilitating a streamlined workflow for data analysis.\nggplot2: A powerful data visualization package within the tidyverse, allowing the creation of complex and customizable graphs. It supports a wide range of geospatial plotting when combined with sf objects, enabling seamless integration of spatial data in layered plots like points, polygons, and lines.\nsfdep: Facilitates spatial dependence analysis by providing tools to compute spatial weights, autocorrelation statistics, and other spatial econometric measures. It integrates with the sf package, allowing users to easily conduct spatial analysis on spatial data frames.\nRColorBrewer: Provides a collection of color palettes specifically designed for data visualization.\n\n\npacman::p_load(sf, tmap, tidyverse, sfdep, ggplot2,RColorBrewer)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#dataset",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#dataset",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "For the purpose of this take-home exercise, two data sets shall be used, they are:\n\nThailand Drug Offenses [2017-2022] at Kaggle.\nThailand - Subnational Administrative Boundaries at HDX."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#importing-thailand-drug-offenses",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#importing-thailand-drug-offenses",
    "title": "Take Home Exercise 2",
    "section": "2.1 Importing Thailand Drug Offenses",
    "text": "2.1 Importing Thailand Drug Offenses\nThis line of code imports the Thailand Drug Offenses dataset from Kaggle while using the select() function to remove the province_th column which is not useful to us. For this dataset, we have a total of 7392 rows.\n\ndrug_offenses_sf &lt;- read_csv(\"data/aspatial/drug_offence/thai_drug_offenses_2017_2022.csv\") %&gt;% select(-province_th)\n\n\nsummary(drug_offenses_sf)\n\n  fiscal_year   types_of_drug_offenses    no_cases       province_en       \n Min.   :2017   Length:7392            Min.   :    0.0   Length:7392       \n 1st Qu.:2018   Class :character       1st Qu.:    1.0   Class :character  \n Median :2020   Mode  :character       Median :   70.0   Mode  :character  \n Mean   :2020                          Mean   :  535.3                     \n 3rd Qu.:2021                          3rd Qu.:  623.0                     \n Max.   :2022                          Max.   :17131.0"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#importing-thailand---subnational-administrative-boundaries",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#importing-thailand---subnational-administrative-boundaries",
    "title": "Take Home Exercise 2",
    "section": "2.2 Importing Thailand - Subnational Administrative Boundaries",
    "text": "2.2 Importing Thailand - Subnational Administrative Boundaries\nThis line of code imports the Thailand - Subnational Administrative Boundaries dataset using st_read(). As shown, there is a total of 77 geographic features, representing the 76 provinces in Thailand as well as one additional special administrative area (Bangkok).\n\nthailand_province_sf &lt;- st_read(dsn=\"data/geospatial/tha_adm_rtsd_itos_20210121_shp/\", \n                   layer=\"tha_admbnda_adm1_rtsd_20220121\") %&gt;%\n  select(ADM1_EN, geometry)  \n\nReading layer `tha_admbnda_adm1_rtsd_20220121' from data source \n  `/Users/georgiaxng/georgiaxng/is415-handson/Take-home_Ex/Take-home_Ex02/data/geospatial/tha_adm_rtsd_itos_20210121_shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n\n\nThis line of code checks if all the polygons are valid.\n\nst_is_valid(thailand_province_sf)\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[31] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[46] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[61] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[76] TRUE TRUE\n\n\nThis line of code displays the study area.\n\ntm_shape(thailand_province_sf) +\n  tm_polygons()"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data-cleansing-extraction",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data-cleansing-extraction",
    "title": "Take Home Exercise 2",
    "section": "2.3 Data Cleansing & Extraction",
    "text": "2.3 Data Cleansing & Extraction\n\n2.3.1 Extracting Data\nTo see the different types of drug offenses currently in the dataset, we can use unique(). From this we can see that there a total of 16 types of different offenses.\n\nunique(drug_offenses_sf$types_of_drug_offenses)\n\n [1] \"drug_use_cases\"                                        \n [2] \"suspects_in_drug_use_cases\"                            \n [3] \"possession_cases\"                                      \n [4] \"suspects_in_possession_cases\"                          \n [5] \"possession_with_intent_to_distribute_cases\"            \n [6] \"suspects_in_possession_with_intent_to_distribute_cases\"\n [7] \"trafficking_cases\"                                     \n [8] \"suspects_in_trafficking_cases\"                         \n [9] \"production_cases\"                                      \n[10] \"suspects_in_production_cases\"                          \n[11] \"import_cases\"                                          \n[12] \"suspects_in_import_cases\"                              \n[13] \"export_cases\"                                          \n[14] \"suspects_in_export_cases\"                              \n[15] \"conspiracy_cases\"                                      \n[16] \"suspects_in_conspiracy_cases\"                          \n\n\nFor this study, we will only be focusing on drug use cases which we will use filter() to do so, limiting the scope of this project to only that.\n\ndrug_use_sf &lt;-drug_offenses_sf%&gt;%filter(types_of_drug_offenses==\"drug_use_cases\")\nsummary(drug_use_sf)\n\n  fiscal_year   types_of_drug_offenses    no_cases       province_en       \n Min.   :2017   Length:462             Min.   :   32.0   Length:462        \n 1st Qu.:2018   Class :character       1st Qu.:  798.2   Class :character  \n Median :2020   Mode  :character       Median : 1403.5   Mode  :character  \n Mean   :2020                          Mean   : 1981.7                     \n 3rd Qu.:2021                          3rd Qu.: 2440.2                     \n Max.   :2022                          Max.   :16480.0                     \n\n\n\n\n2.3.2 Performing relational join\n\n2.3.2.1 Failed Attempt\nTo fix and standardize the data for consistent matching, we will use toupper() to ensure the columns ADM1_EN of thailand_province_sf and province_en of drug_use_sf can match correctly.\n\ndrug_use_sf$province_en &lt;- toupper(drug_use_sf$province_en)\nthailand_province_sf$ADM1_EN &lt;- toupper(thailand_province_sf$ADM1_EN)\n\nAnd to ensure that the changes are done correctly, we can use head() for this. As seen here, the province names are all now in upper cases.\n\ndrug_use_sfthailand_province_sf\n\n\n\nhead(drug_use_sf)\n\n# A tibble: 6 × 4\n  fiscal_year types_of_drug_offenses no_cases province_en             \n        &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt; &lt;chr&gt;                   \n1        2017 drug_use_cases            11871 BANGKOK                 \n2        2017 drug_use_cases              200 CHAI NAT                \n3        2017 drug_use_cases              553 NONTHABURI              \n4        2017 drug_use_cases              450 PATHUM THANI            \n5        2017 drug_use_cases              378 PHRA NAKHON SI AYUTTHAYA\n6        2017 drug_use_cases              727 LOBURI                  \n\n\n\n\n\nhead(thailand_province_sf)\n\nSimple feature collection with 6 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 100.1913 ymin: 13.47842 xmax: 100.9639 ymax: 14.80246\nGeodetic CRS:  WGS 84\n                   ADM1_EN                       geometry\n1                  BANGKOK MULTIPOLYGON (((100.6139 13...\n2             SAMUT PRAKAN MULTIPOLYGON (((100.7306 13...\n3               NONTHABURI MULTIPOLYGON (((100.3415 14...\n4             PATHUM THANI MULTIPOLYGON (((100.8916 14...\n5 PHRA NAKHON SI AYUTTHAYA MULTIPOLYGON (((100.5131 14...\n6                ANG THONG MULTIPOLYGON (((100.3332 14...\n\n\n\n\n\nThe code chunk below will be used to update the attribute table of drug_use_sf and thailand_province_sf dataframe. This is performed by using left_join() of dplyr package.\n\ncombined_drug_use_sf &lt;- left_join(thailand_province_sf, drug_use_sf,\n                          by =  c(\"ADM1_EN\" = \"province_en\"))\n\nAfter performing the left_join() between thailand_province_sf and drug_use_sf, we can usesummary() to examine the resulting dataset. The output indicated that the combined dataset contains only 452 rows as compared to the original 462 rows for drug_use_sf.\nThis suggests that there were rows in drug_use_sf that did not successfully join with thailand_province_sf.\n\nsummary(combined_drug_use_sf)\n\n   ADM1_EN           fiscal_year   types_of_drug_offenses    no_cases      \n Length:452         Min.   :2017   Length:452             Min.   :   32.0  \n Class :character   1st Qu.:2018   Class :character       1st Qu.:  788.8  \n Mode  :character   Median :2020   Mode  :character       Median : 1399.5  \n                    Mean   :2020                          Mean   : 1994.2  \n                    3rd Qu.:2021                          3rd Qu.: 2444.5  \n                    Max.   :2022                          Max.   :16480.0  \n                    NA's   :2                             NA's   :2        \n          geometry  \n MULTIPOLYGON :452  \n epsg:4326    :  0  \n +proj=long...:  0  \n                    \n                    \n                    \n                    \n\n\n\n\n2.3.2.2 Fixing the Mismatch in Province Names\nThe most likely reasons for this could be a mismatch in certain Province Names, hence to effectively identified the province names that do not match between the two datasets, we can utilize anti_join() to do so, finding the rows in each that did not manage to join in each dataset. This provides a straightforward comparison of the discrepancies between the two datasets.\n\n# Get rows in drug_use_sf that didn't match with thailand_province_sf\nunmatched_in_drug_use &lt;- anti_join(drug_use_sf, thailand_province_sf, \n                                        by = c(\"province_en\" = \"ADM1_EN\"))\n\n# Get rows in thailand_province_sf that didn't match with drug_offenses_sf\nunmatched_in_thailand_province &lt;- anti_join(thailand_province_sf, drug_use_sf, \n                                            by = c(\"ADM1_EN\" = \"province_en\"))\n\n# Display mismatched values\nunmatched_province_en &lt;- unique(unmatched_in_drug_use$province_en)\nunmatched_adm1_en &lt;- unique(unmatched_in_thailand_province$ADM1_EN)\n\n# View the mismatched sets\nlist(\"Unmatched in drug_use_sf\" = unmatched_province_en,\n     \"Unmatched in thailand_province_sf\" = unmatched_adm1_en)\n\n$`Unmatched in drug_use_sf`\n[1] \"LOBURI\"  \"BUOGKAN\"\n\n$`Unmatched in thailand_province_sf`\n[1] \"LOP BURI\"  \"BUENG KAN\"\n\n\nAs shown, there is a mismatch in the naming of the two provinces in each dataset although they are referring to the same provinces, hence we will need to standardise that. Since “Lopburi” and “Bueng Kan” were the names used in wikipedia, we will just be sticking with that.\nThis code performs standardization of province names in two datasets: drug_offenses_sf and thailand_province_sf, using recode() to change specific values.\n\n# Update the values in the province_en column\ndrug_use_sf &lt;- drug_use_sf %&gt;%\n  mutate(province_en = recode(province_en,\n                               \"LOBURI\" = \"LOPBURI\",\n                               \"BUOGKAN\" = \"BUENG KAN\"))\n\n# Update the values in the ADM1_EN column\nthailand_province_sf &lt;- thailand_province_sf %&gt;%\n  mutate(ADM1_EN = recode(ADM1_EN,\n                          \"LOP BURI\" = \"LOPBURI\",\n                          \"BUENG KAN\" = \"BUENG KAN\"))\n\n\n\n2.3.2.3 Reattempt At Performing Relational Join\nFinally, let us try to perform the relational join again. As seen, we have a total of 462 rows which aligns with the total number of rows that is in drug_offenses_sf and for us to easier refer to it, we will rename the column to province.\n\ncombined_drug_use_sf &lt;- left_join(thailand_province_sf, drug_use_sf,\n                          by =  c(\"ADM1_EN\" = \"province_en\")) %&gt;%\n  rename(province = ADM1_EN)\n\nsummary(combined_drug_use_sf)\n\n   province          fiscal_year   types_of_drug_offenses    no_cases      \n Length:462         Min.   :2017   Length:462             Min.   :   32.0  \n Class :character   1st Qu.:2018   Class :character       1st Qu.:  798.2  \n Mode  :character   Median :2020   Mode  :character       Median : 1403.5  \n                    Mean   :2020                          Mean   : 1981.7  \n                    3rd Qu.:2021                          3rd Qu.: 2440.2  \n                    Max.   :2022                          Max.   :16480.0  \n          geometry  \n MULTIPOLYGON :462  \n epsg:4326    :  0  \n +proj=long...:  0"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#drug-use-cases-visualisation",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#drug-use-cases-visualisation",
    "title": "Take Home Exercise 2",
    "section": "2.3 Drug Use Cases Visualisation",
    "text": "2.3 Drug Use Cases Visualisation\n\n# Step 1: Aggregate data by year, summing the number of cases\ndrug_use_by_year &lt;- combined_drug_use_sf %&gt;% select(fiscal_year, no_cases) %&gt;%\n  group_by(fiscal_year) %&gt;%\n  summarise(total_cases = sum(no_cases), .groups = 'drop')\n\nwrite_rds(drug_use_by_year, \"data/rds/drug_use_by_year.rds\")\n\n\n# Create the bar plot with exact labels\nggplot(drug_use_by_year, aes(x = factor(fiscal_year), y = total_cases)) +  # Convert fiscal_year to factor\n  geom_col(fill = \"red\", color = \"black\") +  # Use geom_col for the bar plot\n  geom_text(aes(label = total_cases), vjust = -0.5, size = 5) +  # Add text labels on top of bars\n  labs(title = \"Total Drug Use Cases by Year\", x = \"Year\", y = \"Total Cases\") +\n  theme_minimal() +\n  scale_x_discrete(drop = FALSE) + # Ensures all years are shown on the x-axis\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can see that 2021 has the most drug use cases recorded out of all these years."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#geographic-distribution-of-drug-use-cases-by-province-and-by-year",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#geographic-distribution-of-drug-use-cases-by-province-and-by-year",
    "title": "Take Home Exercise 2",
    "section": "2.4 Geographic Distribution of Drug Use Cases by Province and by Year",
    "text": "2.4 Geographic Distribution of Drug Use Cases by Province and by Year\nUsing the tmap methods below, we can create chloropeth maps to visualise the geographic distribution of the drug use cases by province and by year.\n\n# Set tmap mode to view or plot\ntmap_mode(\"plot\")\n\ntm_shape(combined_drug_use_sf) +\n  tm_polygons(\"no_cases\", title = \"Drug Use Cases\", style = \"quantile\", palette = \"Reds\", n=5) +\n  tm_facets(by = \"fiscal_year\", nrow = 2, ncol = 3) +  # Adjust rows and columns as needed\n    tm_layout(\n    main.title = \"Geographic Distribution of Drug Use Cases by Province and by Year\",  # Main title for the entire plot\n    legend.position = c(\"left\", \"top\"),\n    legend.text.size = 1.2,\n    main.title.size = 1.5,  \n    main.title.position = \"center\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can observe that the northern and eastern regions take up the bulk of the numbers at different time periods."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#computing-contiguity-spatial-weights-queen",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#computing-contiguity-spatial-weights-queen",
    "title": "Take Home Exercise 2",
    "section": "3.1 Computing Contiguity Spatial Weights (QUEEN)",
    "text": "3.1 Computing Contiguity Spatial Weights (QUEEN)\nBefore calculating global spatial autocorrelation statistics, we first need to construct spatial weights for the study area. These weights define the neighborhood relationships between the geographical units (such as counties) within the study region. To do so, we will use st_conguity() of the sfdep package, the equivalent of poly2nb() function from the spdep package. This function creates a list of neighboring regions based on shared boundaries and is needed for when we conduct the subsequent tests.\nIn particular, we are using the Queen contiguity criterion, which defines neighbors as regions that share either a boundary or a vertex. This method is often preferred for creating spatial weights because it includes more neighboring units than the Rook criterion (which only considers shared boundaries).\nThe code chunk below is used to compute Queen contiguity weight matrix.\n\nnb_list &lt;- st_contiguity(thailand_province_sf, queen = TRUE)\n\nwrite_rds(nb_list, \"data/rds/nb_list.rds\")\nsummary(nb_list)\n\n\n\nNeighbour list object:\nNumber of regions: 77 \nNumber of nonzero links: 352 \nPercentage nonzero weights: 5.93692 \nAverage number of links: 4.571429 \n1 region with no links:\n67\n2 disjoint connected subgraphs\nLink number distribution:\n\n 0  1  2  3  4  5  6  7  8  9 \n 1  1  5 17 15 17 10  5  4  2 \n1 least connected region:\n14 with 1 link\n2 most connected regions:\n29 51 with 9 links\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe most connected area has 9 neighbours. There are two area units with only one neighbours.\n\nThe summary report above shows that there are 77 area units in Thailand.\nThe least connected province is Trat, with only 1 neighbour.\nThe most connected provinces on the other hand, are Khon Kaen and Tak, with 9 neighbours.\nA province with no neighbour is Phuket.\n\n\n\nFor reference on where these provinces of interest are.\n\n\nClick to view the code\n# Define the regions and corresponding colors\nhighlight_info &lt;- c(\n  \"TRAT\" = \"yellow\",     \n  \"KHON KAEN\" = \"green\",\n  \"TAK\" = \"blue\",\n  \"PHUKET\" = \"red\"\n)\n\n# Set tmap mode to plot\ntmap_mode(\"plot\")  # Use \"plot\" for static maps\n\n# Create the base map with all provinces\nbase_map &lt;- tm_shape(thailand_province_sf) +\n  tm_polygons(col = \"lightgray\", title = \"Province Highlight\")\n\n# Initialize combined map with the base map\ncombined_map &lt;- base_map\n\n# Loop through the highlight_info to add each region\nfor (region in names(highlight_info)) {\n  combined_map &lt;- combined_map + \n    tm_shape(thailand_province_sf[thailand_province_sf$ADM1_EN == region, ]) +\n    tm_polygons(col = highlight_info[region], \n                 alpha = 0.7)  # No title needed here\n}\n\n# Manually add a legend using tm_add_legend\nlegend_elements &lt;- lapply(names(highlight_info), function(region) {\n  list(label = region, col = highlight_info[region])\n})\n\n# Add layout options and custom legend\ncombined_map +\n  tm_add_legend(type = \"fill\", \n                labels = names(highlight_info), \n                col = highlight_info, \n                title = \"Highlighted Provinces\") +\n  tm_layout(\n    title = \"Highlighted Provinces in Thailand\", \n    legend.position = c(\"right\", \"bottom\"),\n    title.position = c(\"center\", \"top\"),\n    legend.title.size = 1.2,  # Adjust legend title size\n    legend.text.size = 1      # Adjust legend text size\n  )\n\n\n\n\n\n\n\n\n\nTo further investigate why Phuket has no neighboring provinces, we can examine the zoomed-in map of the region. The map clearly illustrates that Phuket is an island, situated away from the mainland. This geographical separation accounts for its lack of direct neighboring provinces.\n\n\nClick to view the code\n# Filter the dataset for Phuket province\nphuket_sf &lt;- thailand_province_sf %&gt;% \n  filter(ADM1_EN == \"PHUKET\")\n\n# Get bounding box for Phuket and slightly expand it to include surrounding provinces\nbbox_phuket &lt;- st_bbox(phuket_sf)\n\n# Expand the bounding box by a certain factor (e.g., 0.1 degrees in each direction)\nbbox_phuket_expanded &lt;- bbox_phuket\nbbox_phuket_expanded[1] &lt;- bbox_phuket[1] - 0.1  # xmin\nbbox_phuket_expanded[3] &lt;- bbox_phuket[3] + 0.1  # xmax\nbbox_phuket_expanded[2] &lt;- bbox_phuket[2] + 0.1  # ymin (correcting direction)\nbbox_phuket_expanded[4] &lt;- bbox_phuket[4] + 0.1  # ymax\n\n# Create a temporary dataset for plotting\nphuket_plot_data &lt;- thailand_province_sf %&gt;%\n  mutate(is_phuket = ifelse(ADM1_EN == \"PHUKET\", \"Phuket\", \"Other Provinces\"))\n\n# Set the color palette explicitly for Phuket and other provinces\ncolor_palette &lt;- c(\"Phuket\" = \"red\", \"Other Provinces\" = \"gray85\")\n\n# Plot using tmap and focus on the Phuket region and its surroundings\ntmap_mode(\"plot\")  # Set tmap to static plotting mode\n\ntm_shape(phuket_plot_data, bbox = bbox_phuket_expanded) +\n  tm_borders() +        # Add borders of your spatial data\n  tm_fill(col = \"is_phuket\", palette = color_palette) +  # Fill Phuket with red, others gray  \n  tm_layout(title = \"Phuket and Surrounding Provinces\", frame = TRUE)  # Zoom in to Phuket region"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#generating-spatial-weights",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#generating-spatial-weights",
    "title": "Take Home Exercise 2",
    "section": "3.2 Generating Spatial Weights",
    "text": "3.2 Generating Spatial Weights\nThe chunk of code below utilizes the st_weights() function from the sfdep package to generate spatial weights for each neighboring polygon. Specifically, it constructs a weights matrix based on the contiguity defined by the nb_list object, allowing for the specification of weights style (here set to “W” for row-standardized weights). The allow_zero parameter is set to TRUE, which permits the inclusion of polygons with no neighbors in the analysis, ensuring that all spatial units are represented in the weights matrix.\n\n# nb_list is our neighbors list created using st_contiguity\ngm_wt &lt;- st_weights(nb_list, style = \"W\", allow_zero = TRUE)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#morans-i-test",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#morans-i-test",
    "title": "Take Home Exercise 2",
    "section": "3.3 Moran’s I Test",
    "text": "3.3 Moran’s I Test\nWe will be using Moran’s I to compute global spatial autocorrelation instead of Geary’s C because Moran’s I provides a more comprehensive measure of spatial autocorrelation. While Geary’s C focuses on the squared differences between neighboring observations, which can make it sensitive to local variations and outliers, Moran’s I considers the overall correlation between values and their spatial locations. This allows us to capture both positive and negative spatial autocorrelation more effectively.\nFurthermore, Moran’s I is well-suited for assessing patterns across larger regions, making it ideal for our analysis of spatial relationships of drug use across provinces in Thailand. The confidence interval for this experiment will be set at 95%.\nHypothesis Testing:\nNull Hypothesis: The spatial distribution of drug use cases resemble random distribution.\nAlternative Hypothesis: The spatial distribution of drug use cases does not resemble random distribution.\nThe below line of code uses global_moran_test() of sfdep package to performs Moran’s I statistical testing and global_moran_perm() of sfdep is used to perform permutation testing for Moran’s I statistic. A total of 1000 simulation will be performed. We will also be using seed() to ensure the reproducibility of our code.\n\n201720182019202020212022\n\n\n\nglobal_moran_test(drug_use_2017_sf$no_cases,nb_list,gm_wt, zero.policy = TRUE)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw  \nn reduced by no-neighbour observations  \n\nMoran I statistic standard deviate = 1.5049, p-value = 0.06618\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.077263424      -0.013333333       0.003624223 \n\n\n\nset.seed(1234)\ngmc_i_2017&lt;-global_moran_perm(drug_use_2017_sf$no_cases,nb_list,gm_wt, zero.policy = TRUE, nsim=999)\ngmc_i_2017\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.077263, observed rank = 922, p-value = 0.156\nalternative hypothesis: two.sided\n\n\nThe statistical report for 2017 above show that the p-value is greater than alpha value of 0.05. Hence, we do not have enough statistical evidence to reject the null hypothesis that the spatial distribution of drug use cases resemble random distribution (i.e. independent from spatial). Because the Moran’s I statistics is greater than 0 but not close to 1, we can infer that the spatial distribution shows weak signs of clustering.\n\n\n\nglobal_moran_test(drug_use_2018_sf$no_cases,nb_list,gm_wt, zero.policy = TRUE)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw  \nn reduced by no-neighbour observations  \n\nMoran I statistic standard deviate = 1.673, p-value = 0.04716\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.091283835      -0.013333333       0.003910294 \n\n\n\nset.seed(1234)\ngmc_i_2018&lt;-global_moran_perm(drug_use_2018_sf$no_cases,nb_list,gm_wt, zero.policy = TRUE, nsim=999)\ngmc_i_2018\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.091284, observed rank = 944, p-value = 0.112\nalternative hypothesis: two.sided\n\n\nThe statistical report for 2018 above show that the p-value is higher than alpha value of 0.05. Hence, we do not have enough statistical evidence to reject the null hypothesis that the spatial distribution of drug use cases resemble random distribution (i.e. independent from spatial). Because the Moran’s I statistics is greater than 0 but not close to 1, we can infer that the spatial distribution shows weak signs of clustering.\n\n\n\nglobal_moran_test(drug_use_2019_sf$no_cases,nb_list,gm_wt, zero.policy = TRUE)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw  \nn reduced by no-neighbour observations  \n\nMoran I statistic standard deviate = 2.1385, p-value = 0.01624\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.138046280      -0.013333333       0.005010889 \n\n\n\nset.seed(1234)\ngmc_i_2019&lt;-global_moran_perm(drug_use_2019_sf$no_cases,nb_list,gm_wt, zero.policy = TRUE, nsim=999)\ngmc_i_2019\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.13805, observed rank = 972, p-value = 0.056\nalternative hypothesis: two.sided\n\n\nThe statistical report for 2019 above show that the p-value is higher than alpha value of 0.05. Hence, we do not have enough statistical evidence to reject the null hypothesis that the spatial distribution of drug use cases resemble random distribution (i.e. independent from spatial). Because the Moran’s I statistics is greater than 0 but not close to 1, we can infer that the spatial distribution shows weak signs of clustering. This number (0.13805) is however greater than the previous years, a sign that stronger clustering occurred in this year as compared to the previous.\n\n\n\nglobal_moran_test(drug_use_2020_sf$no_cases,nb_list,gm_wt, zero.policy = TRUE)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw  \nn reduced by no-neighbour observations  \n\nMoran I statistic standard deviate = 1.382, p-value = 0.08349\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.087091664      -0.013333333       0.005280586 \n\n\n\nset.seed(1234)\ngmc_i_2020&lt;-global_moran_perm(drug_use_2020_sf$no_cases,nb_list,gm_wt, zero.policy = TRUE, nsim=999)\ngmc_i_2020\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.087092, observed rank = 924, p-value = 0.152\nalternative hypothesis: two.sided\n\n\nThe statistical report for 2020 above show that the p-value is higher than alpha value of 0.05. Hence, we do not have enough statistical evidence to reject the null hypothesis that the spatial distribution of drug use cases resemble random distribution (i.e. independent from spatial). Because the Moran’s I statistics is greater than 0 but not close to 1, we can infer that the spatial distribution shows weak signs of clustering.\n\n\n\nglobal_moran_test(drug_use_2021_sf$no_cases,nb_list,gm_wt, zero.policy = TRUE)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw  \nn reduced by no-neighbour observations  \n\nMoran I statistic standard deviate = 2.862, p-value = 0.002105\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n       0.20376109       -0.01333333        0.00575372 \n\n\n\nset.seed(1234)\ngmc_i_2021&lt;-global_moran_perm(drug_use_2021_sf$no_cases,nb_list,gm_wt, zero.policy = TRUE, nsim=999)\ngmc_i_2021\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.20376, observed rank = 993, p-value = 0.014\nalternative hypothesis: two.sided\n\n\nThe statistical report for 2021 above show that the p-value is lower than alpha value of 0.05. Hence, we have enough statistical evidence to reject the null hypothesis that the spatial distribution of drug use cases resemble random distribution (i.e. independent from spatial). Because the Moran’s I statistics is greater than 0 but not close to 1, we can infer that the spatial distribution shows weak signs of clustering. Overall, this shows a weak but significant positive spatial autocorrelation in drug use cases.\n\n\n\nglobal_moran_test(drug_use_2022_sf$no_cases,nb_list,gm_wt, zero.policy = TRUE)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw  \nn reduced by no-neighbour observations  \n\nMoran I statistic standard deviate = 2.7688, p-value = 0.002813\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.197931897      -0.013333333       0.005822019 \n\n\n\nset.seed(1234)\ngmc_i_2022&lt;-global_moran_perm(drug_use_2022_sf$no_cases,nb_list,gm_wt, zero.policy = TRUE, nsim=999)\ngmc_i_2022\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.19793, observed rank = 994, p-value = 0.012\nalternative hypothesis: two.sided\n\n\nThe statistical report for 2022 above show that the p-value is lower than alpha value of 0.05. Hence, we have enough statistical evidence to reject the null hypothesis that the spatial distribution of drug use cases resemble random distribution (i.e. independent from spatial). Because the Moran’s I statistics is greater than 0 but not close to 1, we can infer that the spatial distribution shows weak signs of clustering. Overall, this shows a weak but significant positive spatial autocorrelation in drug use cases.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nOver the years, we can observe that the Moran’s I statistics is displaying an increasing trend all the way until 2020 when it decreased and then reaching an all-time high in 2021.\nWe can also observe that the drug use cases in 2021 and 2022 no longer resemble random distribution, indicating significant spatial autocorrelation. This change suggests that drug use is becoming increasingly concentrated in specific geographic areas, warranting further investigation into the factors contributing to this clustering.\nThe observed clustering, particularly in 2021 and 2022, suggests that policymakers should consider region-specific interventions. Identifying hotspots for drug use can help allocate resources more effectively and develop targeted prevention and treatment programs."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualising-monte-carlo-morans-i",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualising-monte-carlo-morans-i",
    "title": "Take Home Exercise 2",
    "section": "3.4 Visualising Monte Carlo Moran’s I",
    "text": "3.4 Visualising Monte Carlo Moran’s I\nIn order to examine the simulated Moran’s I test statistics in greater detail, we can collate the descriptive statistics of the test and plot the distribution of the statistical values as a histogram by using the code chunk below.\nIn the code chunk below summary(), var(), hist() and abline() are used.\n\nHistogramSummaryVariance\n\n\n\n\nClick to view the code\n# Set up the plotting area for 2 columns and 3 rows\npar(mfrow = c(3, 2)) \n\n# Histogram for 2017\nhist(gmc_i_2017$res, \n     freq = TRUE, \n     breaks = 20, \n     xlab = \"Simulated Moran's I\", \n     main = \"Histogram for 2017\")\nabline(v = 0, col = \"red\")\n\n# Histogram for 2018\nhist(gmc_i_2018$res, \n     freq = TRUE, \n     breaks = 20, \n     xlab = \"Simulated Moran's I\", \n     main = \"Histogram for 2018\")\nabline(v = 0, col = \"red\")\n\n# Histogram for 2019\nhist(gmc_i_2019$res, \n     freq = TRUE, \n     breaks = 20, \n     xlab = \"Simulated Moran's I\", \n     main = \"Histogram for 2019\")\nabline(v = 0, col = \"red\")\n\n# Histogram for 2020\nhist(gmc_i_2020$res, \n     freq = TRUE, \n     breaks = 20, \n     xlab = \"Simulated Moran's I\", \n     main = \"Histogram for 2020\")\nabline(v = 0, col = \"red\")\n\n# Histogram for 2021\nhist(gmc_i_2021$res, \n     freq = TRUE, \n     breaks = 20, \n     xlab = \"Simulated Moran's I\", \n     main = \"Histogram for 2021\")\nabline(v = 0, col = \"red\")\n\n# Histogram for 2022\nhist(gmc_i_2022$res, \n     freq = TRUE, \n     breaks = 20, \n     xlab = \"Simulated Moran's I\", \n     main = \"Histogram for 2022\")\nabline(v = 0, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n\n# 2017\nprint(summary(gmc_i_2017$res[1:999]))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.14280 -0.05100 -0.02108 -0.01248  0.02061  0.20105 \n\n# 2018\nprint(summary(gmc_i_2018$res[1:999]))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.14516 -0.05259 -0.01839 -0.01148  0.02319  0.26485 \n\n# 2019\nprint(summary(gmc_i_2019$res[1:999]))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.15737 -0.05925 -0.01691 -0.01110  0.02752  0.31845 \n\n# 2020\nprint(summary(gmc_i_2020$res[1:999]))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.15929 -0.06245 -0.02221 -0.01443  0.02685  0.33025 \n\n# 2021\nprint(summary(gmc_i_2021$res[1:999]))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.23519 -0.06342 -0.01758 -0.01355  0.03012  0.29476 \n\n# 2022\nprint(summary(gmc_i_2022$res[1:999]))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.21825 -0.05910 -0.01462 -0.01072  0.03411  0.30446 \n\n\n\n\n\n# Calculating variances and storing them in variables\ngmc_i_var_2017 &lt;- var(gmc_i_2017$res[1:999])\ngmc_i_var_2018 &lt;- var(gmc_i_2018$res[1:999])\ngmc_i_var_2019 &lt;- var(gmc_i_2019$res[1:999])\ngmc_i_var_2020 &lt;- var(gmc_i_2020$res[1:999])\ngmc_i_var_2021 &lt;- var(gmc_i_2021$res[1:999])\ngmc_i_var_2022 &lt;- var(gmc_i_2022$res[1:999])\n\n# Printing the variances\ncat(sprintf(\"Variance of '2017': %f\\n\", gmc_i_var_2017))\n\nVariance of '2017': 0.003137\n\ncat(sprintf(\"Variance of '2018': %f\\n\", gmc_i_var_2018))\n\nVariance of '2018': 0.003527\n\ncat(sprintf(\"Variance of '2019': %f\\n\", gmc_i_var_2019))\n\nVariance of '2019': 0.004564\n\ncat(sprintf(\"Variance of '2020': %f\\n\", gmc_i_var_2020))\n\nVariance of '2020': 0.004721\n\ncat(sprintf(\"Variance of '2021': %f\\n\", gmc_i_var_2021))\n\nVariance of '2021': 0.005436\n\ncat(sprintf(\"Variance of '2022': %f\\n\", gmc_i_var_2022))\n\nVariance of '2022': 0.005386\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nRight skewed histogram: Majority of the data have negative spatial autocorrelation but there are some regions with significantly higher positive spatial autocorrelation (high drug use cases in certain neighborhoods, low or moderate in majority).\nNegative Median: Indicates negative spatial autocorrelation, majority of provinces have low or moderate drug use)\nLow Variance (Range from 0.003 to 0.0055): Suggests consistency across simulations, indicating that the presence of spatial patterns are not influenced by local conditions"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualising-local-morans-i-p-value-of-local-morans-i",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualising-local-morans-i-p-value-of-local-morans-i",
    "title": "Take Home Exercise 2",
    "section": "4.1 Visualising Local Moran’s I & p-value of Local Moran’s I",
    "text": "4.1 Visualising Local Moran’s I & p-value of Local Moran’s I\nIn the code chunk below, we are are using local_moran() of sfdep package to performs Moran’s I statistical testing and tmap functions to prepare a choropleth map by using value in the p_ii_sim field.\nFor consistency sake, we will be sticking to p_ii_sim for the entire experiment.\n\n201720182019202020212022\n\n\n\n\nClick to view the code\nset.seed(1234)\nlisa_2017 &lt;- drug_use_2017_sf %&gt;% \n  mutate(local_moran = local_moran(\n    no_cases, nb_list, gm_wt, nsim = 999, zero.policy=TRUE),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\ntmap_mode(\"plot\")\nmap1_2017 &lt;- tm_shape(lisa_2017) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"Local Moran's I of Drug Use Cases in 2017\",\n            main.title.size = 0.8)\n\nmap2_2017 &lt;- tm_shape(lisa_2017) +\n  tm_fill(\"p_ii_sim\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"P-value of Local Moran's I in 2017\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1_2017, map2_2017, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to view the code\nset.seed(1234)\nlisa_2018 &lt;- drug_use_2018_sf %&gt;% \n  mutate(local_moran = local_moran(\n    no_cases, nb_list, gm_wt, nsim = 999, zero.policy=TRUE),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\ntmap_mode(\"plot\")\nmap1_2018 &lt;- tm_shape(lisa_2018) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"Local Moran's I of Drug Use Cases in 2018\",\n            main.title.size = 0.8)\n\nmap2_2018 &lt;- tm_shape(lisa_2018) +\n  tm_fill(\"p_ii_sim\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"P-value of Local Moran's I in 2018\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1_2018, map2_2018, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to view the code\nset.seed(1234)\nlisa_2019 &lt;- drug_use_2019_sf %&gt;% \n  mutate(local_moran = local_moran(\n    no_cases, nb_list, gm_wt, nsim = 999, zero.policy=TRUE),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\ntmap_mode(\"plot\")\nmap1_2019 &lt;- tm_shape(lisa_2019) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"Local Moran's I of Drug Use Cases in 2019\",\n            main.title.size = 0.8)\n\nmap2_2019 &lt;- tm_shape(lisa_2019) +\n  tm_fill(\"p_ii_sim\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"P-value of Local Moran's I in 2019\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1_2019, map2_2019, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to view the code\nset.seed(1234)\nlisa_2020 &lt;- drug_use_2020_sf %&gt;% \n  mutate(local_moran = local_moran(\n    no_cases, nb_list, gm_wt, nsim = 999, zero.policy=TRUE),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\ntmap_mode(\"plot\")\nmap1_2020 &lt;- tm_shape(lisa_2020) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"Local Moran's I of Drug Use Cases in 2020\",\n            main.title.size = 0.8)\n\nmap2_2020 &lt;- tm_shape(lisa_2020) +\n  tm_fill(\"p_ii_sim\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"P-value of Local Moran's I in 2020\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1_2020, map2_2020, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to view the code\nset.seed(1234)\nlisa_2021 &lt;- drug_use_2021_sf %&gt;% \n  mutate(local_moran = local_moran(\n    no_cases, nb_list, gm_wt, nsim = 999, zero.policy=TRUE),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\ntmap_mode(\"plot\")\nmap1_2021 &lt;- tm_shape(lisa_2021) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"Local Moran's I of Drug Use Cases in 2021\",\n            main.title.size = 0.8)\n\nmap2_2021 &lt;- tm_shape(lisa_2021) +\n  tm_fill(\"p_ii_sim\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"P-value of Local Moran's I in 2021\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1_2021, map2_2021, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to view the code\nset.seed(1234)\nlisa_2022 &lt;- drug_use_2022_sf %&gt;% \n  mutate(local_moran = local_moran(\n    no_cases, nb_list, gm_wt, nsim = 999, zero.policy=TRUE),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\ntmap_mode(\"plot\")\nmap1_2022 &lt;- tm_shape(lisa_2022) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"Local Moran's I of Drug Use Cases in 2022\",\n            main.title.size = 0.8)\n\nmap2_2022 &lt;- tm_shape(lisa_2022) +\n  tm_fill(\"p_ii_sim\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"P-value of Local Moran's I in 2022\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1_2022, map2_2022, ncol = 2)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#skewness-of-lisa",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#skewness-of-lisa",
    "title": "Take Home Exercise 2",
    "section": "4.2 Skewness of Lisa",
    "text": "4.2 Skewness of Lisa\nIn order to determine whether to use mean or median for the subsequent visualisation, we will need to check if the data has a normal distribution of if its skewed.\n\nsummary(lisa_2017$skewness)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n-4.5356 -2.4136 -1.8628 -0.7379  1.8934  3.3641       1 \n\nsummary(lisa_2018$skewness)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n-4.2038 -2.1536 -1.7613 -0.7909  1.5514  3.0979       1 \n\nsummary(lisa_2019$skewness)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n-2.7683 -1.4886 -1.1985 -0.5408  1.0407  2.1595       1 \n\nsummary(lisa_2020$skewness)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n-2.5457 -1.3493 -1.1252 -0.5833  0.9249  2.1810       1 \n\nsummary(lisa_2021$skewness)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n-1.4815 -0.7478 -0.5488 -0.2148  0.5724  1.1561       1 \n\nsummary(lisa_2022$skewness)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's \n-0.97047 -0.51021 -0.32683 -0.06994  0.48088  0.85687        1 \n\n\nThe median of the skewness statistics are in the negative range, from -0.32683 to -1.8628 which is close to 0. Hence we will be using mean to perform classification."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualising-lisa-map",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualising-lisa-map",
    "title": "Take Home Exercise 2",
    "section": "4.3 Visualising Lisa Map",
    "text": "4.3 Visualising Lisa Map\nLISA map is a categorical map showing outliers and clusters.\nTwo Types of Outliers:\nHigh-Low:\n\nAreas that have high values (e.g., a high incidence of a certain event) that are surrounded by areas with low values. This pattern may suggest that the high value is an anomaly compared to the neighboring areas.\n\nLow-High:\n\nAreas that exhibit low values surrounded by areas with high values. This situation may suggest that the low value is an anomaly or an area of concern, indicating that it is isolated from surrounding regions with higher levels of the phenomenon.\n\nTwo Types of Clusters:\nHigh-High:\n\nAreas where high values are surrounded by other high values. This indicates a cluster of similar high values, suggesting positive spatial autocorrelation. This pattern may reflect shared influences or conditions contributing to the elevated values in those regions. For instance, areas with high rates of drug use that are geographically close may indicate a pervasive issue or common underlying factors.\n\nLow-Low:\n\nAreas where low values are surrounded by other low values. This pattern indicates a cluster of similar low values, suggesting that these areas consistently experience lower levels of the phenomenon in question.\n\nThe below chunk of code generates the lisa map for each year based on the parameter mean, we will also be filtering for only significant data (&gt;95%).\n\n201720182019202020212022\n\n\n\n\nClick to view the code\nlisa_sig_2017 &lt;- lisa_2017 %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\ntmap_mode(\"plot\")\ntm_shape(lisa_2017) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig_2017) +\n  tm_fill(\"mean\", palette = brewer.pal(4, \"Set1\")) +  # Change the color palette here\n  tm_borders(alpha = 0.4) +\n  tm_layout(main.title = \"Lisa Map for 2017\", main.title.size = 1.2, main.title.position = \"center\", legend.text.size = 1.2)\n\n\n\n\n\n\n\n\n\nClick to view the code\nprint(lisa_sig_2017 %&gt;%  select(province, mean))\n\n\nSimple feature collection with 7 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 99.01629 ymin: 13.17847 xmax: 101.9901 ymax: 17.74368\nGeodetic CRS:  WGS 84\n# A tibble: 7 × 3\n  province       mean                                                   geometry\n  &lt;chr&gt;          &lt;fct&gt;                                        &lt;MULTIPOLYGON [°]&gt;\n1 SAMUT PRAKAN   Low-High  (((100.7306 13.71713, 100.7307 13.71681, 100.7313 13…\n2 NONTHABURI     Low-High  (((100.3415 14.10079, 100.3415 14.10001, 100.3415 14…\n3 CHACHOENGSAO   High-High (((101.0612 13.97613, 101.0625 13.976, 101.0629 13.9…\n4 NAKHON SAWAN   Low-Low   (((100.0266 16.189, 100.0267 16.18889, 100.0268 16.1…\n5 KAMPHAENG PHET Low-Low   (((99.48875 16.91044, 99.48883 16.91016, 99.48884 16…\n6 PHITSANULOK    Low-Low   (((101.0033 17.7334, 101.0037 17.73339, 101.0043 17.…\n7 SAMUT SAKHON   Low-High  (((100.3091 13.7217, 100.3091 13.72169, 100.3096 13.…\n\n\n\n\n\n\nClick to view the code\nlisa_sig_2018 &lt;- lisa_2018 %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\ntmap_mode(\"plot\")\ntm_shape(lisa_2018) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig_2018) +\n  tm_fill(\"mean\", palette = brewer.pal(4, \"Set1\")) +  # Change the color palette here\n  tm_borders(alpha = 0.4) +\n  tm_layout(main.title = \"Lisa Map for 2018\", main.title.size = 1.2, main.title.position = \"center\", legend.text.size = 1.2)\n\n\n\n\n\n\n\n\n\nClick to view the code\nprint(lisa_sig_2018 %&gt;%  select(province, mean))\n\n\nSimple feature collection with 5 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 100.0267 ymin: 13.17847 xmax: 103.4129 ymax: 18.30525\nGeodetic CRS:  WGS 84\n# A tibble: 5 × 3\n  province     mean                                                     geometry\n  &lt;chr&gt;        &lt;fct&gt;                                          &lt;MULTIPOLYGON [°]&gt;\n1 SAMUT PRAKAN High-High (((100.7306 13.71713, 100.7307 13.71681, 100.7313 13.7…\n2 NONTHABURI   Low-High  (((100.3415 14.10079, 100.3415 14.10001, 100.3415 14.0…\n3 CHACHOENGSAO High-High (((101.0612 13.97613, 101.0625 13.976, 101.0629 13.976…\n4 NONG KHAI    High-Low  (((103.2985 18.29698, 103.2984 18.29697, 103.2982 18.2…\n5 SAMUT SAKHON Low-High  (((100.3091 13.7217, 100.3091 13.72169, 100.3096 13.72…\n\n\n\n\n\n\nClick to view the code\nlisa_sig_2019 &lt;- lisa_2019 %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\ntmap_mode(\"plot\")\ntm_shape(lisa_2019) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_shape(lisa_sig_2019) +\n  tm_fill(\"mean\", palette = brewer.pal(4, \"Set1\")) + \n  tm_borders(alpha = 0.4) +\n  tm_layout(main.title = \"Lisa Map for 2019\", main.title.size = 1.2, main.title.position = \"center\", legend.text.size = 1.2)\n\n\n\n\n\n\n\n\n\nClick to view the code\nprint(lisa_sig_2019 %&gt;%  select(province, mean))\n\n\nSimple feature collection with 4 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 100.2624 ymin: 13.17847 xmax: 103.4129 ymax: 18.30525\nGeodetic CRS:  WGS 84\n# A tibble: 4 × 3\n  province     mean                                                     geometry\n  &lt;chr&gt;        &lt;fct&gt;                                          &lt;MULTIPOLYGON [°]&gt;\n1 SAMUT PRAKAN High-High (((100.7306 13.71713, 100.7307 13.71681, 100.7313 13.7…\n2 NONTHABURI   Low-High  (((100.3415 14.10079, 100.3415 14.10001, 100.3415 14.0…\n3 CHACHOENGSAO High-High (((101.0612 13.97613, 101.0625 13.976, 101.0629 13.976…\n4 NONG KHAI    Low-Low   (((103.2985 18.29698, 103.2984 18.29697, 103.2982 18.2…\n\n\n\n\n\n\nClick to view the code\nlisa_sig_2020 &lt;- lisa_2020 %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\ntmap_mode(\"plot\")\ntm_shape(lisa_2020) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_shape(lisa_sig_2020) +\n  tm_fill(\"mean\", palette = brewer.pal(4, \"Set1\")) + \n  tm_borders(alpha = 0.4) +\n  tm_layout(main.title = \"Lisa Map for 2020\", main.title.size = 1.2, main.title.position = \"center\", legend.text.size = 1.2)\n\n\n\n\n\n\n\n\n\nClick to view the code\nprint(lisa_sig_2020 %&gt;%  select(province, mean))\n\n\nSimple feature collection with 3 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 99.08612 ymin: 13.17847 xmax: 101.9901 ymax: 16.19126\nGeodetic CRS:  WGS 84\n# A tibble: 3 × 3\n  province     mean                                                     geometry\n  &lt;chr&gt;        &lt;fct&gt;                                          &lt;MULTIPOLYGON [°]&gt;\n1 SAMUT PRAKAN Low-High  (((100.7306 13.71713, 100.7307 13.71681, 100.7313 13.7…\n2 CHACHOENGSAO High-High (((101.0612 13.97613, 101.0625 13.976, 101.0629 13.976…\n3 NAKHON SAWAN Low-Low   (((100.0266 16.189, 100.0267 16.18889, 100.0268 16.188…\n\n\n\n\n\n\nClick to view the code\nlisa_sig_2021 &lt;- lisa_2021 %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\ntmap_mode(\"plot\")\ntm_shape(lisa_2021) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_shape(lisa_sig_2021) +\n  tm_fill(\"mean\", palette = brewer.pal(4, \"Set1\")) + \n  tm_borders(alpha = 0.4) +\n  tm_layout(main.title = \"Lisa Map for 2021\", main.title.size = 1.2, main.title.position = \"center\", legend.text.size = 1.2)\n\n\n\n\n\n\n\n\n\nClick to view the code\nprint(lisa_sig_2021 %&gt;%  select(province, mean))\n\n\nSimple feature collection with 11 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 98.18107 ymin: 12.56321 xmax: 104.825 ymax: 16.91058\nGeodetic CRS:  WGS 84\n# A tibble: 11 × 3\n   province        mean                                                 geometry\n   &lt;chr&gt;           &lt;fct&gt;                                      &lt;MULTIPOLYGON [°]&gt;\n 1 SING BURI       Low-Low   (((100.3691 15.0894, 100.3697 15.0891, 100.3708 15…\n 2 CHAI NAT        Low-Low   (((100.1199 15.41243, 100.121 15.41234, 100.1229 1…\n 3 YASOTHON        High-High (((104.3952 16.34843, 104.3983 16.34707, 104.4 16.…\n 4 NAKHON SAWAN    Low-Low   (((100.0266 16.189, 100.0267 16.18889, 100.0268 16…\n 5 UTHAI THANI     Low-Low   (((99.13905 15.79655, 99.13918 15.79652, 99.13965 …\n 6 KAMPHAENG PHET  Low-Low   (((99.48875 16.91044, 99.48883 16.91016, 99.48884 …\n 7 RATCHABURI      Low-Low   (((99.8821 13.94977, 99.88218 13.94976, 99.88248 1…\n 8 KANCHANABURI    Low-Low   (((98.58631 15.65465, 98.58662 15.65384, 98.58697 …\n 9 SUPHAN BURI     Low-Low   (((99.37118 15.05073, 99.37454 15.0495, 99.3762 15…\n10 SAMUT SONGKHRAM Low-Low   (((100.0116 13.51359, 100.0117 13.51359, 100.0117 …\n11 PHETCHABURI     Low-Low   (((99.75869 13.34249, 99.75876 13.34248, 99.75883 …\n\n\n\n\n\n\nClick to view the code\nlisa_sig_2022 &lt;- lisa_2022 %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\ntmap_mode(\"plot\")\ntm_shape(lisa_2022) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_shape(lisa_sig_2022) +\n  tm_fill(\"mean\", palette = brewer.pal(4, \"Set1\")) + \n  tm_borders(alpha = 0.4) +\n  tm_layout(main.title = \"Lisa Map for 2022\", main.title.size = 1.2, main.title.position = \"center\", legend.text.size = 1.2)\n\n\n\n\n\n\n\n\n\nClick to view the code\nprint(lisa_sig_2022 %&gt;%  select(province, mean))\n\n\nSimple feature collection with 8 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 99.01629 ymin: 7.090332 xmax: 104.4353 ymax: 18.30525\nGeodetic CRS:  WGS 84\n# A tibble: 8 × 3\n  province         mean                                                 geometry\n  &lt;chr&gt;            &lt;fct&gt;                                      &lt;MULTIPOLYGON [°]&gt;\n1 NONG BUA LAM PHU Low-High  (((102.2866 17.69207, 102.2867 17.69206, 102.2867 …\n2 NONG KHAI        High-High (((103.2985 18.29698, 103.2984 18.29697, 103.2982 …\n3 MAHA SARAKHAM    High-High (((103.1562 16.6425, 103.1567 16.64236, 103.1568 1…\n4 KALASIN          High-High (((103.584 17.09981, 103.5845 17.09973, 103.5846 1…\n5 SAKON NAKHON     High-High (((103.5404 18.06785, 103.5405 18.0677, 103.5405 1…\n6 NAKHON SAWAN     Low-Low   (((100.0266 16.189, 100.0267 16.18889, 100.0268 16…\n7 KAMPHAENG PHET   Low-Low   (((99.48875 16.91044, 99.48883 16.91016, 99.48884 …\n8 PHATTHALUNG      Low-High  (((99.96416 7.90199, 99.9642 7.901912, 99.96425 7.…\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nHigh-High Clusters in 2017-2020\n\nFrom 2017-2020, we can observe Chachoengsao being in the “High-High” category, suggesting that this province consistently has a high number of drug cases surrounded by other provinces with similarly high numbers. This type of spatial autocorrelation indicates a cluster of high drug use incidents in that region.\nPossible Reason: A likely reason for this observation is the geographic location of this province. Chachoengsao is strategically located near major transportation routes, including highways and railways, facilitating the movement of drugs. Its proximity to Bangkok and other major urban centers may have also contribute to increased drug trafficking and distribution.\n\nOutliers: Nonthaburi, Samut Prakan\n\nWe can also observe that in the same time period, there exists different provinces (e.g Nonthaburi, Samut Prakan) close by in the “Low-High” category west of Chachoengsao, suggesting that these provinces are outliers with low drug uses surrounded by provinces with high drug uses.\n\nTrend of High-High Clusters\n\nFor 2021, we can observe that the “High-High” cluster has moved to the eastern region of Thailand to Yasothon.\nIn the subsequent year 2022 however, this “High-High” cluster has moved slightly north of Yasothon and now includes 4 provinces: Nong Khai, Maha Sarakham Kalasin and Sakon Nakhon.\nThis indicates a continued eastward movement of drug use, reflecting ongoing dynamics in drug trafficking and consumption.\n\nLow-Low Clusters\n\nBetween 2017 - 2022, we can observe that there are clusters of provinces with low drug use cases in the west of Thailand."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#computing-local-gi-statistics",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#computing-local-gi-statistics",
    "title": "Take Home Exercise 2",
    "section": "5.1 Computing Local Gi* Statistics",
    "text": "5.1 Computing Local Gi* Statistics\nAs usual, we will need to derive a spatial weight matrix before we can compute local Gi* statistics. This weight matrix defines the spatial relationships (such as neighbors or distance) between the regions or areas and determines how much influence neighboring areas have on each other, which is crucial for calculating spatial autocorrelation (like Gi*)\nThe code chunk below is used to firstly derive a spatial weight matrix by using sfdep functions and tidyverse approach and secondly, compute the local Gi*. Since the geometry column of our dataset contains multipolygons (which represent areas with multiple disjointed sections) instead of polygons, we will be calculating the centroid which will help simplify the spatial relationships by reducing the complex shapes into single points. This makes it easier to compute distance-based spatial statistics.\n\n201720182019202020212022\n\n\n\n\nClick to view the code\nset.seed(1234)\nwm_idw_2017 &lt;- drug_use_2017_sf %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         centroid = st_centroid(geometry), \n         wts = st_inverse_distance(nb, centroid,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\nHCSA_2017 &lt;- wm_idw_2017 %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    no_cases, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\nwrite_rds(HCSA_2017, \"data/rds/HCSA_2017.rds\")\n\n\n\n\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n# A tibble: 77 × 17\n   gi_star cluster   e_gi    var_gi std_dev  p_value p_sim p_folded_sim skewness\n     &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1   2.54  High    0.0288 0.0000135 -0.0865  9.31e-1  0.98         0.49    0.586\n 2   4.42  Low     0.0109 0.0000546  6.04    1.58e-9  0.02         0.01    2.95 \n 3   2.72  Low     0.0117 0.0000494  3.04    2.39e-3  0.06         0.03    3.11 \n 4   2.24  Low     0.0109 0.0000223  3.35    8.17e-4  0.06         0.03    1.95 \n 5  -1.04  Low     0.0121 0.0000288 -0.932   3.51e-1  0.24         0.12    1.98 \n 6  -1.06  Low     0.0114 0.0000508 -0.867   3.86e-1  0.2          0.1     2.00 \n 7  -1.25  Low     0.0128 0.0000302 -1.19    2.35e-1  0.12         0.06    0.963\n 8  -1.30  Low     0.0106 0.0000357 -1.04    2.96e-1  0.2          0.1     1.72 \n 9  -1.26  Low     0.0116 0.0000400 -1.25    2.12e-1  0.06         0.03    1.70 \n10  -0.720 Low     0.0110 0.0000311 -0.504   6.14e-1  0.7          0.35    1.61 \n# ℹ 67 more rows\n# ℹ 8 more variables: kurtosis &lt;dbl&gt;, nb &lt;nb&gt;, wts &lt;list&gt;, province &lt;chr&gt;,\n#   fiscal_year &lt;dbl&gt;, types_of_drug_offenses &lt;chr&gt;, no_cases &lt;dbl&gt;,\n#   geometry &lt;MULTIPOLYGON [°]&gt;\n\n\n\n\n\n\nClick to view the code\nset.seed(1234)\nwm_idw_2018 &lt;- drug_use_2018_sf %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         centroid = st_centroid(geometry), \n         wts = st_inverse_distance(nb, centroid,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\nHCSA_2018 &lt;- wm_idw_2018 %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    no_cases, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\n\nwrite_rds(HCSA_2018, \"data/rds/HCSA_2018.rds\")\n\n\n\n\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n# A tibble: 77 × 17\n   gi_star cluster   e_gi     var_gi std_dev p_value p_sim p_folded_sim skewness\n     &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1  2.71   High    0.0264 0.00000980   0.363 7.17e-1  0.66         0.33    0.574\n 2  4.30   High    0.0147 0.0000436    5.24  1.63e-7  0.02         0.01    2.69 \n 3  3.01   Low     0.0125 0.0000369    3.28  1.03e-3  0.04         0.02    2.98 \n 4  2.18   Low     0.0122 0.0000175    2.98  2.90e-3  0.08         0.04    1.64 \n 5 -0.552  Low     0.0123 0.0000259   -0.406 6.85e-1  0.74         0.37    1.97 \n 6 -0.833  Low     0.0118 0.0000382   -0.672 5.02e-1  0.46         0.23    1.80 \n 7 -0.373  Low     0.0134 0.0000234   -0.436 6.62e-1  0.74         0.37    0.878\n 8 -1.15   Low     0.0109 0.0000269   -0.898 3.69e-1  0.26         0.13    1.65 \n 9 -1.28   Low     0.0120 0.0000334   -1.26  2.07e-1  0.08         0.04    1.62 \n10  0.0122 Low     0.0114 0.0000229    0.340 7.34e-1  0.62         0.31    1.47 \n# ℹ 67 more rows\n# ℹ 8 more variables: kurtosis &lt;dbl&gt;, nb &lt;nb&gt;, wts &lt;list&gt;, province &lt;chr&gt;,\n#   fiscal_year &lt;dbl&gt;, types_of_drug_offenses &lt;chr&gt;, no_cases &lt;dbl&gt;,\n#   geometry &lt;MULTIPOLYGON [°]&gt;\n\n\n\n\n\n\nClick to view the code\nset.seed(1234)\nwm_idw_2019 &lt;- drug_use_2019_sf %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         centroid = st_centroid(geometry),\n         wts = st_inverse_distance(nb, centroid,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\nHCSA_2019 &lt;- wm_idw_2019 %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    no_cases, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\n\nwrite_rds(HCSA_2019, \"data/rds/HCSA_2019.rds\")\n\n\n\n\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n# A tibble: 77 × 17\n   gi_star cluster   e_gi    var_gi std_dev  p_value p_sim p_folded_sim skewness\n     &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1  2.30   High    0.0229 0.0000125  0.316   7.52e-1  0.72         0.36    0.607\n 2  3.81   High    0.0165 0.0000346  4.27    1.92e-5  0.02         0.01    1.95 \n 3  2.63   Low     0.0123 0.0000294  2.91    3.65e-3  0.04         0.02    1.86 \n 4  1.83   High    0.0130 0.0000152  2.25    2.45e-2  0.1          0.05    1.01 \n 5 -0.504  High    0.0131 0.0000218 -0.494   6.21e-1  0.68         0.34    1.54 \n 6 -0.757  Low     0.0116 0.0000309 -0.532   5.95e-1  0.66         0.33    1.16 \n 7  0.0342 Low     0.0134 0.0000193 -0.0508  9.60e-1  0.88         0.44    0.557\n 8 -1.22   Low     0.0110 0.0000212 -0.956   3.39e-1  0.24         0.12    1.17 \n 9 -1.24   Low     0.0127 0.0000273 -1.30    1.93e-1  0.1          0.05    1.08 \n10  0.650  Low     0.0117 0.0000185  1.09    2.77e-1  0.32         0.16    0.699\n# ℹ 67 more rows\n# ℹ 8 more variables: kurtosis &lt;dbl&gt;, nb &lt;nb&gt;, wts &lt;list&gt;, province &lt;chr&gt;,\n#   fiscal_year &lt;dbl&gt;, types_of_drug_offenses &lt;chr&gt;, no_cases &lt;dbl&gt;,\n#   geometry &lt;MULTIPOLYGON [°]&gt;\n\n\n\n\n\n\nClick to view the code\nset.seed(1234)\nwm_idw_2020 &lt;- drug_use_2020_sf %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         centroid = st_centroid(geometry),\n         wts = st_inverse_distance(nb, centroid,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\nHCSA_2020 &lt;- wm_idw_2020 %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    no_cases, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\n\nwrite_rds(HCSA_2020, \"data/rds/HCSA_2020.rds\")\n\n\n\n\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n# A tibble: 77 × 17\n   gi_star cluster   e_gi    var_gi std_dev  p_value p_sim p_folded_sim skewness\n     &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1   1.34  High    0.0205 0.0000115  -0.463 0.643     0.86         0.43    0.521\n 2   2.98  Low     0.0125 0.0000308   3.80  0.000143  0.02         0.01    1.85 \n 3   1.71  Low     0.0121 0.0000272   1.90  0.0571    0.16         0.08    1.50 \n 4   1.21  Low     0.0116 0.0000131   1.87  0.0612    0.12         0.06    0.727\n 5  -1.08  Low     0.0129 0.0000161  -1.09  0.275     0.24         0.12    1.11 \n 6  -1.09  Low     0.0115 0.0000312  -0.762 0.446     0.46         0.23    1.32 \n 7  -1.28  Low     0.0136 0.0000210  -1.20  0.230     0.18         0.09    0.584\n 8  -1.39  Low     0.0110 0.0000184  -1.10  0.272     0.14         0.07    0.893\n 9  -1.36  Low     0.0122 0.0000219  -1.37  0.170     0.06         0.03    0.877\n10  -0.713 Low     0.0118 0.0000152  -0.567 0.571     0.7          0.35    0.550\n# ℹ 67 more rows\n# ℹ 8 more variables: kurtosis &lt;dbl&gt;, nb &lt;nb&gt;, wts &lt;list&gt;, province &lt;chr&gt;,\n#   fiscal_year &lt;dbl&gt;, types_of_drug_offenses &lt;chr&gt;, no_cases &lt;dbl&gt;,\n#   geometry &lt;MULTIPOLYGON [°]&gt;\n\n\n\n\n\n\nClick to view the code\nset.seed(1234)\nwm_idw_2021 &lt;- drug_use_2021_sf %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         centroid = st_centroid(geometry),\n         wts = st_inverse_distance(nb, centroid,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\nHCSA_2021 &lt;- wm_idw_2021 %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    no_cases, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\n\nwrite_rds(HCSA_2021, \"data/rds/HCSA_2021.rds\")\n\n\n\n\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n# A tibble: 77 × 17\n   gi_star cluster   e_gi    var_gi std_dev p_value p_sim p_folded_sim skewness\n     &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1   0.289 High    0.0173 0.0000114  -0.943  0.346   0.38         0.19   0.187 \n 2   1.74  Low     0.0122 0.0000230   2.33   0.0199  0.06         0.03   0.973 \n 3   0.538 High    0.0137 0.0000239   0.349  0.727   0.66         0.33   1.14  \n 4   0.482 Low     0.0114 0.0000130   0.937  0.349   0.34         0.17   0.645 \n 5  -1.68  Low     0.0126 0.0000120  -1.62   0.106   0.1          0.05   0.754 \n 6  -1.68  Low     0.0113 0.0000196  -1.36   0.175   0.08         0.04   0.630 \n 7  -0.904 Low     0.0127 0.0000133  -0.731  0.465   0.5          0.25   0.339 \n 8  -1.99  Low     0.0113 0.0000143  -1.73   0.0844  0.02         0.01   0.539 \n 9  -2.00  Low     0.0121 0.0000183  -1.93   0.0533  0.02         0.01   0.327 \n10  -0.514 Low     0.0118 0.0000130  -0.256  0.798   0.84         0.42  -0.0263\n# ℹ 67 more rows\n# ℹ 8 more variables: kurtosis &lt;dbl&gt;, nb &lt;nb&gt;, wts &lt;list&gt;, province &lt;chr&gt;,\n#   fiscal_year &lt;dbl&gt;, types_of_drug_offenses &lt;chr&gt;, no_cases &lt;dbl&gt;,\n#   geometry &lt;MULTIPOLYGON [°]&gt;\n\n\n\n\n\n\nClick to view the code\nset.seed(1234)\nwm_idw_2022 &lt;- drug_use_2022_sf %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         centroid = st_centroid(geometry),\n         wts = st_inverse_distance(nb, centroid,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\nHCSA_2022 &lt;- wm_idw_2022 %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    no_cases, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\n\nwrite_rds(HCSA_2022, \"data/rds/HCSA_2022.rds\")\n\n\n\n\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n# A tibble: 77 × 17\n   gi_star cluster   e_gi     var_gi std_dev p_value p_sim p_folded_sim skewness\n     &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1 -0.308  High    0.0144 0.00000913 -0.797   0.425   0.48         0.24  -0.124 \n 2  0.647  High    0.0138 0.0000194   0.549   0.583   0.48         0.24   1.01  \n 3  0.0640 Low     0.0124 0.0000152   0.221   0.825   0.84         0.42   0.530 \n 4 -0.575  Low     0.0123 0.00000864 -0.408   0.683   0.76         0.38   0.421 \n 5 -1.21   Low     0.0130 0.00000830 -1.26    0.209   0.2          0.1    0.393 \n 6 -1.40   Low     0.0124 0.0000141  -1.28    0.202   0.1          0.05   0.460 \n 7 -0.834  Low     0.0123 0.00000916 -0.546   0.585   0.66         0.33   0.551 \n 8 -1.75   Low     0.0116 0.0000117  -1.39    0.165   0.16         0.08   0.340 \n 9 -1.77   Low     0.0122 0.0000130  -1.68    0.0925  0.06         0.03   0.253 \n10 -0.327  Low     0.0117 0.00000940  0.0358  0.971   0.98         0.49  -0.0725\n# ℹ 67 more rows\n# ℹ 8 more variables: kurtosis &lt;dbl&gt;, nb &lt;nb&gt;, wts &lt;list&gt;, province &lt;chr&gt;,\n#   fiscal_year &lt;dbl&gt;, types_of_drug_offenses &lt;chr&gt;, no_cases &lt;dbl&gt;,\n#   geometry &lt;MULTIPOLYGON [°]&gt;"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualising-local-hcsa",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualising-local-hcsa",
    "title": "Take Home Exercise 2",
    "section": "5.2 Visualising Local HCSA",
    "text": "5.2 Visualising Local HCSA\nTo visualise local HCSA, we can plot the gi* value and the p-value of each year side by side.\nThe chunk of code below uses tmap() to plot and visualize the spatial distribution of the Gi* statistic for the different years. For this section we have the maps of Gi* and the p-value arranged side by side for easier comparisons.\n\n201720182019202020212022\n\n\n\n\nClick to view the code\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(HCSA_2017) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"Gi* of 2017\",\n            main.title.size = 0.8)\nmap2 &lt;- tm_shape(HCSA_2017) +\n  tm_fill(\"p_value\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of Gi* of 2017\",\n            main.title.size = 0.8)\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to view the code\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(HCSA_2018) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8))+\n  tm_layout(main.title = \"Gi* of 2018\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(HCSA_2018) +\n  tm_fill(\"p_value\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of Gi* of 2018\",\n            main.title.size = 0.8)\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to view the code\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(HCSA_2019) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8))+\n  tm_layout(main.title = \"Gi* of 2019\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(HCSA_2019) +\n  tm_fill(\"p_value\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of Gi* of 2019\",\n            main.title.size = 0.8)\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to view the code\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(HCSA_2020) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8))+\n  tm_layout(main.title = \"Gi* of 2020\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(HCSA_2020) +\n  tm_fill(\"p_value\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of Gi* of 2020\",\n            main.title.size = 0.8)\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to view the code\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(HCSA_2021) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8))+\n  tm_layout(main.title = \"Gi* of 2021\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(HCSA_2021) +\n  tm_fill(\"p_value\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of Gi* of 2021\",\n            main.title.size = 0.8)\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to view the code\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(HCSA_2022) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8))+\n  tm_layout(main.title = \"Gi* of 2022\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(HCSA_2022) +\n  tm_fill(\"p_value\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of Gi* of 2022\",\n            main.title.size = 0.8)\ntmap_arrange(map1, map2, ncol = 2)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualising-hot-spot-cold-spot-areas",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualising-hot-spot-cold-spot-areas",
    "title": "Take Home Exercise 2",
    "section": "5.3 Visualising Hot Spot & Cold Spot Areas",
    "text": "5.3 Visualising Hot Spot & Cold Spot Areas\nHere, we will plot the significant (i.e. p-values less than 0.05) hot spot and cold spot areas by using the appropriate tmap functions.\n\n2017, 20182019, 20202021, 2022\n\n\n\n\nClick to view the code\ntmap_mode(\"plot\")\n\nHCSA_2017_sig &lt;- HCSA_2017  %&gt;%\n  filter(p_sim &lt; 0.05)\nHCSA_2018_sig &lt;- HCSA_2018  %&gt;%\n  filter(p_sim &lt; 0.05)\n\nmap1&lt;- tm_shape(HCSA_2017) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_shape(HCSA_2017_sig) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.4) +\n  tm_layout(main.title = \"HCSA of 2017\",\n            main.title.size = 0.8)\nmap2&lt;- tm_shape(HCSA_2018) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_shape(HCSA_2018_sig) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.4) +\n  tm_layout(main.title = \"HCSA of 2018\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\nHCSA_2017_sig %&gt;%\n  select(gi_star, province)\n\nSimple feature collection with 5 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 99.01629 ymin: 13.4252 xmax: 101.11 ymax: 17.74368\nGeodetic CRS:  WGS 84\n# A tibble: 5 × 3\n  gi_star province                                                      geometry\n    &lt;dbl&gt; &lt;chr&gt;                                               &lt;MULTIPOLYGON [°]&gt;\n1    4.42 SAMUT PRAKAN   (((100.7306 13.71713, 100.7307 13.71681, 100.7313 13.7…\n2   -1.61 NAKHON SAWAN   (((100.0266 16.189, 100.0267 16.18889, 100.0268 16.188…\n3   -1.38 KAMPHAENG PHET (((99.48875 16.91044, 99.48883 16.91016, 99.48884 16.9…\n4   -1.42 PHITSANULOK    (((101.0033 17.7334, 101.0037 17.73339, 101.0043 17.73…\n5    3.14 SAMUT SAKHON   (((100.3091 13.7217, 100.3091 13.72169, 100.3096 13.72…\n\nHCSA_2018_sig %&gt;%\n  select(gi_star, province)\n\nSimple feature collection with 4 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 100.0267 ymin: 13.4252 xmax: 103.4129 ymax: 18.30525\nGeodetic CRS:  WGS 84\n# A tibble: 4 × 3\n  gi_star province                                                      geometry\n    &lt;dbl&gt; &lt;chr&gt;                                               &lt;MULTIPOLYGON [°]&gt;\n1    4.30 SAMUT PRAKAN (((100.7306 13.71713, 100.7307 13.71681, 100.7313 13.716…\n2    3.01 NONTHABURI   (((100.3415 14.10079, 100.3415 14.10001, 100.3415 14.099…\n3   -1.00 NONG KHAI    (((103.2985 18.29698, 103.2984 18.29697, 103.2982 18.296…\n4    3.03 SAMUT SAKHON (((100.3091 13.7217, 100.3091 13.72169, 100.3096 13.7217…\n\n\n\n\n\n\nClick to view the code\ntmap_mode(\"plot\")\n\nHCSA_2019_sig &lt;- HCSA_2019  %&gt;%\n  filter(p_sim &lt; 0.05)\nHCSA_2020_sig &lt;- HCSA_2020  %&gt;%\n  filter(p_sim &lt; 0.05)\n\nmap1&lt;- tm_shape(HCSA_2019) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_shape(HCSA_2019_sig) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.4) +\n  tm_layout(main.title = \"HCSA of 2019\",\n            main.title.size = 0.8)\nmap2&lt;- tm_shape(HCSA_2020) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_shape(HCSA_2020_sig) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.4) +\n  tm_layout(main.title = \"HCSA of 2020\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\nHCSA_2019_sig %&gt;%\n  select(gi_star, province)\n\nSimple feature collection with 4 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 100.0267 ymin: 13.4252 xmax: 103.4129 ymax: 18.30525\nGeodetic CRS:  WGS 84\n# A tibble: 4 × 3\n  gi_star province                                                      geometry\n    &lt;dbl&gt; &lt;chr&gt;                                               &lt;MULTIPOLYGON [°]&gt;\n1    3.81 SAMUT PRAKAN (((100.7306 13.71713, 100.7307 13.71681, 100.7313 13.716…\n2    2.63 NONTHABURI   (((100.3415 14.10079, 100.3415 14.10001, 100.3415 14.099…\n3   -1.28 NONG KHAI    (((103.2985 18.29698, 103.2984 18.29697, 103.2982 18.296…\n4    2.14 SAMUT SAKHON (((100.3091 13.7217, 100.3091 13.72169, 100.3096 13.7217…\n\nHCSA_2020_sig %&gt;%\n  select(gi_star, province)\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 100.4445 ymin: 13.47842 xmax: 100.9639 ymax: 13.71819\nGeodetic CRS:  WGS 84\n# A tibble: 1 × 3\n  gi_star province                                                      geometry\n    &lt;dbl&gt; &lt;chr&gt;                                               &lt;MULTIPOLYGON [°]&gt;\n1    2.98 SAMUT PRAKAN (((100.7306 13.71713, 100.7307 13.71681, 100.7313 13.716…\n\n\n\n\n\n\nClick to view the code\ntmap_mode(\"plot\")\n\nHCSA_2021_sig &lt;- HCSA_2021  %&gt;%\n  filter(p_sim &lt; 0.05)\nHCSA_2022_sig &lt;- HCSA_2022  %&gt;%\n  filter(p_sim &lt; 0.05)\n\nmap1&lt;- tm_shape(HCSA_2021) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_shape(HCSA_2021_sig) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.4) +\n  tm_layout(main.title = \"HCSA of 2021\",\n            main.title.size = 0.8)\nmap2&lt;- tm_shape(HCSA_2022) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_shape(HCSA_2022_sig) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.4) +\n  tm_layout(main.title = \"HCSA of 2022\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\nHCSA_2021_sig %&gt;%\n  select(gi_star, province)\n\nSimple feature collection with 10 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 98.18107 ymin: 12.56321 xmax: 100.8335 ymax: 16.91058\nGeodetic CRS:  WGS 84\n# A tibble: 10 × 3\n   gi_star province                                                     geometry\n     &lt;dbl&gt; &lt;chr&gt;                                              &lt;MULTIPOLYGON [°]&gt;\n 1   -1.99 SING BURI       (((100.3691 15.0894, 100.3697 15.0891, 100.3708 15.0…\n 2   -2.00 CHAI NAT        (((100.1199 15.41243, 100.121 15.41234, 100.1229 15.…\n 3   -2.39 NAKHON SAWAN    (((100.0266 16.189, 100.0267 16.18889, 100.0268 16.1…\n 4   -2.11 UTHAI THANI     (((99.13905 15.79655, 99.13918 15.79652, 99.13965 15…\n 5   -2.02 KAMPHAENG PHET  (((99.48875 16.91044, 99.48883 16.91016, 99.48884 16…\n 6   -2.17 RATCHABURI      (((99.8821 13.94977, 99.88218 13.94976, 99.88248 13.…\n 7   -2.02 KANCHANABURI    (((98.58631 15.65465, 98.58662 15.65384, 98.58697 15…\n 8   -2.41 SUPHAN BURI     (((99.37118 15.05073, 99.37454 15.0495, 99.3762 15.0…\n 9   -1.94 SAMUT SONGKHRAM (((100.0116 13.51359, 100.0117 13.51359, 100.0117 13…\n10   -1.73 PHETCHABURI     (((99.75869 13.34249, 99.75876 13.34248, 99.75883 13…\n\nHCSA_2022_sig %&gt;%\n  select(gi_star, province)\n\nSimple feature collection with 6 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 99.01629 ymin: 15.40586 xmax: 104.2406 ymax: 18.08639\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 3\n  gi_star province                                                      geometry\n    &lt;dbl&gt; &lt;chr&gt;                                               &lt;MULTIPOLYGON [°]&gt;\n1    2.25 NONG BUA LAM PHU (((102.2866 17.69207, 102.2867 17.69206, 102.2867 17…\n2    2.80 KHON KAEN        (((102.7072 17.08713, 102.708 17.087, 102.7096 17.08…\n3    2.90 UDON THANI       (((102.0581 18.0862, 102.0583 18.08599, 102.059 18.0…\n4    2.29 MAHA SARAKHAM    (((103.1562 16.6425, 103.1567 16.64236, 103.1568 16.…\n5    3.42 KALASIN          (((103.584 17.09981, 103.5845 17.09973, 103.5846 17.…\n6   -2.38 KAMPHAENG PHET   (((99.48875 16.91044, 99.48883 16.91016, 99.48884 16…\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nBetween 2017 - 2019, we can observe that Samut Prakan and Samut Sakhon are both hotspots of drug uses, with Samut Prakan continuing this trend even in 2020.\nBesides that however, there are no other hotspots or coldspots identified in 2020, showing that there are no significant data for the rest of the provinces.\nIn 2021, we can observe a large area of coldspots in the west and interestingly, this aligns with the observations we have made in the LISA map where this area was observed as a Low-Low cluster.\nIn 2022, we can observe a hotspot area spanning several provinces in the east of Thailand. This is a trend that was not observed in the previous years, showing that the concentrations of drug use has changed significantly from the previous years."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#interesting-observations",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#interesting-observations",
    "title": "Take Home Exercise 2",
    "section": "5.4 Interesting Observations",
    "text": "5.4 Interesting Observations\n\n5.4.1 Seemingly Contradictory Results in LISA map & HCSA for 2017\n\n\nClick to view the code\ntmap_mode(\"plot\")\n\nmap1&lt;-tm_shape(lisa_2017) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig_2017) +\n  tm_fill(\"mean\", palette = brewer.pal(4, \"Set1\")) +  # Change the color palette here\n  tm_borders(alpha = 0.4) +\n  tm_layout(main.title = \"Lisa Map for 2017\", main.title.size = 1.2, main.title.position = \"center\", legend.text.size = 1.2)\n\nmap2&lt;- tm_shape(HCSA_2017) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_shape(HCSA_2017_sig) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.4) +\n  tm_layout(main.title = \"HCSA of 2017\",\n            main.title.size = 0.8)\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the LISA map, we can observe that Samut Prakan and Samut Sakhon (the green parts) were identified as Low-High clusters, meaning areas with low drug uses surrounded by areas with high drug uses. However, when observing the HCSA map for the same year, the same provinces were however identified as “hotspots” which could possibly seem contradictory.\nHowever, these two methods, though related, measure different aspects of spatial patterns and their interpretation varies. LISA measures local spatial autocorrelation whereas the HCSA method identifies hot and cold spots by assessing the concentration of high or low values within an area, considering the values of the area itself and those of its neighbors.\nWhy the Area is a Low-High Cluster in LISA but Has a High Gi* Value:\nLISA vs HCSA:\n\nIn LISA, the area is classified as low-high because its value is lower than its neighbors. Even though its neighbors have higher values, the area itself might be somewhat lower in comparison, creating a spatial outlier (a low value in a region of high values).\nGi* (used in HCSA), on the other hand, assesses the collective clustering of values. Even if the area has a lower value compared to its neighbors, the fact that it is part of a larger high-value cluster (with its neighbors) results in a high Gi* score.\n\nTherefore, in this case, these areas are low-high cluster in the LISA map because they have a lower value than its neighbors, but still contributes to a high-value hotspot in the Gi* analysis, leading to a high positive Gi* score."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "title": "Take-Home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "",
    "text": "Since early 2021, Myanmar has been engulfed in a brutal civil war that erupted following the military coup on February 1, 2021, which ousted the democratically elected government led by Aung San Suu Kyi. The coup ignited widespread protests and a civil disobedience movement across the country, escalating into armed resistance as various ethnic armed groups and newly formed militias began confronting the military junta. The conflict has led to severe humanitarian crises, including widespread violence, displacement, and human rights abuses. The military’s response has been marked by extreme repression, including airstrikes and targeted attacks on civilian populations, exacerbating the suffering of ordinary Myanmar citizens. Despite international condemnation and calls for a return to democratic governance, the violence continues, deepening the political instability and socio-economic challenges faced by Myanmar.\nIn light of this ongoing crisis, I will be conducting a detailed geospatial analysis of the conflict events in Myanmar. This analysis aims to map and evaluate the spatial distribution and intensity of conflict incidents from 2021 to 2024. By utilizing geospatial techniques and spatial data, I will examine patterns of violence, identify hotspots of conflict, and assess the impact on civilian areas. This approach will provide valuable insights into the spatial dynamics of the conflict, helping to inform humanitarian responses and policy decisions.\n\n\n\nWe will be focusing on 4 event types: Battles, Explosion/Remote violence, Strategic developments, and Violence against civilians.\n\nUsing appropriate function of sf and tidyverse packages, import and transform the downloaded armed conflict data and administrative boundary data into sf tibble data.frames.\nUsing the geospatial data sets prepared, derive quarterly KDE layers.\nUsing the geospatial data sets prepared, perform 2nd-Order Spatial Point Patterns Analysis.\nUsing the geospatial data sets prepared, derive quarterly spatio-temporal KDE layers.\nUsing the geospatial data sets prepared, perform 2nd-Order Spatio-temporal Point Patterns Analysis.\nUsing appropriate tmap functions, display the KDE and Spatio-temporal KDE layers on openstreetmap of Myanmar.\nDescribe the spatial patterns revealed by the KDE and Spatio-temporal KDE maps.1.1 The Data\n\n\n\n\nBefore we start off, we will have to import the necessary packages required for us to conduct our analysis.\nWe will be using the following packages:\n\nsf: Manages spatial vector data, enabling operations like spatial joins, buffering, and transformations for points, lines, and polygons.\nraster: Handles raster data, allowing for operations such as raster calculations, resampling, and visualization of spatial grids (e.g., elevation or satellite images).\nspNetwork: Analyzes spatial networks by modeling connectivity and movement within networks like road systems or utility grids.\ntmap: Creates and customizes thematic maps for spatial data visualization, including static and interactive maps with various map elements.\ntidyverse: A suite of packages for data manipulation (dplyr), visualization (ggplot2), and tidying (tidyr), facilitating a streamlined workflow for data analysis.\nRColorBrewer: Provides a range of color palettes for effective and aesthetically pleasing data visualization, including options for categorical, sequential, and diverging data.\nspatstat: Conducts spatial statistics and analysis, including point pattern analysis and spatial simulations, to study spatial distributions and interactions.\nsparr: Facilitates the analysis of spatial point patterns and spatial autoregressive models. It includes functions for fitting and analyzing spatial point processes, particularly useful for examining spatial dependencies and interactions in point pattern data.\nstpp : Provides tools for spatio-temporal point pattern analysis, including methods for modeling and analyzing the interactions and distributions of points over both space and time.\n\n\npacman::p_load(sf, raster, spNetwork, tmap, tidyverse, RColorBrewer, spatstat, sparr, stpp,lgcp)\n\n\n\n\nFor the purpose of this study, we will be using the following datasets. Particularly, I will be focusing on the quarterly armed conflict events from January 2021 until June 2024.\n\nArmed Conflict Location & Event Data of Myanmar between Jan 2021 to Jun 2024 from Armed Conflict Location & Event Data (ACLED), an independent, impartial, international non-profit organization collecting data on violent conflict and protest in all countries and territories in the world.\nMyanmar State and Region Boundaries with Sub-region from Myanmar Information Management Unit, MIMU\nOpenStreetMap dataset, which is an open-sourced geospatial dataset including shapefiles of important layers including road networks, forests, building footprints and many other points of interest.\nMyanmar District Boundaries from Myanmar Information Management Unit, MIMU"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#introduction",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#introduction",
    "title": "Take-Home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "",
    "text": "Since early 2021, Myanmar has been engulfed in a brutal civil war that erupted following the military coup on February 1, 2021, which ousted the democratically elected government led by Aung San Suu Kyi. The coup ignited widespread protests and a civil disobedience movement across the country, escalating into armed resistance as various ethnic armed groups and newly formed militias began confronting the military junta. The conflict has led to severe humanitarian crises, including widespread violence, displacement, and human rights abuses. The military’s response has been marked by extreme repression, including airstrikes and targeted attacks on civilian populations, exacerbating the suffering of ordinary Myanmar citizens. Despite international condemnation and calls for a return to democratic governance, the violence continues, deepening the political instability and socio-economic challenges faced by Myanmar.\nIn light of this ongoing crisis, I will be conducting a detailed geospatial analysis of the conflict events in Myanmar. This analysis aims to map and evaluate the spatial distribution and intensity of conflict incidents from 2021 to 2024. By utilizing geospatial techniques and spatial data, I will examine patterns of violence, identify hotspots of conflict, and assess the impact on civilian areas. This approach will provide valuable insights into the spatial dynamics of the conflict, helping to inform humanitarian responses and policy decisions."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#goal",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#goal",
    "title": "Take-Home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "",
    "text": "We will be focusing on 4 event types: Battles, Explosion/Remote violence, Strategic developments, and Violence against civilians.\n\nUsing appropriate function of sf and tidyverse packages, import and transform the downloaded armed conflict data and administrative boundary data into sf tibble data.frames.\nUsing the geospatial data sets prepared, derive quarterly KDE layers.\nUsing the geospatial data sets prepared, perform 2nd-Order Spatial Point Patterns Analysis.\nUsing the geospatial data sets prepared, derive quarterly spatio-temporal KDE layers.\nUsing the geospatial data sets prepared, perform 2nd-Order Spatio-temporal Point Patterns Analysis.\nUsing appropriate tmap functions, display the KDE and Spatio-temporal KDE layers on openstreetmap of Myanmar.\nDescribe the spatial patterns revealed by the KDE and Spatio-temporal KDE maps.1.1 The Data"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#importing-of-packages",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#importing-of-packages",
    "title": "Take-Home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "",
    "text": "Before we start off, we will have to import the necessary packages required for us to conduct our analysis.\nWe will be using the following packages:\n\nsf: Manages spatial vector data, enabling operations like spatial joins, buffering, and transformations for points, lines, and polygons.\nraster: Handles raster data, allowing for operations such as raster calculations, resampling, and visualization of spatial grids (e.g., elevation or satellite images).\nspNetwork: Analyzes spatial networks by modeling connectivity and movement within networks like road systems or utility grids.\ntmap: Creates and customizes thematic maps for spatial data visualization, including static and interactive maps with various map elements.\ntidyverse: A suite of packages for data manipulation (dplyr), visualization (ggplot2), and tidying (tidyr), facilitating a streamlined workflow for data analysis.\nRColorBrewer: Provides a range of color palettes for effective and aesthetically pleasing data visualization, including options for categorical, sequential, and diverging data.\nspatstat: Conducts spatial statistics and analysis, including point pattern analysis and spatial simulations, to study spatial distributions and interactions.\nsparr: Facilitates the analysis of spatial point patterns and spatial autoregressive models. It includes functions for fitting and analyzing spatial point processes, particularly useful for examining spatial dependencies and interactions in point pattern data.\nstpp : Provides tools for spatio-temporal point pattern analysis, including methods for modeling and analyzing the interactions and distributions of points over both space and time.\n\n\npacman::p_load(sf, raster, spNetwork, tmap, tidyverse, RColorBrewer, spatstat, sparr, stpp,lgcp)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#the-data",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#the-data",
    "title": "Take-Home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "",
    "text": "For the purpose of this study, we will be using the following datasets. Particularly, I will be focusing on the quarterly armed conflict events from January 2021 until June 2024.\n\nArmed Conflict Location & Event Data of Myanmar between Jan 2021 to Jun 2024 from Armed Conflict Location & Event Data (ACLED), an independent, impartial, international non-profit organization collecting data on violent conflict and protest in all countries and territories in the world.\nMyanmar State and Region Boundaries with Sub-region from Myanmar Information Management Unit, MIMU\nOpenStreetMap dataset, which is an open-sourced geospatial dataset including shapefiles of important layers including road networks, forests, building footprints and many other points of interest.\nMyanmar District Boundaries from Myanmar Information Management Unit, MIMU"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#acled-data",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#acled-data",
    "title": "Take-Home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "2.1 ACLED data",
    "text": "2.1 ACLED data\n\n2.1.1 Importing Data\nThe below chunk of code is used to import ACLED conflict data from a CSV file and convert it into a geospatial data frame using longitude and latitude as coordinates.\nIn order to perform geoprocessing using two geospatial data, we need to ensure that both geospatial data are projected using similar coordinate system which is why in this case we will project it to WGS84 with the crs code of 32647 using st_transform.\nSince the column event_date was stored as characters, dmy() is also used to format the event_date column into a standardized date format for further analysis.\n\nst_as_sf is used to convert a data frame or other tabular data (like from CSVs, data frames, or tibbles) into a simple features (sf) object.\n\n\nacled_sf &lt;- read_csv(\"data/aspatial/2021-01-01-2024-06-30-Myanmar.csv\") %&gt;% \n  st_as_sf(coords = c(\n    \"longitude\", \"latitude\"), crs = 4326) %&gt;% \n  st_transform(crs= 32647)%&gt;%\n  mutate(event_date = dmy(event_date))\n\nNow, let us check the CRS again by using the code chunk below.\n\nst_crs(acled_sf)\n\nCoordinate Reference System:\n  User input: EPSG:32647 \n  wkt:\nPROJCRS[\"WGS 84 / UTM zone 47N\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 47N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",99,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"Between 96°E and 102°E, northern hemisphere between equator and 84°N, onshore and offshore. China. Indonesia. Laos. Malaysia - West Malaysia. Mongolia. Myanmar (Burma). Russian Federation. Thailand.\"],\n        BBOX[0,96,84,102]],\n    ID[\"EPSG\",32647]]\n\n\n\n\n2.1.2 Data Pruning and Structuring\n\n2.1.2.1 Extracting Data for the 4 Main Event Types\nSince the data we have encapsulates all 6 types of events and we are only focusing on the 4 event types that I have indicated above, we will only be extracting the data related to that.\n\nacled_sf &lt;- acled_sf %&gt;%\n  filter(event_type %in% c(\"Battles\", \"Strategic developments\", \"Violence against civilians\", \"Explosions/Remote violence\"))\n\nUsing unique(), we can check that only rows that fall under the 4 event types have been kept.\n\nunique(acled_sf$event_type)\n\n[1] \"Battles\"                    \"Strategic developments\"    \n[3] \"Violence against civilians\" \"Explosions/Remote violence\"\n\n\n\n\n2.1.2.2 Data Transformation for Quarterly Analysis\nSince we will be analyzing it by quarters, let’s use mutate() to extract the information. Using quarter() from the lubridate package, we can derive the quarters from the event_date.\n\nacled_sf &lt;- acled_sf %&gt;% mutate(quarter = quarter(event_date))\n\nUsing colnames() we can see that the new column quarter is added.\n\ncolnames(acled_sf) \n\n [1] \"event_id_cnty\"      \"event_date\"         \"year\"              \n [4] \"time_precision\"     \"disorder_type\"      \"event_type\"        \n [7] \"sub_event_type\"     \"actor1\"             \"assoc_actor_1\"     \n[10] \"inter1\"             \"actor2\"             \"assoc_actor_2\"     \n[13] \"inter2\"             \"interaction\"        \"civilian_targeting\"\n[16] \"iso\"                \"region\"             \"country\"           \n[19] \"admin1\"             \"admin2\"             \"admin3\"            \n[22] \"location\"           \"geo_precision\"      \"source\"            \n[25] \"source_scale\"       \"notes\"              \"fatalities\"        \n[28] \"tags\"               \"timestamp\"          \"geometry\"          \n[31] \"quarter\"           \n\n\n\n\n2.1.2.3 Removal of Redundant Columns\nTo enhance the efficiency of our dataset, we will also remove redundant columns. This practice reduces memory usage and processing time while simplifying the analysis. By focusing on only the relevant data, we minimize complexity and ensure clearer, more focused results. Examples of the columns we will be removing are: time_precision, inter1 etc. With this, we are able to reduce to a total of 20 columns.\n\n# Define columns to be removed\ncolumns_to_remove &lt;- c(\"time_precision\", \"inter1\", \"inter2\", \"iso\", \"source\", \"source_scale\", \"notes\", \"tags\", \"region\", \"geo_precision\", \"source_scale\",\"country\")\n\n# Remove columns only if they exist in the dataframe\nacled_sf &lt;- acled_sf %&gt;%\n  dplyr::select(-all_of(columns_to_remove[columns_to_remove %in% names(acled_sf)]))\n\nThis is an overview of all the columns left:\n\ncolnames(acled_sf)\n\n [1] \"event_id_cnty\"      \"event_date\"         \"year\"              \n [4] \"disorder_type\"      \"event_type\"         \"sub_event_type\"    \n [7] \"actor1\"             \"assoc_actor_1\"      \"actor2\"            \n[10] \"assoc_actor_2\"      \"interaction\"        \"civilian_targeting\"\n[13] \"admin1\"             \"admin2\"             \"admin3\"            \n[16] \"location\"           \"fatalities\"         \"timestamp\"         \n[19] \"geometry\"           \"quarter\"           \n\n\n\n\n2.1.2.4 Grouping of Data By Year, Quarter, Event Type\nThis chunk of code groups the ACLED Data by quarter, event and year, allowing us to better manage and access the data later on.\n\ngrped_acled_sf &lt;- acled_sf %&gt;%\n  group_by(year, quarter, event_type) %&gt;%\n  summarize(event_count = n(), .groups = 'drop')"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#myanmar-boundary-and-sub-region-dataset-from-mimu",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#myanmar-boundary-and-sub-region-dataset-from-mimu",
    "title": "Take-Home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "2.2 Myanmar Boundary and Sub-Region Dataset from MIMU",
    "text": "2.2 Myanmar Boundary and Sub-Region Dataset from MIMU\nThe code chunk below uses st_read() function of sf package to import mmr_polbnda2_adm1_250k_mimu_1 shapefile into R as a polygon feature data frame.\n\nmmrsr_sf &lt;- st_read(dsn=\"data/geospatial/mmr_polbnda2_adm1_250k_mimu_1/\", \n                   layer=\"mmr_polbnda2_adm1_250k_mimu_1\")\n\nReading layer `mmr_polbnda2_adm1_250k_mimu_1' from data source \n  `/Users/georgiaxng/georgiaxng/is415-handson/Take-home_Ex/Take-home_Ex01/data/geospatial/mmr_polbnda2_adm1_250k_mimu_1' \n  using driver `ESRI Shapefile'\nSimple feature collection with 18 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 101.17 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\nAs shown below, currently the data is in geographic coordinate system (latitude/longitude), we will need to transform it to a projected CRS before proceeding.\n\nst_crs(mmrsr_sf)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ID[\"EPSG\",6326]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433],\n        ID[\"EPSG\",8901]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic longitude\",east,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic latitude\",north,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]]]\n\n\n\nmmrsr_sf &lt;- st_transform(mmrsr_sf, crs = 32647)\nst_crs(mmrsr_sf)\n\nCoordinate Reference System:\n  User input: EPSG:32647 \n  wkt:\nPROJCRS[\"WGS 84 / UTM zone 47N\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 47N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",99,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"Between 96°E and 102°E, northern hemisphere between equator and 84°N, onshore and offshore. China. Indonesia. Laos. Malaysia - West Malaysia. Mongolia. Myanmar (Burma). Russian Federation. Thailand.\"],\n        BBOX[0,96,84,102]],\n    ID[\"EPSG\",32647]]\n\n\nTo check that all the spatial objects are valid, st_is_valid() is utilized. As shown below, all of the objects are valid.\n\nst_is_valid(mmrsr_sf)\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[16] TRUE TRUE TRUE\n\n\nThis is a brief overview of how the map of Myanmar looks like.\n\nplot(mmrsr_sf)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#mapping-the-geospatial-data-sets",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#mapping-the-geospatial-data-sets",
    "title": "Take-Home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "2.3 Mapping the Geospatial data sets",
    "text": "2.3 Mapping the Geospatial data sets\nNow, let’s plot the spatial map to gain an initial understanding of the geographic distribution of all of the 4 conflict events by year. As observed, with such a large volume of data, identifying patterns through visual inspection alone can be challenging. This highlights the importance of conducting further analysis to uncover trends and achieve a deeper understanding of the spatial dynamics of these events.\nFrom a brief examination of the map, it’s evident that the region south of Sagaing exhibits the highest concentration of conflicts overall. This area shows a notably higher density of conflict events compared to other regions, marking it as a significant hotspot. However, to fully grasp these patterns and their implications, additional analysis is needed.\n\n2021202220232024\n\n\n\n# Filter data for 2021\nacled_2021 &lt;- acled_sf %&gt;% filter(year == 2021)\n\n# Plot map for 2021\ntm_shape(mmrsr_sf) +\n  tm_polygons() + \n  tm_shape(acled_2021) + \n  tm_dots() +\n  tm_layout(main.title = \"ACLED Conflict Events in 2021\", main.title.size = 1.2)\n\n\n\n\n\n\n\n\n\n\n\n# Filter data for 2022\nacled_2022 &lt;- acled_sf %&gt;% filter(year == 2022)\n\n# Plot map for 2022\ntm_shape(mmrsr_sf) +\n  tm_polygons() + \n  tm_shape(acled_2022) + \n  tm_dots() +\n  tm_layout(main.title = \"ACLED Conflict Events in 2022\", main.title.size = 1.2)\n\n\n\n\n\n\n\n\n\n\n\n# Filter data for 2023\nacled_2023 &lt;- acled_sf %&gt;% filter(year == 2023)\n\n# Plot map for 2023\ntm_shape(mmrsr_sf) +\n  tm_polygons() + \n  tm_shape(acled_2023) + \n  tm_dots() +\n  tm_layout(main.title = \"ACLED Conflict Events in 2023\", main.title.size = 1.2)\n\n\n\n\n\n\n\n\n\n\n\n# Filter data for 2024\nacled_2024 &lt;- acled_sf %&gt;% filter(year == 2024)\n\n# Plot map for 2024\ntm_shape(mmrsr_sf) +\n  tm_polygons() + \n  tm_shape(acled_2024) + \n  tm_dots() +\n  tm_layout(main.title = \"ACLED Conflict Events in 2024\", main.title.size = 1.2)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#myanmar-sub-regions-for-reference-and-context",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#myanmar-sub-regions-for-reference-and-context",
    "title": "Take-Home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "2.4 Myanmar Sub-regions: For Reference and Context",
    "text": "2.4 Myanmar Sub-regions: For Reference and Context\nFor reference to the different sub-regions in Myanmar, we can refer to the labelled map plotted below:\n\nnum_colors &lt;- length(unique(mmrsr_sf$ST))\ncolors &lt;- brewer.pal(n = num_colors, name = \"Set3\")\n\ntm_shape(mmrsr_sf) +\n  tm_polygons(col = \"ST\", palette = colors) +  # Apply color palette to polygons\n  tm_text(\"ST\", size = 1, col = \"black\", bg.color = \"white\", just = c(\"center\", \"center\"),  xmod = 0, ymod = 0) + tm_layout(main.title = \"Myanmar\",\n            main.title.position = \"center\",\n            main.title.size = 1.6,\n            legend.outside = TRUE,\n            frame = TRUE)+\n    tm_legend(title = \"Sub-regions\")  # Set custom legend title"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#myanmar-district-data",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#myanmar-district-data",
    "title": "Take-Home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "2.5 Myanmar District Data",
    "text": "2.5 Myanmar District Data\nThis chunk of code imports the geospatial data for the districts of Myanmar from MIMU.\n\nmmrsr_district_sf &lt;- st_read(dsn=\"data/geospatial/mmr_district_mimu/\", \n                   layer=\"mmr_polbnda_adm2_250k_mimu\")\n\nReading layer `mmr_polbnda_adm2_250k_mimu' from data source \n  `/Users/georgiaxng/georgiaxng/is415-handson/Take-home_Ex/Take-home_Ex01/data/geospatial/mmr_district_mimu' \n  using driver `ESRI Shapefile'\nSimple feature collection with 80 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.1721 ymin: 9.696844 xmax: 101.17 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\nmmrsr_district_sf &lt;- st_transform(mmrsr_district_sf, crs = 32647)\n\n\nplot(mmrsr_district_sf)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#setting-seed",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#setting-seed",
    "title": "Take-Home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "2.6 Setting Seed",
    "text": "2.6 Setting Seed\nTo ensure the reproducibility, we will be using a fixed seed for our randomisation.\n\nset.seed(123456)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#creating-owin-object",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#creating-owin-object",
    "title": "Take-Home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "3.1 Creating owin object",
    "text": "3.1 Creating owin object\nTo improve the accuracy of our spatial analysis, we will exclude the smaller islands that are too minor to contribute meaningfully to the Kernel Density Estimation (KDE). We will focus on the main island by merging the geometry and selecting the relevant polygon.\nIn this code snippet, we combine all intersecting geometries in the mmrsr_sf object into a single polygon using st_union(). We then use st_cast() to ensure that the result is a polygon type. From this merged polygon, we select the row with the highest number of features, which typically corresponds to the main island or the most prominent feature in the dataset.\nThis selected polygon is then converted to a window object using as.owin(), which allows us to use it in spatial analyses, such as Kernel Density Estimation (KDE).\n\nmerged_mmr &lt;- st_union(mmrsr_sf) %&gt;%\n    st_cast(\"POLYGON\")\nmerged_mmr &lt;- merged_mmr[c(9)]\n\nmmr_owin &lt;- as.owin(merged_mmr)\n\nHere is the plot of the main island, without its outer islands.\n\nplot(mmr_owin)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#selection-of-sample-data",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#selection-of-sample-data",
    "title": "Take-Home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "3.2 Selection of Sample Data",
    "text": "3.2 Selection of Sample Data\nTo choose a representative sample, we will first calculate the median number of data points across different groupings, such as year, quarter, and event type. This will help us understand the typical size of our data subsets. Using this median value, we will identify the subset of data with an event_count closest to this median.\nBy selecting a sample that aligns with this typical value, we ensure that our sample accurately reflects the general distribution of the dataset. We will then use this representative sample to determine the optimal parameters for Kernel Density Estimation (KDE), such as bandwidth and kernel type. This approach will help us achieve more reliable and meaningful results in our spatial analysis.\nHere is the R code to select the representative sample. In this case, the resulting sample data we have arrived at is for “Battles” in the second quarter of 2023.\n\n# Calculate mean and median of event counts\nmedian_event_count &lt;- median(grped_acled_sf$event_count)\n\n# Find the row with event_count closest to the median\nsample_acled_sf &lt;- grped_acled_sf %&gt;%\n  mutate(distance_to_median = abs(event_count - median_event_count)) %&gt;%\n  arrange(distance_to_median) %&gt;%\n  slice(1)\n\n# Print the closest row(s)\nsample_acled_sf\n\nSimple feature collection with 1 feature and 5 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: -174035.7 ymin: 1103500 xmax: 518300.4 ymax: 3006373\nProjected CRS: WGS 84 / UTM zone 47N\n# A tibble: 1 × 6\n   year quarter event_type event_count                                  geometry\n  &lt;dbl&gt;   &lt;int&gt; &lt;chr&gt;            &lt;int&gt;                          &lt;MULTIPOINT [m]&gt;\n1  2023       2 Battles            814 ((-174035.7 2284958), (-152179.1 2277258…\n# ℹ 1 more variable: distance_to_median &lt;dbl&gt;\n\n\nHere, we converted the representative sample data to ppp format and checked for duplicates. In this case, no duplicates were found, confirming the integrity of our sample for further KDE analysis.\n\nsample_acled_ppp &lt;- as.ppp(st_coordinates(sample_acled_sf),st_bbox(sample_acled_sf))\nany(duplicated(sample_acled_ppp))\n\n[1] FALSE\n\nsample_acled_ppp &lt;- sample_acled_ppp[mmr_owin]\n\nAn overview of the point pattern plot for the sample data.\n\nplot(sample_acled_ppp, pch = 16, cex = 0.8)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#determining-the-optimal-bandwidth-sigma",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#determining-the-optimal-bandwidth-sigma",
    "title": "Take-Home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "3.3 Determining the Optimal Bandwidth & Sigma",
    "text": "3.3 Determining the Optimal Bandwidth & Sigma\n\n3.3.1 Understanding Sigma & Kernel in Kernel Density Estimation (KDE)\nKernel:\nThe kernel in Kernel Density Estimation (KDE) is a smoothing function that estimates the probability density function of a dataset. The choice of kernel function affects how the smoothing is applied to each data point. Common kernels include:\n\nGaussian Kernel: Assumes a normal distribution around each data point, providing a smooth, continuous density estimate.\nEpanechnikov Kernel: Parabolic in shape, it minimizes mean squared error but is less smooth than Gaussian.\nUniform Kernel: Assumes constant density within a fixed distance around each data point, simpler but less smooth.\nTriangular Kernel: Offers a clear density representation with sharp edges but less smoothness.\n\nThe choice of kernel affects the density estimate’s smoothness and shape, with the Gaussian kernel being a versatile and commonly used option.\nSigma:\nThe sigma argument in the density() function represents the bandwidth of the kernel in KDE. It controls the level of smoothing applied to the density estimate:\n\nWhat Sigma Represents: Sigma defines the standard deviation of the kernel function, determining the width of the “smoothing window” around each data point.\nEffect on KDE:\n\nSmaller Sigma: Leads to less smoothing, producing a more sensitive and detailed density plot, but can highlight noise.\nLarger Sigma: Results in greater smoothing, providing a broader view of density but might obscure finer details.\n\nChoosing Sigma: An appropriate sigma balances detail and smoothness. Methods like bw.diggle, bw.ppl, and bw.scott can help determine the optimal sigma value.\n\nIn summary, the kernel determines the density estimate’s shape, while sigma controls the smoothing level. Both are crucial for accurately reflecting the spatial distribution of data, and selecting the right parameters is essential for meaningful KDE results.\n\n\n3.3.2 Computing Default Kernel Density Estimation\nFirst, since the original data is in meters, we need to rescale it to kilometers to facilitate more meaningful spatial analysis and visualization. By converting the units to kilometers, we ensure that the scale of the Kernel Density Estimation (KDE) and subsequent analyses align with the geographical extent of the study area.\n\nsample_acled_ppp.km &lt;- rescale.ppp(sample_acled_ppp, 1000, \"km\")\n\nTo visualize the spatial distribution of conflict events, here is the Kernel Density Estimation (KDE) plot generated using the default settings, as shown in the code. While default settings can be sufficient for some analyses, the resulting KDE plot here appears oversmoothed, which may obscure finer details and potential small-scale clusters.\nThis oversmoothing effect reduces the ability to detect subtle patterns in the conflict events, highlighting the importance of adjusting parameters like bandwidth (sigma) to better capture the underlying spatial structure. Thus, to improve the accuracy and granularity of the KDE, we will need to continue experimenting with different bandwidth values and kernel functions.\n\npar(mar = c(0,1,1,1))\nkde_default_destination &lt;- density(sample_acled_ppp.km)\nplot(kde_default_destination,main = \"Default Density KDE for ACLED Myanmar 2023 Q2 'Battles'\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#experimenting-with-fixed-bandwidth",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#experimenting-with-fixed-bandwidth",
    "title": "Take-Home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "3.4 Experimenting With Fixed Bandwidth",
    "text": "3.4 Experimenting With Fixed Bandwidth\nTo effectively choose the appropriate bandwidth for Kernel Density Estimation (KDE), several automatic bandwidth selection methods can be employed. Each method has its own approach and focus, which can significantly impact the resulting density estimate. Below are brief descriptions of four common bandwidth selection methods:\n\nbw.CvL: Calculates bandwidth using cross-validation to minimize the integrated mean squared error (IMSE) of the density estimate. This method aims to balance detail and smoothness by optimizing the bandwidth based on a global error metric, which might lead to a broad smoothing effect.\nbw.scott: Applies a rule-of-thumb bandwidth based on the sample size and dimensionality of the data. This method is often effective for larger datasets, providing a more generalized estimate by assuming isotropic smoothing (uniform smoothing in all directions).\nbw.ppl: Uses a bandwidth tailored for point pattern analysis, considering the spatial distribution of data points. This approach focuses on capturing local variations and spatial patterns, resulting in a more localized smoothing effect compared to other methods.\nbw.diggle: Employs a method specifically designed for point patterns to minimize the variance of the density estimate. This results in a smaller bandwidth, leading to a finer and more detailed density plot that may capture small-scale variations but could also emphasize noise.\n\n\n3.4.1 Automatic Bandwidth Methods\nLets begin by examining the bandwidth values obtained from various automatic bandwidth selection methods using the following code snippet.\n\nbw.ppl(sample_acled_ppp.km)\n\n   sigma \n21.20514 \n\nbw.diggle(sample_acled_ppp.km)\n\n   sigma \n8.459462 \n\nbw.CvL(sample_acled_ppp.km)\n\n   sigma \n90.92206 \n\nbw.scott(sample_acled_ppp.km)\n\n  sigma.x   sigma.y \n 51.29856 133.58599 \n\n\nBased on the sigma values obtained from the automatic bandwidth calculation methods, we can draw the following inferences about the Kernel Density Estimation (KDE) for the sample data:\n\nbw.ppl: sigma = 21.21\nA smaller sigma value suggests a more localized smoothing effect. This method is designed for point pattern analysis and aims to capture spatial patterns with greater precision, highlighting finer details within the data.\nbw.diggle: sigma = 8.46\nAs the smallest sigma value, it implies a very narrow smoothing window. This method focuses on minimizing variance and tends to produce a detailed density plot. However, it may also accentuate noise or small fluctuations in the data.\nbw.CvL: sigma = 90.92\nThis large value indicates a broad smoothing window. Consequently, each data point affects a wide surrounding area, leading to a smoother KDE that may obscure fine details and small-scale variations.\nbw.scott: sigma.x = 51.30, sigma.y = 133.59\nThe method provides different bandwidths for the x and y dimensions, suggesting anisotropic smoothing. The significant difference between these values indicates varying spatial variations in different directions, with a broader smoothing effect along the y-direction.\n\n\n\n3.4.2 Plotting KDE for Different Bandwidth & Kernel\nTo better understand these differences, we will plot the KDE results using each bandwidth value and different kernels. This visual comparison will help us assess how each bandwidth setting and kernel type influences the density estimates, allowing us to determine which combination provides the most meaningful representation of our data.\nIn practice, choosing the optimal KDE bandwidth is not straightforward, as there is no one-size-fits-all approach. Many studies emphasize that determining the best bandwidth often relies on visually comparing various settings and kernel types to assess their effectiveness and select the most appropriate one for the specific data at hand.\n\n3.4.2.1 Overview of KDE Maps with Gaussian Kernel and Various Bandwidths\nHere is an overview of how in general the KDE maps generated with the respective bandwidths looks like using the Gaussian kernel.\n\nkde_diggle &lt;- density(sample_acled_ppp.km, bw.diggle(sample_acled_ppp.km))\nkde_CvL &lt;- density(sample_acled_ppp.km, bw.CvL(sample_acled_ppp.km))\nkde_scott &lt;- density(sample_acled_ppp.km, bw.scott(sample_acled_ppp.km))\nkde_ppl &lt;- density(sample_acled_ppp.km, bw.ppl(sample_acled_ppp.km))\n\npar(mar = c(1,1,1,1.5),mfrow = c(2,2))\nplot(kde_diggle,main = \"kde_diggle\")\nplot(kde_CvL,main = \"kde_CvL\")\nplot(kde_scott,main = \"kde_scott\")\nplot(kde_ppl,main = \"kde_ppl\")\n\n\n\n\n\n\n\n\n\n\n3.4.2.2 Comparing KDE Results Across Different Bandwidths and Kernels\nThe code chunk below plots different KDE plots for a various different pairings of kernels and bandwidths.\n\nbw.pplbw.digglebw.CvLbw.scott\n\n\n\npar(mfrow=c(2,2))\nplot(density(sample_acled_ppp.km, sigma=bw.ppl, edge=TRUE, kernel=\"gaussian\"), main=\"Gaussian\")\nplot(density(sample_acled_ppp.km, sigma=bw.ppl, edge=TRUE, kernel=\"epanechnikov\"), main=\"Epanechnikov\")\nplot(density(sample_acled_ppp.km, sigma=bw.ppl, edge=TRUE, kernel=\"quartic\"), main=\"Quartic\")\nplot(density(sample_acled_ppp.km, sigma=bw.ppl, edge=TRUE, kernel=\"disc\"), main=\"Disc\")\n\n\n\n\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(density(sample_acled_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\"), main=\"Gaussian\")\nplot(density(sample_acled_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"epanechnikov\"), main=\"Epanechnikov\")\nplot(density(sample_acled_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"quartic\"), main=\"Quartic\")\nplot(density(sample_acled_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"disc\"), main=\"Disc\")\n\n\n\n\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(density(sample_acled_ppp.km, sigma=bw.CvL, edge=TRUE, kernel=\"gaussian\"), main=\"Gaussian\")\nplot(density(sample_acled_ppp.km, sigma=bw.CvL, edge=TRUE, kernel=\"epanechnikov\"), main=\"Epanechnikov\")\nplot(density(sample_acled_ppp.km, sigma=bw.CvL, edge=TRUE, kernel=\"quartic\"), main=\"Quartic\")\nplot(density(sample_acled_ppp.km, sigma=bw.CvL, edge=TRUE, kernel=\"disc\"), main=\"Disc\")\n\n\n\n\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(density(sample_acled_ppp.km, sigma=bw.scott, edge=TRUE, kernel=\"gaussian\"), main=\"Gaussian\")\nplot(density(sample_acled_ppp.km, sigma=bw.scott, edge=TRUE, kernel=\"epanechnikov\"), main=\"Epanechnikov\")\nplot(density(sample_acled_ppp.km, sigma=bw.scott, edge=TRUE, kernel=\"quartic\"), main=\"Quartic\")\nplot(density(sample_acled_ppp.km, sigma=bw.scott, edge=TRUE, kernel=\"disc\"), main=\"Disc\")\n\n\n\n\n\n\n\n\n\n\n\nAfter evaluating the KDE plots, I opted for the Gaussian kernel with the bandwidth provided by bw.ppl because this method provided a more localized smoothing effect, better capturing the spatial patterns in the data. In comparison, the bw.CvL method yielded a much larger bandwidth, leading to excessive smoothing, while bw.scott showed considerable disparity between x and y dimensions, suggesting anisotropic smoothing. The bw.diggle method, though precise, resulted in very narrow smoothing that might overemphasize noise.\nDespite this, I found that the Gaussian KDE with bw.ppl still appeared to be undersmoothed. Consequently, further optimization is necessary to refine the bandwidth setting and achieve a more balanced density estimate that accurately represents the underlying spatial patterns in the data.\n\n\n\n\n3.4.3 Fine-Tuning KDE Parameters\nSince the Gaussian KDE using bw.ppl seems to be undersmoothed, we will need to adjust the sigma to achieve the desired results. To address this undersmoothing issue, we can increase the sigma value to widen the smoothing window, as demonstrated below. Through continued experimentation, we have determined these final parameters.\n\nadjusted_bw &lt;- bw.ppl(sample_acled_ppp.km) * 1.5\nkde_adjusted &lt;- density(sample_acled_ppp.km, sigma=adjusted_bw, edge=TRUE, kernel=\"gaussian\")\nplot(kde_adjusted)\n\n\n\n\n\n\n\n\nHere is a comparison between the point pattern and the KDE plot of the sample data, this comparison allows us to evaluate how effectively the KDE captures the spatial distribution of the events and highlights areas where the KDE might need further refinement.\n\npar(mar = c(1,0,1,0))\npar(mfrow=c(1,2))\nplot(sample_acled_ppp, pch = 16, cex = 0.5) \nplot(kde_adjusted)\n\n\n\n\n\n\n\n\nValue of the adjusted bandwidth:\n\nadjusted_bw\n\n   sigma \n31.80771"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#section-4",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#section-4",
    "title": "Take-Home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "4.1 2021",
    "text": "4.1 2021\n\n4.1.1 Q1\nThis chunk of code generates the KDE layers using the adjusted bandwidth we have previously counted and the gaussian kernel, plotting the resulting KDE layers by each quarter.\n\n  par(mar = c(1,0,1,0))\n  par(mfrow=c(2,2))\n  current_year = 2021\n  current_quarter = 1\n  quarter_data &lt;- grped_acled_sf %&gt;% filter(year == current_year, quarter == current_quarter) \n  # Loop through each event\n  for (i in seq_len(nrow(quarter_data))) {\n    \n    filtered_sf &lt;- quarter_data[i, ]\n\n    # Extract the current year, quarter, and event type\n    current_event &lt;- filtered_sf$event_type\n    \n    # Convert to ppp object\n    filtered_ppp &lt;- as.ppp(st_coordinates(filtered_sf), st_bbox(filtered_sf))\n    \n    if(any(duplicated(filtered_ppp))){\n      rjitter(filtered_ppp, retry=TRUE, nsim=1, drop=TRUE)\n    }\n    filtered_ppp &lt;- filtered_ppp[mmr_owin]\n    # Rescale to kilometres\n    filtered_ppp.km &lt;- rescale.ppp(filtered_ppp, 1000, \"km\")\n  \n    # Conduct KDE with the chosen bandwidth\n    kde_result &lt;- density(filtered_ppp.km, sigma = adjusted_bw, edge = TRUE, kernel = \"gaussian\")\n    \n    # Plot the KDE\n    plot(kde_result, main = paste(current_event,\" \",current_year, \" Q\", current_quarter, sep = \"\"))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nBattles: Distinct hotspot in Shan (North), which indicates this is likely the main battlefield during this quarter.\nExplosions/Remote Violence: Appears to have a wider spatial distribution, with more intense activity seen along the southern, western and central-eastern parts of the country. This suggests that attacks carried out here were more widespread and not confined to one region. This could imply different actors using remote violence across various territories or a strategic shift towards this type of warfare in multiple conflict zones.\nStrategic Developments, Violence Against Civilians: We can observe that both events display a similar KDE map, indicating that both events could have been carried out concurrently, hence leading to the similarity. The co-occurrence of strategic developments and civilian violence is concerning and could imply that the areas experiencing strategic military activity are also sites where civilians are more vulnerable to harm. These could be areas under military occupation, zones of forced displacement, or regions where there is a breakdown of law and order.\n\n\n\n\n\n4.1.2 Q2\n\n\nClick to view the code\n  par(mar = c(1,0,1,0))\n  par(mfrow=c(2,2))\n  current_year = 2021\n  current_quarter = 2\n  quarter_data &lt;- grped_acled_sf %&gt;% filter(year == current_year, quarter == current_quarter) \n  # Loop through each event\n  for (i in seq_len(nrow(quarter_data))) {\n    \n    filtered_sf &lt;- quarter_data[i, ]\n\n    # Extract the current year, quarter, and event type\n    current_event &lt;- filtered_sf$event_type\n    \n    # Convert to ppp object\n    filtered_ppp &lt;- as.ppp(st_coordinates(filtered_sf), st_bbox(filtered_sf))\n    \n    if(any(duplicated(filtered_ppp))){\n      rjitter(filtered_ppp, retry=TRUE, nsim=1, drop=TRUE)\n    }\n    filtered_ppp &lt;- filtered_ppp[mmr_owin]\n    # Rescale to kilometres\n    filtered_ppp.km &lt;- rescale.ppp(filtered_ppp, 1000, \"km\")\n  \n    # Conduct KDE with the chosen bandwidth\n    kde_result &lt;- density(filtered_ppp.km, sigma = adjusted_bw, edge = TRUE, kernel = \"gaussian\")\n    \n    # Plot the KDE\n    plot(kde_result, main = paste(current_event,\" \",current_year, \" Q\", current_quarter, sep = \"\"))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n* Battles: Presents a more spatially distributed distribution, with new hotspots at the Chin/Sagaing/Magway intersection.\n* Explosions/Remote Violence: The intensity of explosions and remote violence has shifted southward, with the most significant hotspot now in Yangon, the capital of the country. The shift to Yangon could also be a sign of more strategic attacks aimed at disrupting economic or political centers of power, following the military coup in early 2021.\n* Strategic Developments, Violence Against Civilians: Both events still present similar spatial distribution, reinforcing the idea that military strategic moves are closely linked to targeted violence against civilians.\n\n\n\n\n4.1.3 Q3\n\n\nClick to view the code\n  par(mar = c(1,0,1,0))\n  par(mfrow=c(2,2))\n  current_year = 2021\n  current_quarter = 3\n  quarter_data &lt;- grped_acled_sf %&gt;% filter(year == current_year, quarter == current_quarter) \n  # Loop through each event\n  for (i in seq_len(nrow(quarter_data))) {\n    \n    filtered_sf &lt;- quarter_data[i, ]\n\n    # Extract the current year, quarter, and event type\n    current_event &lt;- filtered_sf$event_type\n    \n    # Convert to ppp object\n    filtered_ppp &lt;- as.ppp(st_coordinates(filtered_sf), st_bbox(filtered_sf))\n    \n    if(any(duplicated(filtered_ppp))){\n      rjitter(filtered_ppp, retry=TRUE, nsim=1, drop=TRUE)\n    }\n    filtered_ppp &lt;- filtered_ppp[mmr_owin]\n    # Rescale to kilometres\n    filtered_ppp.km &lt;- rescale.ppp(filtered_ppp, 1000, \"km\")\n  \n    # Conduct KDE with the chosen bandwidth\n    kde_result &lt;- density(filtered_ppp.km, sigma = adjusted_bw, edge = TRUE, kernel = \"gaussian\")\n    \n    # Plot the KDE\n    plot(kde_result, main = paste(current_event,\" \",current_year, \" Q\", current_quarter, sep = \"\"))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAll 4 events shows a distinct hotspot at southern Sagaing, which appears to be the main battlefield and where most of the conflicts happen for 2021 Q4. Similarly, Yangon appears to have a significant amount of conflicts happening as well.\n\n\n\n\n4.1.4 Q4\n\n\nClick to view the code\n  par(mar = c(1,0,1,0))\n  par(mfrow=c(2,2))\n  current_year = 2021\n  current_quarter = 4\n  quarter_data &lt;- grped_acled_sf %&gt;% filter(year == current_year, quarter == current_quarter) \n  # Loop through each event\n  for (i in seq_len(nrow(quarter_data))) {\n    \n    filtered_sf &lt;- quarter_data[i, ]\n\n    # Extract the current year, quarter, and event type\n    current_event &lt;- filtered_sf$event_type\n    \n    # Convert to ppp object\n    filtered_ppp &lt;- as.ppp(st_coordinates(filtered_sf), st_bbox(filtered_sf))\n    \n    if(any(duplicated(filtered_ppp))){\n      rjitter(filtered_ppp, retry=TRUE, nsim=1, drop=TRUE)\n    }\n    filtered_ppp &lt;- filtered_ppp[mmr_owin]\n    # Rescale to kilometres\n    filtered_ppp.km &lt;- rescale.ppp(filtered_ppp, 1000, \"km\")\n  \n    # Conduct KDE with the chosen bandwidth\n    kde_result &lt;- density(filtered_ppp.km, sigma = adjusted_bw, edge = TRUE, kernel = \"gaussian\")\n    \n    # Plot the KDE\n    plot(kde_result, main = paste(current_event,\" \",current_year, \" Q\", current_quarter, sep = \"\"))\n}\n\n\n\n\n\n\n\n\n\n\n\n4.1.5 Overall Insights for 2021\nFluid Conflict Zones: The hotspots of conflict in Myanmar were more fluid and dynamic, with notable shifts in violence across various regions throughout the year. This shifting pattern likely reflects the early stages of the Myanmar civil conflict, which began in full force following the February 2021 military coup. There is a strong focus placed on Yangon and Southern Sagaing in particular."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#section-5",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#section-5",
    "title": "Take-Home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "4.2 2022",
    "text": "4.2 2022\n\n4.2.1 Q1\n\n\nClick to view the code\n  par(mar = c(1,0,1,0))\n  par(mfrow=c(2,2))\n  current_year = 2022\n  current_quarter = 1\n  quarter_data &lt;- grped_acled_sf %&gt;% filter(year == current_year, quarter == current_quarter) \n  # Loop through each event\n  for (i in seq_len(nrow(quarter_data))) {\n    \n    filtered_sf &lt;- quarter_data[i, ]\n\n    # Extract the current year, quarter, and event type\n    current_event &lt;- filtered_sf$event_type\n    \n    # Convert to ppp object\n    filtered_ppp &lt;- as.ppp(st_coordinates(filtered_sf), st_bbox(filtered_sf))\n    \n    if(any(duplicated(filtered_ppp))){\n      rjitter(filtered_ppp, retry=TRUE, nsim=1, drop=TRUE)\n    }\n    filtered_ppp &lt;- filtered_ppp[mmr_owin]\n    # Rescale to kilometres\n    filtered_ppp.km &lt;- rescale.ppp(filtered_ppp, 1000, \"km\")\n  \n    # Conduct KDE with the chosen bandwidth\n    kde_result &lt;- density(filtered_ppp.km, sigma = adjusted_bw, edge = TRUE, kernel = \"gaussian\")\n    \n    # Plot the KDE\n    plot(kde_result, main = paste(current_event,\" \",current_year, \" Q\", current_quarter, sep = \"\"))\n}\n\n\n\n\n\n\n\n\n\n\n\n4.2.2 Q2\n\n\nClick to view the code\n  par(mar = c(1,0,1,0))\n  par(mfrow=c(2,2))\n  current_year = 2022\n  current_quarter = 2\n  quarter_data &lt;- grped_acled_sf %&gt;% filter(year == current_year, quarter == current_quarter) \n  # Loop through each event\n  for (i in seq_len(nrow(quarter_data))) {\n    \n    filtered_sf &lt;- quarter_data[i, ]\n\n    # Extract the current year, quarter, and event type\n    current_event &lt;- filtered_sf$event_type\n    \n    # Convert to ppp object\n    filtered_ppp &lt;- as.ppp(st_coordinates(filtered_sf), st_bbox(filtered_sf))\n    \n    if(any(duplicated(filtered_ppp))){\n      rjitter(filtered_ppp, retry=TRUE, nsim=1, drop=TRUE)\n    }\n    filtered_ppp &lt;- filtered_ppp[mmr_owin]\n    # Rescale to kilometres\n    filtered_ppp.km &lt;- rescale.ppp(filtered_ppp, 1000, \"km\")\n  \n    # Conduct KDE with the chosen bandwidth\n    kde_result &lt;- density(filtered_ppp.km, sigma = adjusted_bw, edge = TRUE, kernel = \"gaussian\")\n    \n    # Plot the KDE\n    plot(kde_result, main = paste(current_event,\" \",current_year, \" Q\", current_quarter, sep = \"\"))\n}\n\n\n\n\n\n\n\n\n\n\n\n4.2.3 Q3\n\n\nClick to view the code\n  par(mar = c(1,0,1,0))\n  par(mfrow=c(2,2))\n  current_year = 2022\n  current_quarter = 3\n  quarter_data &lt;- grped_acled_sf %&gt;% filter(year == current_year, quarter == current_quarter) \n  # Loop through each event\n  for (i in seq_len(nrow(quarter_data))) {\n    \n    filtered_sf &lt;- quarter_data[i, ]\n\n    # Extract the current year, quarter, and event type\n    current_event &lt;- filtered_sf$event_type\n    \n    # Convert to ppp object\n    filtered_ppp &lt;- as.ppp(st_coordinates(filtered_sf), st_bbox(filtered_sf))\n    \n    if(any(duplicated(filtered_ppp))){\n      rjitter(filtered_ppp, retry=TRUE, nsim=1, drop=TRUE)\n    }\n    filtered_ppp &lt;- filtered_ppp[mmr_owin]\n    # Rescale to kilometres\n    filtered_ppp.km &lt;- rescale.ppp(filtered_ppp, 1000, \"km\")\n  \n    # Conduct KDE with the chosen bandwidth\n    kde_result &lt;- density(filtered_ppp.km, sigma = adjusted_bw, edge = TRUE, kernel = \"gaussian\")\n    \n    # Plot the KDE\n    plot(kde_result, main = paste(current_event,\" \",current_year, \" Q\", current_quarter, sep = \"\"))\n}\n\n\n\n\n\n\n\n\n\n\n\n4.2.4 Q4\n\n\nClick to view the code\n  par(mar = c(1,0,1,0))\n  par(mfrow=c(2,2))\n  current_year = 2022\n  current_quarter = 4\n  quarter_data &lt;- grped_acled_sf %&gt;% filter(year == current_year, quarter == current_quarter) \n  # Loop through each event\n  for (i in seq_len(nrow(quarter_data))) {\n    \n    filtered_sf &lt;- quarter_data[i, ]\n\n    # Extract the current year, quarter, and event type\n    current_event &lt;- filtered_sf$event_type\n    \n    # Convert to ppp object\n    filtered_ppp &lt;- as.ppp(st_coordinates(filtered_sf), st_bbox(filtered_sf))\n    \n    if(any(duplicated(filtered_ppp))){\n      rjitter(filtered_ppp, retry=TRUE, nsim=1, drop=TRUE)\n    }\n    filtered_ppp &lt;- filtered_ppp[mmr_owin]\n    # Rescale to kilometres\n    filtered_ppp.km &lt;- rescale.ppp(filtered_ppp, 1000, \"km\")\n  \n    # Conduct KDE with the chosen bandwidth\n    kde_result &lt;- density(filtered_ppp.km, sigma = adjusted_bw, edge = TRUE, kernel = \"gaussian\")\n    \n    # Plot the KDE\n    plot(kde_result, main = paste(current_event,\" \",current_year, \" Q\", current_quarter, sep = \"\"))\n}\n\n\n\n\n\n\n\n\n\n\n\n4.2.5 Overall Insights for 2022\nStabilization of Conflict Zones: The KDE plot for all quarters for the year of 2022 appears to stay identical, with majority of the conflicts being in the vicinity of southern Sagaing. This consistency indicates a stabilization of the conflict, with fewer fluctuations in the geography of violence. This suggests that the main battleground has become more fixed, with both government forces and resistance groups focusing their efforts in this region.\nImplications: The entrenchment of conflict in southern Sagaing also likely reflects a combination of military control over key areas and entrenched resistance by local groups. This stabilization might also indicate a stalemate or protracted conflict, where neither side is able to decisively push the front lines or gain control of new territories."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#section-6",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#section-6",
    "title": "Take-Home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "4.3 2023",
    "text": "4.3 2023\n\n4.3.1 Q1\n\n\nClick to view the code\n  par(mar = c(1,0,1,0))\n  par(mfrow=c(2,2))\n  current_year = 2023\n  current_quarter = 1\n  quarter_data &lt;- grped_acled_sf %&gt;% filter(year == current_year, quarter == current_quarter) \n  # Loop through each event\n  for (i in seq_len(nrow(quarter_data))) {\n    \n    filtered_sf &lt;- quarter_data[i, ]\n\n    # Extract the current year, quarter, and event type\n    current_event &lt;- filtered_sf$event_type\n    \n    # Convert to ppp object\n    filtered_ppp &lt;- as.ppp(st_coordinates(filtered_sf), st_bbox(filtered_sf))\n    \n    if(any(duplicated(filtered_ppp))){\n      rjitter(filtered_ppp, retry=TRUE, nsim=1, drop=TRUE)\n    }\n    filtered_ppp &lt;- filtered_ppp[mmr_owin]\n    # Rescale to kilometres\n    filtered_ppp.km &lt;- rescale.ppp(filtered_ppp, 1000, \"km\")\n  \n    # Conduct KDE with the chosen bandwidth\n    kde_result &lt;- density(filtered_ppp.km, sigma = adjusted_bw, edge = TRUE, kernel = \"gaussian\")\n    \n    # Plot the KDE\n    plot(kde_result, main = paste(current_event,\" \",current_year, \" Q\", current_quarter, sep = \"\"))\n}\n\n\n\n\n\n\n\n\n\n\n\n4.3.2 Q2\n\n\nClick to view the code\n  par(mar = c(1,0,1,0))\n  par(mfrow=c(2,2))\n  current_year = 2023\n  current_quarter = 2\n  quarter_data &lt;- grped_acled_sf %&gt;% filter(year == current_year, quarter == current_quarter) \n  # Loop through each event\n  for (i in seq_len(nrow(quarter_data))) {\n    \n    filtered_sf &lt;- quarter_data[i, ]\n\n    # Extract the current year, quarter, and event type\n    current_event &lt;- filtered_sf$event_type\n    \n    # Convert to ppp object\n    filtered_ppp &lt;- as.ppp(st_coordinates(filtered_sf), st_bbox(filtered_sf))\n    \n    if(any(duplicated(filtered_ppp))){\n      rjitter(filtered_ppp, retry=TRUE, nsim=1, drop=TRUE)\n    }\n    filtered_ppp &lt;- filtered_ppp[mmr_owin]\n    # Rescale to kilometres\n    filtered_ppp.km &lt;- rescale.ppp(filtered_ppp, 1000, \"km\")\n  \n    # Conduct KDE with the chosen bandwidth\n    kde_result &lt;- density(filtered_ppp.km, sigma = adjusted_bw, edge = TRUE, kernel = \"gaussian\")\n    \n    # Plot the KDE\n    plot(kde_result, main = paste(current_event,\" \",current_year, \" Q\", current_quarter, sep = \"\"))\n}\n\n\n\n\n\n\n\n\n\n\n\n4.3.3 Q3\n\n\nClick to view the code\n  par(mar = c(1,0,1,0))\n  par(mfrow=c(2,2))\n  current_year = 2023\n  current_quarter = 3\n  quarter_data &lt;- grped_acled_sf %&gt;% filter(year == current_year, quarter == current_quarter) \n  # Loop through each event\n  for (i in seq_len(nrow(quarter_data))) {\n    \n    filtered_sf &lt;- quarter_data[i, ]\n\n    # Extract the current year, quarter, and event type\n    current_event &lt;- filtered_sf$event_type\n    \n    # Convert to ppp object\n    filtered_ppp &lt;- as.ppp(st_coordinates(filtered_sf), st_bbox(filtered_sf))\n    \n    if(any(duplicated(filtered_ppp))){\n      rjitter(filtered_ppp, retry=TRUE, nsim=1, drop=TRUE)\n    }\n    filtered_ppp &lt;- filtered_ppp[mmr_owin]\n    # Rescale to kilometres\n    filtered_ppp.km &lt;- rescale.ppp(filtered_ppp, 1000, \"km\")\n  \n    # Conduct KDE with the chosen bandwidth\n    kde_result &lt;- density(filtered_ppp.km, sigma = adjusted_bw, edge = TRUE, kernel = \"gaussian\")\n    \n    # Plot the KDE\n    plot(kde_result, main = paste(current_event,\" \",current_year, \" Q\", current_quarter, sep = \"\"))\n}\n\n\n\n\n\n\n\n\n\n\n\n4.3.4 Q4\n\n\nClick to view the code\n  par(mar = c(1,0,1,0))\n  par(mfrow=c(2,2))\n  current_year = 2023\n  current_quarter = 4\n  quarter_data &lt;- grped_acled_sf %&gt;% filter(year == current_year, quarter == current_quarter) \n  # Loop through each event\n  for (i in seq_len(nrow(quarter_data))) {\n    \n    filtered_sf &lt;- quarter_data[i, ]\n\n    # Extract the current year, quarter, and event type\n    current_event &lt;- filtered_sf$event_type\n    \n    # Convert to ppp object\n    filtered_ppp &lt;- as.ppp(st_coordinates(filtered_sf), st_bbox(filtered_sf))\n    \n    if(any(duplicated(filtered_ppp))){\n      rjitter(filtered_ppp, retry=TRUE, nsim=1, drop=TRUE)\n    }\n    filtered_ppp &lt;- filtered_ppp[mmr_owin]\n    # Rescale to kilometres\n    filtered_ppp.km &lt;- rescale.ppp(filtered_ppp, 1000, \"km\")\n  \n    # Conduct KDE with the chosen bandwidth\n    kde_result &lt;- density(filtered_ppp.km, sigma = adjusted_bw, edge = TRUE, kernel = \"gaussian\")\n    \n    # Plot the KDE\n    plot(kde_result, main = paste(current_event,\" \",current_year, \" Q\", current_quarter, sep = \"\"))\n}\n\n\n\n\n\n\n\n\n\n\n\n4.3.5 Overall Insights for 2023\nConsistent hotspots: There’s a persistent area of high activity in the region around Southern Sagaing across all event types and quarters, similar to the previous year.\nEvent type variations: Different event types show slightly different patterns. For example, “Strategic developments” tend to be more concentrated, while “Battles” and “Explosions/Remote violence” show more widespread activity.\nGeographical spread: Activity is not uniform across the country. The southern peninsular region generally shows less activity, while the central and northern regions show more.\nQ4 escalation: There appears to be an increase in activity across all event types in Q4, particularly in the northern regions."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#section-7",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#section-7",
    "title": "Take-Home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "4.4 2024",
    "text": "4.4 2024\n\n4.4.1 Q1\n\n\nClick to view the code\n  par(mar = c(1,0,1,0))\n  par(mfrow=c(2,2))\n  current_year = 2024\n  current_quarter = 1\n  quarter_data &lt;- grped_acled_sf %&gt;% filter(year == current_year, quarter == current_quarter) \n  # Loop through each event\n  for (i in seq_len(nrow(quarter_data))) {\n    \n    filtered_sf &lt;- quarter_data[i, ]\n\n    # Extract the current year, quarter, and event type\n    current_event &lt;- filtered_sf$event_type\n    \n    # Convert to ppp object\n    filtered_ppp &lt;- as.ppp(st_coordinates(filtered_sf), st_bbox(filtered_sf))\n    \n    if(any(duplicated(filtered_ppp))){\n      rjitter(filtered_ppp, retry=TRUE, nsim=1, drop=TRUE)\n    }\n    filtered_ppp &lt;- filtered_ppp[mmr_owin]\n    # Rescale to kilometres\n    filtered_ppp.km &lt;- rescale.ppp(filtered_ppp, 1000, \"km\")\n  \n    # Conduct KDE with the chosen bandwidth\n    kde_result &lt;- density(filtered_ppp.km, sigma = adjusted_bw, edge = TRUE, kernel = \"gaussian\")\n    \n    # Plot the KDE\n    plot(kde_result, main = paste(current_event,\" \",current_year, \" Q\", current_quarter, sep = \"\"))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLess conflicts: Conflict events appears to have lessen significantly, as evidenced from the decrease in the intensity level of the hotspots across Myanmar. This decline is particularly apparent for “Explosions/Remote Violence,” where the once prominent hotspots have become less concentrated and more dispersed compared to previous quarters.\n\n\n\n\n4.4.2 Q2\n\n\nClick to view the code\n  par(mar = c(1,0,1,0))\n  par(mfrow=c(2,2))\n  current_year = 2024\n  current_quarter = 2\n  quarter_data &lt;- grped_acled_sf %&gt;% filter(year == current_year, quarter == current_quarter) \n  # Loop through each event\n  for (i in seq_len(nrow(quarter_data))) {\n    \n    filtered_sf &lt;- quarter_data[i, ]\n\n    # Extract the current year, quarter, and event type\n    current_event &lt;- filtered_sf$event_type\n    \n    # Convert to ppp object\n    filtered_ppp &lt;- as.ppp(st_coordinates(filtered_sf), st_bbox(filtered_sf))\n    \n    if(any(duplicated(filtered_ppp))){\n      rjitter(filtered_ppp, retry=TRUE, nsim=1, drop=TRUE)\n    }\n    filtered_ppp &lt;- filtered_ppp[mmr_owin]\n    # Rescale to kilometres\n    filtered_ppp.km &lt;- rescale.ppp(filtered_ppp, 1000, \"km\")\n  \n    # Conduct KDE with the chosen bandwidth\n    kde_result &lt;- density(filtered_ppp.km, sigma = adjusted_bw, edge = TRUE, kernel = \"gaussian\")\n    \n    # Plot the KDE\n    plot(kde_result, main = paste(current_event,\" \",current_year, \" Q\", current_quarter, sep = \"\"))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBattles: Hotspot in southern Sagaing has decreased in intensity\nViolence Against Civilians: Hotspot in Sagaing remains, new hotspots are also appearing on the southern part of Myanmar."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#analysis-background",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#analysis-background",
    "title": "Take-Home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "5.1 Analysis Background",
    "text": "5.1 Analysis Background\nFor the analysis for the whole of Myanmar, I will be primarily using the G-function to conduct second-order spatial point pattern analysis. I chose it for its computational efficiency, straightforward interpretation of nearest-neighbor distances, and suitability for analyzing a single type of event. The K and L-functions, while valuable, are more resource-intensive, making them not ideal for my large dataset, while the F-function is less applicable to our single-type event analysis.\nFor the analysis of Sagaing however, I will be using the K-function to conduct the relevant analysis.\n\n5.1.1 Monte Carlo Simulation\nI will also be conducting a Monte Carlo simulation for testing Complete Spatial Randomness (CSR) in the spatial distribution of conflict events. This involves generating multiple random point patterns under CSR to create a range of expected spatial statistics. By comparing the observed G-function of the conflict events against these simulated envelopes, I can determine if the observed clustering or dispersion is significantly different from what would be expected by chance.\nThe envelope function provides upper and lower bounds for the G-function, helping to assess whether the spatial pattern of conflict events is significantly clustered or dispersed compared to random patterns. This method ensures a rigorous evaluation of spatial randomness and enhances the interpretation of observed patterns.\n\n\n5.1.2 Example of G-Function Plot\nThis code snippet below provides an example of a G-function plot of a sample of the data, calculated using an envelope derived from 99 simulations. We will also be keeping the confidence level of the envelopes to the default 95%.\nThis approach is used to evaluate the spatial distribution of events and assess how the observed point pattern compares to what might be expected under complete spatial randomness. Note that we will be using a set seed to ensure the reproducibility of our code.\n\nset.seed(123456)\ntest_second_sf &lt;- grped_acled_sf %&gt;% filter(event_type == \"Battles\", year == 2023, quarter ==2)\ntest_second_ppp&lt;-as.ppp(st_coordinates(test_second_sf), st_bbox(test_second_sf))\n\nif(any(duplicated(test_second_ppp))){\n  test_second_ppp &lt;- rjitter(test_second_ppp, retry=TRUE, nsim=1, drop=TRUE)\n}\n\ntest_second_ppp &lt;- test_second_ppp[mmr_owin]\ntest_second.csr &lt;- envelope(test_second_ppp, Gest, nsim = 99)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\nplot(test_second.csr, xlim=c(0,10000), main=\"Example of G-Function Plot\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe plot typically shows several key elements related to both the observed data and the simulated patterns:\n\nG^obs (Observed G-function):This represents the empirical G-function calculated from your actual point pattern data. It shows the cumulative distribution of the nearest-neighbor distances between points in the dataset.\nG^theo (Theoretical G-function): Theoretical G-function that would be expected if the point pattern follows Complete Spatial Randomness (CSR). Under CSR, the points are distributed independently and uniformly across the study area.\nG^hi (Upper Envelope):\n\nUpper bound of the G-function values generated from the random simulations (based on nsim, the number of simulations). It defines the maximum value that the G-function could take under CSR in these simulations.\nIf the observed G-function (G^obs) exceeds G^hi, it suggests clustering—points are closer together than expected by chance.\n\nG^lo (Lower Envelope):\n\nLower bound of the G-function values from the random simulations. It defines the minimum G-function value expected under CSR.\nIf the observed G-function (G^obs) falls below G^lo, it suggests dispersion or regularity—points are more evenly spaced than expected by chance.\n\nShaded area: Represents the 95% confidence interval"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#whole-of-myanmar",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#whole-of-myanmar",
    "title": "Take-Home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "5.2 Whole of Myanmar",
    "text": "5.2 Whole of Myanmar\n\n5.2.1 Hypothesis\nTo confirm the observed spatial patterns, I will conduct a hypothesis test using the G-function along with Monte Carlo simulation envelopes. The test will be designed as follows:\n\nH₀: The distribution of conflict events in Myanmar is randomly distributed (follows Complete Spatial Randomness, CSR).\nH₁: The distribution of conflict events in Myanmar is not randomly distributed.\nIf the observed G-function lies outside the envelope, we will reject the null hypothesis, suggesting the presence of spatial clustering or dispersion.\nA 95% confidence interval will be used for the envelopes.\n\n\n\n5.2.2 Battles\nThis chunk of code generates a G-function plot for all quarters of a particular event, using an envelope derived from 99 simulations for significance testing. Then, it saves the results to a list to plot it later on.\n\nset.seed(123456)\nbattle_data_sf &lt;- grped_acled_sf %&gt;% filter(event_type == \"Battles\")\n\n# Initialize an empty list to store the results\nbattle_G_CK_list &lt;- list()\n\n# Loop through each combination of year and quarter\nfor (i in seq_len(nrow(battle_data_sf))) {\n  \n  # Filter the data for the current iteration\n  filtered_sf &lt;- battle_data_sf[i, ]\n  \n  # Extract the current year, quarter, and event type\n  current_year &lt;- filtered_sf$year\n  current_quarter &lt;- filtered_sf$quarter\n  \n  # Convert to ppp object\n  filtered_ppp &lt;- as.ppp(st_coordinates(filtered_sf), st_bbox(filtered_sf))\n  \n  if(any(duplicated(filtered_ppp))){\n    filtered_ppp &lt;- rjitter(filtered_ppp, retry=TRUE, nsim=1, drop=TRUE)\n  }\n  \n  filtered_ppp &lt;- filtered_ppp[mmr_owin]\n  G_CK.csr &lt;- envelope(filtered_ppp, Gest, nsim = 99)\n  \n  # Save the result in the list with a unique name\n  name &lt;- paste0(current_year, \"_Q\", current_quarter)\n  battle_G_CK_list[[name]] &lt;- G_CK.csr\n}\n\nwrite_rds(battle_G_CK_list, \"data/rds/battle_G_CK_list.rds\")\n\n\nThis chunk of code plots the G-function plot for all quarters.\n\npar(mar = c(2,1,2,1))\npar(mfrow=c(4,4))\n# Plotting\nfor(i in seq_along(battle_G_CK_list)) {\n  g_ck &lt;- battle_G_CK_list[[i]]\n  plot(g_ck, xlim=c(0,10000), main=names(battle_G_CK_list)[i])\n}\n\n\n\n\n\n\n\n\n\n\n5.2.3 Explosions/Remote Violence\n\n\nClick to view the code\nset.seed(123456)\nexplosions_data_sf &lt;- grped_acled_sf %&gt;% filter(event_type == \"Explosions/Remote violence\")\n\n# Initialize an empty list to store the results\nexplosions_G_CK_list &lt;- list()\n\n# Loop through each combination of year and quarter\nfor (i in seq_len(nrow(explosions_data_sf))) {\n  \n  # Filter the data for the current iteration\n  filtered_sf &lt;- explosions_data_sf[i, ]\n  \n  # Extract the current year, quarter, and event type\n  current_year &lt;- filtered_sf$year\n  current_quarter &lt;- filtered_sf$quarter\n  \n  # Convert to ppp object\n  filtered_ppp &lt;- as.ppp(st_coordinates(filtered_sf), st_bbox(filtered_sf))\n  \n  if(any(duplicated(filtered_ppp))){\n    filtered_ppp &lt;- rjitter(filtered_ppp, retry=TRUE, nsim=1, drop=TRUE)\n  }\n  \n  filtered_ppp &lt;- filtered_ppp[mmr_owin]\n  G_CK.csr &lt;- envelope(filtered_ppp, Gest, nsim = 99)\n  \n  # Save the result in the list with a unique name\n  name &lt;- paste0(current_year, \"_Q\", current_quarter)\n  explosions_G_CK_list[[name]] &lt;- G_CK.csr\n}\nwrite_rds(explosions_G_CK_list, \"data/rds/explosions_G_CK_list.rds\")\n\n\n\n\npar(mar = c(2,1,2,1))\npar(mfrow=c(4,4))\n# Plotting\nfor(i in seq_along(explosions_G_CK_list)) {\n  g_ck &lt;- explosions_G_CK_list[[i]]\n  plot(g_ck, xlim=c(0,10000), main=names(explosions_G_CK_list)[i])\n}\n\n\n\n\n\n\n\n\n\n\n5.2.4 Strategic Developments\n\n\nClick to view the code\nset.seed(123456)\nstrategic_data_sf &lt;- grped_acled_sf %&gt;% filter(event_type == \"Strategic developments\")\n\n# Initialize an empty list to store the results\nstrategic_G_CK_list &lt;- list()\n\n# Loop through each combination of year and quarter\nfor (i in seq_len(nrow(strategic_data_sf))) {\n  \n  # Filter the data for the current iteration\n  filtered_sf &lt;- strategic_data_sf[i, ]\n  \n  # Extract the current year, quarter, and event type\n  current_year &lt;- filtered_sf$year\n  current_quarter &lt;- filtered_sf$quarter\n  \n  # Convert to ppp object\n  filtered_ppp &lt;- as.ppp(st_coordinates(filtered_sf), st_bbox(filtered_sf))\n  \n  if(any(duplicated(filtered_ppp))){\n    filtered_ppp &lt;- rjitter(filtered_ppp, retry=TRUE, nsim=1, drop=TRUE)\n  }\n  \n  filtered_ppp &lt;- filtered_ppp[mmr_owin]\n  G_CK.csr &lt;- envelope(filtered_ppp, Gest, nsim = 99)\n  \n  # Save the result in the list with a unique name\n  name &lt;- paste0(current_year, \"_Q\", current_quarter)\n  strategic_G_CK_list[[name]] &lt;- G_CK.csr\n}\nwrite_rds(strategic_G_CK_list, \"data/rds/strategic_G_CK_list.rds\")\n\n\n\n\npar(mar = c(2,1,2,1))\npar(mfrow=c(4,4))\n# Plotting\nfor(i in seq_along(strategic_G_CK_list)) {\n  g_ck &lt;- strategic_G_CK_list[[i]]\n  plot(g_ck, xlim=c(0,10000), main=names(strategic_G_CK_list)[i])\n}\n\n\n\n\n\n\n\n\n\n\n5.2.5 Violence Against Civilians\n\n\nClick to view the code\nset.seed(123456)\nviolence_data_sf &lt;- grped_acled_sf %&gt;% filter(event_type == \"Violence against civilians\")\n\n# Initialize an empty list to store the results\nviolence_G_CK_list &lt;- list()\n\n# Loop through each combination of year and quarter\nfor (i in seq_len(nrow(violence_data_sf))) {\n  \n  # Filter the data for the current iteration\n  filtered_sf &lt;- violence_data_sf[i, ]\n  \n  # Extract the current year, quarter, and event type\n  current_year &lt;- filtered_sf$year\n  current_quarter &lt;- filtered_sf$quarter\n  \n  # Convert to ppp object\n  filtered_ppp &lt;- as.ppp(st_coordinates(filtered_sf), st_bbox(filtered_sf))\n  \n  if(any(duplicated(filtered_ppp))){\n    filtered_ppp &lt;- rjitter(filtered_ppp, retry=TRUE, nsim=1, drop=TRUE)\n  }\n  \n  filtered_ppp &lt;- filtered_ppp[mmr_owin]\n  G_CK.csr &lt;- envelope(filtered_ppp, Gest, nsim = 99)\n  \n  # Save the result in the list with a unique name\n  name &lt;- paste0(current_year, \"_Q\", current_quarter)\n  violence_G_CK_list[[name]] &lt;- G_CK.csr\n}\nwrite_rds(violence_G_CK_list, \"data/rds/violence_G_CK_list.rds\")\n\n\n\n\npar(mar = c(2,1,2,1))\npar(mfrow=c(4,4))\n# Plotting\nfor(i in seq_along(violence_G_CK_list)) {\n  g_ck &lt;- violence_G_CK_list[[i]]\n  plot(g_ck, xlim=c(0,10000), main=names(violence_G_CK_list)[i])\n}\n\n\n\n\n\n\n\n\n\n\n5.2.6 Insights\n\nMajority of the observed G-function plotted for each quarter for the different events lies above the envelope, suggesting that the points, in this case the conflicts, are more clustered than expected under CSR. This means that the distribution of the conflicts is not at random and tends to cluster in specific regions.\nThe slope of the observed G-function also generally shows a smooth and constant trend.\nThe only exception to the observed trends is the plots for 2021 Q1, which display a more irregular pattern. This irregularity can be attributed to having fewer data points during this time, as the civil war had only just begun in February 2021. When the number of events is smaller, the statistical power of the analysis decreases, leading to less smooth or more erratic G-function patterns. As a result, the G-function for this early quarter may not accurately reflect clustering or dispersion. Given the sparse data from this period, we will not be placing significant weight on the 2021 Q1 results when making decisions about accepting or rejecting the hypothesis.\n\nConclusion:\n\nBased on the analysis, the majority of the observed G-function lies outside the envelope, indicating that the conflict events in Myanmar exhibit a pattern of clustering rather than random distribution. This leads us to reject the null hypothesis that the conflicts are randomly distributed. Instead, the results suggest that conflict events are spatially concentrated in specific regions."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#hotspot-of-conflict",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#hotspot-of-conflict",
    "title": "Take-Home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "5.3 Hotspot of Conflict",
    "text": "5.3 Hotspot of Conflict\n\n5.3.1 Identifying the Hotspot of Conflict\nWhile the KDE map shows a high concentration of conflict in the region where Sagaing, Magway, and Mandalay intersect, it’s challenging to visually pinpoint the exact region with the highest density of conflict events. To overcome this, we employ a quantitative approach using the following code chunk to calculate the number of conflict events within each region.\nBy counting the number of data points within each region, we observe that the majority of conflict events occur in the Sagaing region. This result aligns with the spatial trends observed in the KDE map, confirming Sagaing as the area with the highest concentration of conflict.\n\n# Create an empty dataframe to store results\nconflict_counts &lt;- data.frame(Region = character(), Conflict_Count = integer(), stringsAsFactors = FALSE)\n\n# Get a list of unique regions\nregions &lt;- unique(mmrsr_sf$ST)\n\n# Loop through each region\nfor (region in regions) {\n  # Filter the region geometry\n  region_sf &lt;- mmrsr_sf %&gt;% filter(ST == region)\n  \n  # Perform spatial intersection with conflict points\n  no_of_conflict_within &lt;- st_intersection(acled_sf, region_sf)\n  \n  # Count the number of conflicts in this region\n  conflict_count &lt;- nrow(no_of_conflict_within)\n  \n  # Store the results in the dataframe\n  conflict_counts &lt;- rbind(conflict_counts, data.frame(Region = region, Conflict_Count = conflict_count))\n}\n\n# Arrange the results in descending order of conflict count\nconflict_counts_ordered &lt;- conflict_counts %&gt;%\n  arrange(desc(Conflict_Count))\n\nwrite_rds(conflict_counts_ordered, \"data/rds/conflict_counts_ordered.rds\")\n\n\nconflict_counts_ordered &lt;- read_rds( \"data/rds/conflict_counts_ordered.rds\")\nprint(conflict_counts_ordered)\n\n         Region Conflict_Count\n1       Sagaing          11128\n2        Magway           4179\n3      Mandalay           3603\n4  Shan (North)           2938\n5        Kachin           2776\n6        Yangon           2608\n7       Rakhine           2277\n8   Tanintharyi           2240\n9         Kayin           1817\n10          Mon           1677\n11         Chin           1557\n12 Shan (South)           1430\n13        Kayah           1327\n14  Bago (East)           1229\n15   Ayeyarwady            852\n16  Bago (West)            639\n17  Nay Pyi Taw            268\n18  Shan (East)             56\n\n\n\n\n5.3.2 Sagaing Region\nGiven that conflict events in Sagaing account for around 25% of the total, we aim to conduct a more detailed analysis to understand the spatial patterns within this significant region. The first step involves isolating the conflict events specific to Sagaing by filtering the data to focus exclusively on this area. For this, instead of the G-function previously used, we will be using the K-function instead since we are now working with a smaller dataset.\n\n5.3.2.1 Overview of Sagaing Region\nSince we are focusing solely on Sagaing, here is a more granular map from MIMU which divides Sagaing into districts so we get a better overview of it.\n\nsagaing_map_sf &lt;- mmrsr_district_sf %&gt;% filter(ST == \"Sagaing\")\ntm_shape(sagaing_map_sf) +\n  tm_polygons(col = \"DT\", palette = colors) +  # Apply color palette to polygons\n  tm_text(\"DT\", size = 1, col = \"black\", bg.color = \"white\", just = c(\"center\", \"center\"),  xmod = 0, ymod = 0) + tm_layout(main.title = \"Sagaing\",\n            main.title.position = \"center\",\n            main.title.size = 1.6,\n            legend.outside = TRUE,\n            frame = TRUE)+\n    tm_legend(title = \"Districts\")  # Set custom legend title\n\n\n\n\n\n\n\n\nThis chunk of code creates a Sagaing owin object, converts acled_sf into a ppp object and plots a map displaying only conflict events in Sagaing Region.\n\nsagaing_owin &lt;- as.owin(sagaing_map_sf)\nacled_ppp &lt;- as.ppp(acled_sf)\nsagaing_ppp &lt;- acled_ppp[sagaing_owin]\nplot(sagaing_ppp, pch = 16, cex = 0.5)\n\n\n\n\n\n\n\n\n\n\n5.3.2.2 Hypothesis\nSimilar to the one we did for the whole of myanmar, we will conduct a hypothesis test using the K-function along with Monte Carlo simulation envelopes. The test will be designed as follows:\n\nH₀: The distribution of conflict events in Sagaing is randomly distributed (follows Complete Spatial Randomness, CSR).\nH₁: The distribution of conflict events in Sagaing is not randomly distributed.\nIf the observed G-function lies outside the envelope, we will reject the null hypothesis, suggesting the presence of spatial clustering or dispersion.\nA 95% confidence interval will be used for the envelopes.\n\n\n\n5.3.2.3 Battles\n\nset.seed(123456)\nbattle_data_sf &lt;- grped_acled_sf %&gt;% filter(event_type == \"Battles\")\n\n# Initialize an empty list to store the results\nsagaing_battle_K_CK_list &lt;- list()\n\n# Loop through each combination of year and quarter\nfor (i in seq_len(nrow(battle_data_sf))) {\n  \n  # Filter the data for the current iteration\n  filtered_sf &lt;- battle_data_sf[i, ]\n  \n  # Extract the current year, quarter, and event type\n  current_year &lt;- filtered_sf$year\n  current_quarter &lt;- filtered_sf$quarter\n  \n  # Convert to ppp object\n  filtered_ppp &lt;- as.ppp(st_coordinates(filtered_sf), st_bbox(filtered_sf))\n  \n  if(any(duplicated(filtered_ppp))){\n    filtered_ppp &lt;- rjitter(filtered_ppp, retry=TRUE, nsim=1, drop=TRUE)\n  }\n  \n  filtered_ppp &lt;- filtered_ppp[sagaing_owin]\n  K_ck.csr &lt;- envelope(filtered_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n  \n  # Save the result in the list with a unique name\n  name &lt;- paste0(current_year, \"_Q\", current_quarter)\n  sagaing_battle_K_CK_list[[name]] &lt;- K_ck.csr\n}\n\nwrite_rds(sagaing_battle_K_CK_list, \"data/rds/sagaing_battle_K_CK_list.rds\")\n\n\npar(mar = c(2,1,2,1))\npar(mfrow=c(4,4))\n# Plotting\nfor(i in seq_along(sagaing_battle_K_CK_list)) {\n  k_ck &lt;- sagaing_battle_K_CK_list[[i]]\n  plot(k_ck, . - r ~ r, xlab=\"d\", ylab=\"k(d)-r\", main=names(sagaing_battle_K_CK_list)[i])\n}\n\n\n\n\n\n\n\n\n\n\n5.3.2.4 Explosions/ Remote Violence\n\nset.seed(123456)\nexplosions_data_sf &lt;- grped_acled_sf %&gt;% filter(event_type == \"Explosions/Remote violence\")\n\n# Initialize an empty list to store the results\nsagaing_explosions_K_CK_list &lt;- list()\n\n# Loop through each combination of year and quarter\nfor (i in seq_len(nrow(explosions_data_sf))) {\n  \n  # Filter the data for the current iteration\n  filtered_sf &lt;- explosions_data_sf[i, ]\n  \n  # Extract the current year, quarter, and event type\n  current_year &lt;- filtered_sf$year\n  current_quarter &lt;- filtered_sf$quarter\n  \n  # Convert to ppp object\n  filtered_ppp &lt;- as.ppp(st_coordinates(filtered_sf), st_bbox(filtered_sf))\n  \n  if(any(duplicated(filtered_ppp))){\n    filtered_ppp &lt;- rjitter(filtered_ppp, retry=TRUE, nsim=1, drop=TRUE)\n  }\n  \n  filtered_ppp &lt;- filtered_ppp[sagaing_owin]\n  K_ck.csr &lt;- envelope(filtered_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n  \n  # Save the result in the list with a unique name\n  name &lt;- paste0(current_year, \"_Q\", current_quarter)\n  sagaing_explosions_K_CK_list[[name]] &lt;- K_ck.csr\n}\n\nwrite_rds(sagaing_explosions_K_CK_list, \"data/rds/sagaing_explosions_K_CK_list.rds\")\n\n\npar(mar = c(2,1,2,1))\npar(mfrow=c(4,4))\n# Plotting\nfor(i in seq_along(sagaing_explosions_K_CK_list)) {\n  k_ck &lt;- sagaing_explosions_K_CK_list[[i]]\n  plot(k_ck, . - r ~ r, xlab=\"d\", ylab=\"k(d)-r\", main=names(sagaing_explosions_K_CK_list)[i])\n}\n\n\n\n\n\n\n\n\n\n\n5.3.2.5 Strategic Developments\n\nset.seed(123456)\nstrategic_data_sf &lt;- grped_acled_sf %&gt;% filter(event_type == \"Strategic developments\")\n\n# Initialize an empty list to store the results\nsagaing_strategic_K_CK_list &lt;- list()\n\n# Loop through each combination of year and quarter\nfor (i in seq_len(nrow(strategic_data_sf))) {\n  \n  # Filter the data for the current iteration\n  filtered_sf &lt;- strategic_data_sf[i, ]\n  \n  # Extract the current year, quarter, and event type\n  current_year &lt;- filtered_sf$year\n  current_quarter &lt;- filtered_sf$quarter\n  \n  # Convert to ppp object\n  filtered_ppp &lt;- as.ppp(st_coordinates(filtered_sf), st_bbox(filtered_sf))\n  \n  if(any(duplicated(filtered_ppp))){\n    filtered_ppp &lt;- rjitter(filtered_ppp, retry=TRUE, nsim=1, drop=TRUE)\n  }\n  \n  filtered_ppp &lt;- filtered_ppp[sagaing_owin]\n  K_ck.csr &lt;- envelope(filtered_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n  \n  # Save the result in the list with a unique name\n  name &lt;- paste0(current_year, \"_Q\", current_quarter)\n  sagaing_strategic_K_CK_list[[name]] &lt;- K_ck.csr\n}\n\nwrite_rds(sagaing_strategic_K_CK_list, \"data/rds/sagaing_strategic_K_CK_list.rds\")\n\n\npar(mar = c(2,1,2,1))\npar(mfrow=c(4,4))\n# Plotting\nfor(i in seq_along(sagaing_strategic_K_CK_list)) {\n  k_ck &lt;- sagaing_strategic_K_CK_list[[i]]\n  plot(k_ck, . - r ~ r, xlab=\"d\", ylab=\"k(d)-r\", main=names(sagaing_strategic_K_CK_list)[i])\n}\n\n\n\n\n\n\n\n\n\n\n5.3.2.6 Violence Against Civilians\n\nset.seed(123456)\nviolence_data_sf &lt;- grped_acled_sf %&gt;% filter(event_type == \"Violence against civilians\")\n\n# Initialize an empty list to store the results\nsagaing_violence_K_CK_list &lt;- list()\n\n# Loop through each combination of year and quarter\nfor (i in seq_len(nrow(violence_data_sf))) {\n  \n  # Filter the data for the current iteration\n  filtered_sf &lt;- violence_data_sf[i, ]\n  \n  # Extract the current year, quarter, and event type\n  current_year &lt;- filtered_sf$year\n  current_quarter &lt;- filtered_sf$quarter\n  \n  # Convert to ppp object\n  filtered_ppp &lt;- as.ppp(st_coordinates(filtered_sf), st_bbox(filtered_sf))\n  \n  if(any(duplicated(filtered_ppp))){\n    filtered_ppp &lt;- rjitter(filtered_ppp, retry=TRUE, nsim=1, drop=TRUE)\n  }\n  \n  filtered_ppp &lt;- filtered_ppp[sagaing_owin]\n  K_ck.csr &lt;- envelope(filtered_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n  \n  # Save the result in the list with a unique name\n  name &lt;- paste0(current_year, \"_Q\", current_quarter)\n  sagaing_violence_K_CK_list[[name]] &lt;- K_ck.csr\n}\n\nwrite_rds(sagaing_violence_K_CK_list, \"data/rds/sagaing_violence_K_CK_list.rds\")\n\n\npar(mar = c(2,1,2,1))\npar(mfrow=c(4,4))\n# Plotting\nfor(i in seq_along(sagaing_violence_K_CK_list)) {\n  k_ck &lt;- sagaing_violence_K_CK_list[[i]]\n  plot(k_ck, . - r ~ r, xlab=\"d\", ylab=\"k(d)-r\", main=names(sagaing_violence_K_CK_list)[i])\n}\n\n\n\n\n\n\n\n\n\n\n5.3.2.7 Insights\n\nSimilar to the G-function plots for Myanmar, we can see that most of the plots are identical with majority of the observed K-function being outside and above the envelope with constant and smooth trends.\nThe smooth trend of the K-function across the plots implies that the clustering of events is not only prevalent but also persistent over time.\nThe only exception to this is yet again 2021 q1, this time we can also observe a stepped pattern with the plots which again is likely attributed to the small amount of data points during this period of time, and hence we will not be placing significant weight on the 2021 Q1 results when making decisions about accepting or rejecting the hypothesis.\nSince the observed K-function are all above the envelope, we can reject the null hypothesis that conflict events in Sagaing is randomly distributed and instead conclude that they are clustered at specific regions. Observing the point pattern map we previously plotted, we can see that most conflict events occur in the following districts: Kanbalu, Shwebo, Sagaing (District, not region), Monywa, Yinmarbi."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#overview-1",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#overview-1",
    "title": "Take-Home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "6.1 Overview",
    "text": "6.1 Overview\nThis chunk of code utilizes spattemp.density() to compute the spatiotemporal KDE for all 14 quarters from 2021 to 2024 Q2.\n\nBattlesExplosions/ Remote ViolenceStrategic DevelopmentsViolence Against Civilians\n\n\n\n\nClick to view the code\n  par(mar = c(2,0,2,0))\n  par(mfrow=c(4,4))\n  current_event = \"Battles\"\n  \n  # Filter the data\n  battle_data_sf &lt;- grped_acled_sf %&gt;% filter(event_type == current_event)\n  \n  # Define time slices for plotting\n  tims &lt;- c(\n  \"2021 Q1\", \"2021 Q2\", \"2021 Q3\", \"2021 Q4\",\n  \"2022 Q1\", \"2022 Q2\", \"2022 Q3\", \"2022 Q4\",\n  \"2023 Q1\", \"2023 Q2\", \"2023 Q3\", \"2023 Q4\",\n  \"2024 Q1\", \"2024 Q2\"\n  )\n  \n  # Convert to ppp object\n  spattemp_ppp &lt;- as.ppp(st_coordinates(battle_data_sf), st_bbox(battle_data_sf))\n  # Remove Duplicates\n  if(any(duplicated(spattemp_ppp))){\n    rjitter(spattemp_ppp, retry=TRUE, nsim=1, drop=TRUE)\n  }\n  \n  # Convert to owin\n  spattemp_ppp &lt;- spattemp_ppp[mmr_owin]\n\n  # Compute the spatiotemporal Kernel Density Estimate (KDE)\n  st_kde &lt;- spattemp.density(spattemp_ppp)\n  for(x in seq_along(tims)){\n    plot(st_kde, x, \n     override.par=FALSE, \n     fix.range=TRUE, \n     main=paste(tims[x]))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to view the code\n  par(mar = c(2,0,2,0))\n  par(mfrow=c(4,4))\n  current_event = \"Explosions/Remote violence\"\n\n  # Filter the data\n  explosions_data_sf &lt;- grped_acled_sf %&gt;% filter(event_type == current_event)\n  \n  # Define time slices for plotting\n  tims &lt;- c(\n  \"2021 Q1\", \"2021 Q2\", \"2021 Q3\", \"2021 Q4\",\n  \"2022 Q1\", \"2022 Q2\", \"2022 Q3\", \"2022 Q4\",\n  \"2023 Q1\", \"2023 Q2\", \"2023 Q3\", \"2023 Q4\",\n  \"2024 Q1\", \"2024 Q2\"\n)\n  \n  # Convert to ppp object\n  spattemp_ppp &lt;- as.ppp(st_coordinates(explosions_data_sf), st_bbox(explosions_data_sf))\n  # Remove Duplicates\n  if(any(duplicated(spattemp_ppp))){\n    rjitter(spattemp_ppp, retry=TRUE, nsim=1, drop=TRUE)\n  }\n  # Convert to owin\n  spattemp_ppp &lt;- spattemp_ppp[mmr_owin]\n  \n  # Compute the spatiotemporal Kernel Density Estimate (KDE)\n  st_kde &lt;- spattemp.density(spattemp_ppp)\n  for(x in seq_along(tims)){\n    plot(st_kde, x, \n     override.par=FALSE, \n     fix.range=TRUE, \n     main=paste(tims[x]))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to view the code\n  par(mar = c(2,0,2,0))\n  par(mfrow=c(4,4))\n  current_event = \"Strategic developments\"\n  \n  # Filter the data\n  strategic_data_sf &lt;- grped_acled_sf %&gt;% filter(event_type == current_event)\n  \n  # Define time slices for plotting\n  tims &lt;- c(\n  \"2021 Q1\", \"2021 Q2\", \"2021 Q3\", \"2021 Q4\",\n  \"2022 Q1\", \"2022 Q2\", \"2022 Q3\", \"2022 Q4\",\n  \"2023 Q1\", \"2023 Q2\", \"2023 Q3\", \"2023 Q4\",\n  \"2024 Q1\", \"2024 Q2\"\n)\n  \n  # Convert to ppp object\n  spattemp_ppp &lt;- as.ppp(st_coordinates(strategic_data_sf), st_bbox(strategic_data_sf))\n  # Remove Duplicates\n  if(any(duplicated(spattemp_ppp))){\n    rjitter(spattemp_ppp, retry=TRUE, nsim=1, drop=TRUE)\n  }\n  # Convert to owin\n  spattemp_ppp &lt;- spattemp_ppp[mmr_owin]\n  \n  # Compute the spatiotemporal Kernel Density Estimate (KDE)\n  st_kde &lt;- spattemp.density(spattemp_ppp)\n  for(x in seq_along(tims)){\n    plot(st_kde, x, \n     override.par=FALSE, \n     fix.range=TRUE, \n     main=paste(tims[x]))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to view the code\n  par(mar = c(2,0,2,0))\n  par(mfrow=c(4,4))\n  current_event = \"Violence against civilians\"\n  # Filter the data\n  violence_data_sf &lt;- grped_acled_sf %&gt;% filter(event_type == current_event)\n  \n  # Define time slices for plotting\n  tims &lt;- c(\n  \"2021 Q1\", \"2021 Q2\", \"2021 Q3\", \"2021 Q4\",\n  \"2022 Q1\", \"2022 Q2\", \"2022 Q3\", \"2022 Q4\",\n  \"2023 Q1\", \"2023 Q2\", \"2023 Q3\", \"2023 Q4\",\n  \"2024 Q1\", \"2024 Q2\"\n)\n  \n  # Convert to ppp object\n  spattemp_ppp &lt;- as.ppp(st_coordinates(violence_data_sf), st_bbox(violence_data_sf))\n  # Remove Duplicates\n  if(any(duplicated(spattemp_ppp))){\n    rjitter(spattemp_ppp, retry=TRUE, nsim=1, drop=TRUE)\n  }\n  # Convert to owin\n  spattemp_ppp &lt;- spattemp_ppp[mmr_owin]\n  \n  # Compute the spatiotemporal Kernel Density Estimate (KDE)\n  st_kde &lt;- spattemp.density(spattemp_ppp)\n  for(x in seq_along(tims)){\n    plot(st_kde, x, \n     override.par=FALSE, \n     fix.range=TRUE, \n     main=paste(tims[x]))\n}"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#spatio-temporal-kde-by-year",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#spatio-temporal-kde-by-year",
    "title": "Take-Home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "6.2 Spatio-Temporal KDE By Year",
    "text": "6.2 Spatio-Temporal KDE By Year\n\n6.2.1 Battles\n\n  current_event = \"Battles\"\n  quarter_2021_sf &lt;- grped_acled_sf %&gt;% filter(event_type == current_event, year == 2021)\n  quarter_2022_sf &lt;- grped_acled_sf %&gt;% filter(event_type == current_event, year == 2022)\n  quarter_2023_sf &lt;- grped_acled_sf %&gt;% filter(event_type == current_event, year == 2023)\n  quarter_2024_sf &lt;- grped_acled_sf %&gt;% filter(event_type == current_event, year == 2024)\n\nThis chunk of code plots the Spatio-Temporal KDE Layers by year.\n\n2021202220232024\n\n\n\n\nClick to view the code\n  par(mar = c(2,0,2,0))\n  par(mfrow=c(2,2))\n  year = 2021\n  \n  tims &lt;- c(\"Q1\", \"Q2\", \"Q3\", \"Q4\")\n  # Convert to ppp object\n  spattemp_ppp &lt;- as.ppp(st_coordinates(quarter_2021_sf), st_bbox(quarter_2021_sf))\n  # Remove Duplicates\n  if(any(duplicated(spattemp_ppp))){\n    rjitter(spattemp_ppp, retry=TRUE, nsim=1, drop=TRUE)\n  }\n  \n  # Convert to owin\n  spattemp_ppp &lt;- spattemp_ppp[mmr_owin]\n  \n  # Compute the spatiotemporal Kernel Density Estimate (KDE)\n  st_kde &lt;- spattemp.density(spattemp_ppp)\n  for(x in seq_along(tims)){\n    plot(st_kde, x, \n     override.par=FALSE, \n     fix.range=TRUE, \n     main=paste(year,tims[x]))}\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to view the code\n  par(mar = c(2,0,2,0))\n  par(mfrow=c(2,2))\n  year = 2022\n  \n  tims &lt;- c(\"Q1\", \"Q2\", \"Q3\", \"Q4\")\n  # Convert to ppp object\n  spattemp_ppp &lt;- as.ppp(st_coordinates(quarter_2022_sf), st_bbox(quarter_2022_sf))\n  # Remove Duplicates\n  if(any(duplicated(spattemp_ppp))){\n    rjitter(spattemp_ppp, retry=TRUE, nsim=1, drop=TRUE)\n  }\n  \n  # Convert to owin\n  spattemp_ppp &lt;- spattemp_ppp[mmr_owin]\n  \n  # Compute the spatiotemporal Kernel Density Estimate (KDE)\n  st_kde &lt;- spattemp.density(spattemp_ppp)\n  for(x in seq_along(tims)){\n    plot(st_kde, x, \n     override.par=FALSE, \n     fix.range=TRUE, \n     main=paste(year,tims[x]))}\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to view the code\n  par(mar = c(2,0,2,0))\n  par(mfrow=c(2,2))\n  year = 2023\n  \n  tims &lt;- c(\"Q1\", \"Q2\", \"Q3\", \"Q4\")\n  # Convert to ppp object\n  spattemp_ppp &lt;- as.ppp(st_coordinates(quarter_2023_sf), st_bbox(quarter_2023_sf))\n  # Remove Duplicates\n  if(any(duplicated(spattemp_ppp))){\n    rjitter(spattemp_ppp, retry=TRUE, nsim=1, drop=TRUE)\n  }\n  \n  # Convert to owin\n  spattemp_ppp &lt;- spattemp_ppp[mmr_owin]\n  \n  # Compute the spatiotemporal Kernel Density Estimate (KDE)\n  st_kde &lt;- spattemp.density(spattemp_ppp)\n  for(x in seq_along(tims)){\n    plot(st_kde, x, \n     override.par=FALSE, \n     fix.range=TRUE, \n     main=paste(year,tims[x]))}\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to view the code\n  par(mar = c(2,0,2,0))\n  par(mfrow=c(2,2))\n  year = 2024\n  \n  tims &lt;- c(\"Q1\", \"Q2\")\n  # Convert to ppp object\n  spattemp_ppp &lt;- as.ppp(st_coordinates(quarter_2024_sf), st_bbox(quarter_2024_sf))\n  # Remove Duplicates\n  if(any(duplicated(spattemp_ppp))){\n    rjitter(spattemp_ppp, retry=TRUE, nsim=1, drop=TRUE)\n  }\n  \n  # Convert to owin\n  spattemp_ppp &lt;- spattemp_ppp[mmr_owin]\n  \n  # Compute the spatiotemporal Kernel Density Estimate (KDE)\n  st_kde &lt;- spattemp.density(spattemp_ppp)\n  for(x in seq_along(tims)){\n    plot(st_kde, x, \n     override.par=FALSE, \n     fix.range=TRUE, \n     main=paste(year,tims[x]))}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nIn early 2021, event density was relatively sparse, with small clusters emerging in central Myanmar.\nBy Q4 of 2021, the density intensified, revealing a significant hotspot in central Myanmar, indicating escalated conflict.\nThroughout 2022 and 2023, the central region remained the most active, though the extent and concentration of hotspots fluctuated, suggesting either an escalation or a shift in conflict zones.\nIn 2024, the pattern became very dispersed, indicating further changes in conflict dynamics.\n\n\n\n\n\n6.2.2 Explosions/Remote violence\n\n  current_event = \"Explosions/Remote violence\"\n  quarter_2021_sf &lt;- grped_acled_sf %&gt;% filter(event_type == current_event, year == 2021)\n  quarter_2022_sf &lt;- grped_acled_sf %&gt;% filter(event_type == current_event, year == 2022)\n  quarter_2023_sf &lt;- grped_acled_sf %&gt;% filter(event_type == current_event, year == 2023)\n  quarter_2024_sf &lt;- grped_acled_sf %&gt;% filter(event_type == current_event, year == 2024)\n\n\n2021202220232024\n\n\n\n\nClick to view the code\n  par(mar = c(2,0,2,0))\n  par(mfrow=c(2,2))\n  year = 2021\n  \n  tims &lt;- c(\"Q1\", \"Q2\", \"Q3\", \"Q4\")\n  # Convert to ppp object\n  spattemp_ppp &lt;- as.ppp(st_coordinates(quarter_2021_sf), st_bbox(quarter_2021_sf))\n  # Remove Duplicates\n  if(any(duplicated(spattemp_ppp))){\n    rjitter(spattemp_ppp, retry=TRUE, nsim=1, drop=TRUE)\n  }\n  \n  # Convert to owin\n  spattemp_ppp &lt;- spattemp_ppp[mmr_owin]\n  \n  # Compute the spatiotemporal Kernel Density Estimate (KDE)\n  st_kde &lt;- spattemp.density(spattemp_ppp)\n  for(x in seq_along(tims)){\n    plot(st_kde, x, \n     override.par=FALSE, \n     fix.range=TRUE, \n     main=paste(year,tims[x]))}\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to view the code\n  par(mar = c(2,0,2,0))\n  par(mfrow=c(2,2))\n  year = 2022\n  \n  tims &lt;- c(\"Q1\", \"Q2\", \"Q3\", \"Q4\")\n  # Convert to ppp object\n  spattemp_ppp &lt;- as.ppp(st_coordinates(quarter_2022_sf), st_bbox(quarter_2022_sf))\n  # Remove Duplicates\n  if(any(duplicated(spattemp_ppp))){\n    rjitter(spattemp_ppp, retry=TRUE, nsim=1, drop=TRUE)\n  }\n  \n  # Convert to owin\n  spattemp_ppp &lt;- spattemp_ppp[mmr_owin]\n  \n  # Compute the spatiotemporal Kernel Density Estimate (KDE)\n  st_kde &lt;- spattemp.density(spattemp_ppp)\n  for(x in seq_along(tims)){\n    plot(st_kde, x, \n     override.par=FALSE, \n     fix.range=TRUE, \n     main=paste(year,tims[x]))}\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to view the code\n  par(mar = c(2,0,2,0))\n  par(mfrow=c(2,2))\n  year = 2023\n  \n  tims &lt;- c(\"Q1\", \"Q2\", \"Q3\", \"Q4\")\n  # Convert to ppp object\n  spattemp_ppp &lt;- as.ppp(st_coordinates(quarter_2023_sf), st_bbox(quarter_2023_sf))\n  # Remove Duplicates\n  if(any(duplicated(spattemp_ppp))){\n    rjitter(spattemp_ppp, retry=TRUE, nsim=1, drop=TRUE)\n  }\n  \n  # Convert to owin\n  spattemp_ppp &lt;- spattemp_ppp[mmr_owin]\n  \n  # Compute the spatiotemporal Kernel Density Estimate (KDE)\n  st_kde &lt;- spattemp.density(spattemp_ppp)\n  for(x in seq_along(tims)){\n    plot(st_kde, x, \n     override.par=FALSE, \n     fix.range=TRUE, \n     main=paste(year,tims[x]))}\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to view the code\n  par(mar = c(2,0,2,0))\n  par(mfrow=c(2,2))\n  year = 2024\n  \n  tims &lt;- c(\"Q1\", \"Q2\")\n  # Convert to ppp object\n  spattemp_ppp &lt;- as.ppp(st_coordinates(quarter_2024_sf), st_bbox(quarter_2024_sf))\n  # Remove Duplicates\n  if(any(duplicated(spattemp_ppp))){\n    rjitter(spattemp_ppp, retry=TRUE, nsim=1, drop=TRUE)\n  }\n  \n  # Convert to owin\n  spattemp_ppp &lt;- spattemp_ppp[mmr_owin]\n  \n  # Compute the spatiotemporal Kernel Density Estimate (KDE)\n  st_kde &lt;- spattemp.density(spattemp_ppp)\n  for(x in seq_along(tims)){\n    plot(st_kde, x, \n     override.par=FALSE, \n     fix.range=TRUE, \n     main=paste(year,tims[x]))}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nIn both 2022 and 2023, the Sagaing region of Myanmar experienced similar significant fluctuations in event density:\n\nQ1: There shows to be a high density of explosions/remote violence in the Sagaing region.\nQ2 & Q3: Conflicts subsided, likely due to localized negotiations and strategic withdrawals by armed groups, which reduced hostilities.\nQ4: Conflicts yet again intensified, surpassing even Q1. A resurgence of conflict seems to have occurred.\n\nIn Q4 of 2023, conflicts exhibited a more dispersed pattern, primarily concentrated in the northern region, with Sagaing remaining at the center of this activity.\n\n\n\n\n\n6.2.3 Strategic developments\n\n  current_event = \"Strategic developments\"\n  quarter_2021_sf &lt;- grped_acled_sf %&gt;% filter(event_type == current_event, year == 2021)\n  quarter_2022_sf &lt;- grped_acled_sf %&gt;% filter(event_type == current_event, year == 2022)\n  quarter_2023_sf &lt;- grped_acled_sf %&gt;% filter(event_type == current_event, year == 2023)\n  quarter_2024_sf &lt;- grped_acled_sf %&gt;% filter(event_type == current_event, year == 2024)\n\n\n2021202220232024\n\n\n\n\nClick to view the code\n  par(mar = c(2,0,2,0))\n  par(mfrow=c(2,2))\n  year = 2021\n  \n  tims &lt;- c(\"Q1\", \"Q2\", \"Q3\", \"Q4\")\n  # Convert to ppp object\n  spattemp_ppp &lt;- as.ppp(st_coordinates(quarter_2021_sf), st_bbox(quarter_2021_sf))\n  # Remove Duplicates\n  if(any(duplicated(spattemp_ppp))){\n    rjitter(spattemp_ppp, retry=TRUE, nsim=1, drop=TRUE)\n  }\n  \n  # Convert to owin\n  spattemp_ppp &lt;- spattemp_ppp[mmr_owin]\n  \n  # Compute the spatiotemporal Kernel Density Estimate (KDE)\n  st_kde &lt;- spattemp.density(spattemp_ppp)\n  for(x in seq_along(tims)){\n    plot(st_kde, x, \n     override.par=FALSE, \n     fix.range=TRUE, \n     main=paste(year,tims[x]))}\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to view the code\n  par(mar = c(2,0,2,0))\n  par(mfrow=c(2,2))\n  year = 2022\n  \n  tims &lt;- c(\"Q1\", \"Q2\", \"Q3\", \"Q4\")\n  # Convert to ppp object\n  spattemp_ppp &lt;- as.ppp(st_coordinates(quarter_2022_sf), st_bbox(quarter_2022_sf))\n  # Remove Duplicates\n  if(any(duplicated(spattemp_ppp))){\n    rjitter(spattemp_ppp, retry=TRUE, nsim=1, drop=TRUE)\n  }\n  \n  # Convert to owin\n  spattemp_ppp &lt;- spattemp_ppp[mmr_owin]\n  \n  # Compute the spatiotemporal Kernel Density Estimate (KDE)\n  st_kde &lt;- spattemp.density(spattemp_ppp)\n  for(x in seq_along(tims)){\n    plot(st_kde, x, \n     override.par=FALSE, \n     fix.range=TRUE, \n     main=paste(year,tims[x]))}\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to view the code\n  par(mar = c(2,0,2,0))\n  par(mfrow=c(2,2))\n  year = 2023\n  \n  tims &lt;- c(\"Q1\", \"Q2\", \"Q3\", \"Q4\")\n  # Convert to ppp object\n  spattemp_ppp &lt;- as.ppp(st_coordinates(quarter_2023_sf), st_bbox(quarter_2023_sf))\n  # Remove Duplicates\n  if(any(duplicated(spattemp_ppp))){\n    rjitter(spattemp_ppp, retry=TRUE, nsim=1, drop=TRUE)\n  }\n  \n  # Convert to owin\n  spattemp_ppp &lt;- spattemp_ppp[mmr_owin]\n  \n  # Compute the spatiotemporal Kernel Density Estimate (KDE)\n  st_kde &lt;- spattemp.density(spattemp_ppp)\n  for(x in seq_along(tims)){\n    plot(st_kde, x, \n     override.par=FALSE, \n     fix.range=TRUE, \n     main=paste(year,tims[x]))}\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to view the code\n  par(mar = c(2,0,2,0))\n  par(mfrow=c(2,2))\n  year = 2024\n  \n  tims &lt;- c(\"Q1\", \"Q2\")\n  # Convert to ppp object\n  spattemp_ppp &lt;- as.ppp(st_coordinates(quarter_2024_sf), st_bbox(quarter_2024_sf))\n  # Remove Duplicates\n  if(any(duplicated(spattemp_ppp))){\n    rjitter(spattemp_ppp, retry=TRUE, nsim=1, drop=TRUE)\n  }\n  \n  # Convert to owin\n  spattemp_ppp &lt;- spattemp_ppp[mmr_owin]\n  \n  # Compute the spatiotemporal Kernel Density Estimate (KDE)\n  st_kde &lt;- spattemp.density(spattemp_ppp)\n  for(x in seq_along(tims)){\n    plot(st_kde, x, \n     override.par=FALSE, \n     fix.range=TRUE, \n     main=paste(year,tims[x]))}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDuring Q2 to Q4 of 2023, there was limited activity in strategic developments, but this changed abruptly with a significant intensification in Q1 of 2024.\n\n\n\n\n6.2.4 Violence against civilians\n\n  current_event = \"Violence against civilians\"\n  quarter_2021_sf &lt;- grped_acled_sf %&gt;% filter(event_type == current_event, year == 2021)\n  quarter_2022_sf &lt;- grped_acled_sf %&gt;% filter(event_type == current_event, year == 2022)\n  quarter_2023_sf &lt;- grped_acled_sf %&gt;% filter(event_type == current_event, year == 2023)\n  quarter_2024_sf &lt;- grped_acled_sf %&gt;% filter(event_type == current_event, year == 2024)\n\n\n2021202220232024\n\n\n\n\nClick to view the code\n  par(mar = c(2,0,2,0))\n  par(mfrow=c(2,2))\n  year = 2021\n  \n  tims &lt;- c(\"Q1\", \"Q2\", \"Q3\", \"Q4\")\n  # Convert to ppp object\n  spattemp_ppp &lt;- as.ppp(st_coordinates(quarter_2021_sf), st_bbox(quarter_2021_sf))\n  # Remove Duplicates\n  if(any(duplicated(spattemp_ppp))){\n    rjitter(spattemp_ppp, retry=TRUE, nsim=1, drop=TRUE)\n  }\n  \n  # Convert to owin\n  spattemp_ppp &lt;- spattemp_ppp[mmr_owin]\n  \n  # Compute the spatiotemporal Kernel Density Estimate (KDE)\n  st_kde &lt;- spattemp.density(spattemp_ppp)\n  for(x in seq_along(tims)){\n    plot(st_kde, x, \n     override.par=FALSE, \n     fix.range=TRUE, \n     main=paste(year,tims[x]))}\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to view the code\n  par(mar = c(2,0,2,0))\n  par(mfrow=c(2,2))\n  year = 2022\n  \n  tims &lt;- c(\"Q1\", \"Q2\", \"Q3\", \"Q4\")\n  # Convert to ppp object\n  spattemp_ppp &lt;- as.ppp(st_coordinates(quarter_2022_sf), st_bbox(quarter_2022_sf))\n  # Remove Duplicates\n  if(any(duplicated(spattemp_ppp))){\n    rjitter(spattemp_ppp, retry=TRUE, nsim=1, drop=TRUE)\n  }\n  \n  # Convert to owin\n  spattemp_ppp &lt;- spattemp_ppp[mmr_owin]\n  \n  # Compute the spatiotemporal Kernel Density Estimate (KDE)\n  st_kde &lt;- spattemp.density(spattemp_ppp)\n  for(x in seq_along(tims)){\n    plot(st_kde, x, \n     override.par=FALSE, \n     fix.range=TRUE, \n     main=paste(year,tims[x]))}\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to view the code\n  par(mar = c(2,0,2,0))\n  par(mfrow=c(2,2))\n  year = 2023\n  \n  tims &lt;- c(\"Q1\", \"Q2\", \"Q3\", \"Q4\")\n  # Convert to ppp object\n  spattemp_ppp &lt;- as.ppp(st_coordinates(quarter_2023_sf), st_bbox(quarter_2023_sf))\n  # Remove Duplicates\n  if(any(duplicated(spattemp_ppp))){\n    rjitter(spattemp_ppp, retry=TRUE, nsim=1, drop=TRUE)\n  }\n  \n  # Convert to owin\n  spattemp_ppp &lt;- spattemp_ppp[mmr_owin]\n  \n  # Compute the spatiotemporal Kernel Density Estimate (KDE)\n  st_kde &lt;- spattemp.density(spattemp_ppp)\n  for(x in seq_along(tims)){\n    plot(st_kde, x, \n     override.par=FALSE, \n     fix.range=TRUE, \n     main=paste(year,tims[x]))}\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick to view the code\n  par(mar = c(2,0,2,0))\n  par(mfrow=c(2,2))\n  year = 2024\n  \n  tims &lt;- c(\"Q1\", \"Q2\")\n  # Convert to ppp object\n  spattemp_ppp &lt;- as.ppp(st_coordinates(quarter_2024_sf), st_bbox(quarter_2024_sf))\n  # Remove Duplicates\n  if(any(duplicated(spattemp_ppp))){\n    rjitter(spattemp_ppp, retry=TRUE, nsim=1, drop=TRUE)\n  }\n  \n  # Convert to owin\n  spattemp_ppp &lt;- spattemp_ppp[mmr_owin]\n  \n  # Compute the spatiotemporal Kernel Density Estimate (KDE)\n  st_kde &lt;- spattemp.density(spattemp_ppp)\n  for(x in seq_along(tims)){\n    plot(st_kde, x, \n     override.par=FALSE, \n     fix.range=TRUE, \n     main=paste(year,tims[x]))}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn 2024, incidents of violence against civilians became more dispersed, featuring a significant hotspot of high density in the Sagaing region, there also appears to be some concentration of activity in southern Myanmar that was not commonly observed in other plots."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#learning-process",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#learning-process",
    "title": "Take-Home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "7.1 Learning Process",
    "text": "7.1 Learning Process\n\n7.1.1 Starting Out\nTo start off, I researched on the potential Second Order Spatio-temporal Point Patterns Analysis methods I could use and ended up shortlisting out these options:\n\nSpatio-temporal K-function: This function extends the traditional K-function to consider both space and time, allowing us to assess how conflicts cluster over time and across different locations.\nRipley’s K-function: Specifically, the K-function can be adapted for spatio-temporal analysis to evaluate the density of events in both dimensions. This helps in identifying patterns of clustering or dispersion over specified time intervals.\nLog-Gaussian Cox Process (LGCP): This is a flexible model that accounts for spatial dependence in point patterns and allows for varying intensity across space and time. LGCP can help in modeling the intensity of conflict events as a function of spatial covariates, providing insights into the underlying processes driving the distribution of conflicts.\n\nInstinctively, I was inclined to stick with the K-function since it is a method I am familiar with and have applied before, making it a comfortable starting point for my analysis. However, I also recognized that the term “Log-Gaussian Cox Process” appeared frequently in the literature I reviewed, which prompted me to include it in my shortlist.\n\n\n7.1.2 Failed Attempts\nDespite trying many different functions and methods, not many of them came through. A method that I had initially thought worked with the help of ChatGPT was using time (in this case quarter number) to mark the point patterns. However, I soon realised after that the standard Kest function that we used in class does not seem to take into account marks.\n\nUsing Mark for K-Function\n\ncurrent_event &lt;- \"Battles\"\n\nquarter_to_continuous &lt;- function(year, quarter) {\n  # Calculate the number of quarters since a reference point (e.g., 2000 Q1)\n  reference_year &lt;- 2021\n  quarters_since_reference &lt;- (year - reference_year) * 4 + quarter\n  return(quarters_since_reference)\n}\n\ngrped_acled_quartered_sf &lt;- acled_sf %&gt;%   mutate(\n                                                          year = as.numeric(year),\n                                                          quarter = as.numeric(quarter),\n                                                          t = quarter_to_continuous(year, quarter)\n                                                        )\ngrped_acled_quartered_sf &lt;- grped_acled_quartered_sf %&gt;% filter(event_type == current_event)\n\n# Step 2: Convert the data to a spatio-temporal point pattern\n# Define the spatial window (using your bounding box)\n\n# Create a spatio-temporal point pattern (ppp is the spatial part, time is added separately)\nppp_data &lt;- as.ppp(st_coordinates(grped_acled_quartered_sf), W = mmr_owin)\nmarks(ppp_data) &lt;- data.frame(time = acled_sf$t)\n\nKst_result &lt;- Kest(ppp_data, tmax = 1, rmax = 10000)\n\n# Step 4: Visualize and interpret the results\nplot(Kst_result)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#space-time-inhomogeneous-k-function",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#space-time-inhomogeneous-k-function",
    "title": "Take-Home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "7.2 Space-Time Inhomogeneous K-function",
    "text": "7.2 Space-Time Inhomogeneous K-function\nThe method that I eventually ended up going for, and succeeding in getting to actually work (or at least have some semblance of success) is using STIKhat() from the package stpp which is said to be designed for “Space-Time Point Pattern Simulation, Visualisation and Analysis”.\nSTIKhat(): Compute an estimate of the Space-Time Inhomogeneous K-function.\nThis chunk of code converts acled data to continuous quarter for time slices and gets rid of duplicate rows with the same event type, geometry and t (overall quarter no, e.g. 2024 Q2 –&gt; 14).\n\n  quarter_to_continuous &lt;- function(year, quarter) {\n    # Calculate the number of quarters since a reference point (e.g., 2000 Q1)\n    reference_year &lt;- 2021\n    quarters_since_reference &lt;- (year - reference_year) * 4 + quarter\n    return(quarters_since_reference)\n  }\n  \n  grped_acled_quartered_sf &lt;- acled_sf %&gt;%   mutate(year = as.numeric(year),\n                                                    quarter = as.numeric(quarter),\n                                                    t = quarter_to_continuous(year, quarter)\n                                                    ) %&gt;% distinct(event_type, geometry, t, .keep_all = TRUE)\n\n\n\n\n\n\n\nImportant\n\n\n\nThere seems to be an issue where the plots appear in the viewer of Rstudio but yet I am unable to have it rendered and appear on the Quarto Document. Hence I have made the choice to instead download and display it instead.\n\n\nThis first chunk of code utilises STIKhat() from the stpp package to compute an estimate of the Space-Time Inhomogeneous K-function and plotK() to plot the graph.\nThe second chunk of code then plots the different graphical representation of the estimate of the Space-Time Inhomogeneous K-function: Projection, Perspective, Image so that we are able to get a view of the different graphs.\n\nBattlesExplosions/ Remote ViolenceStrategic DevelopmentsViolence Against Civilians\n\n\n\n\nClick to view the code\n  current_event &lt;- \"Battles\"\n\n  battle_quartered_sf &lt;- grped_acled_quartered_sf %&gt;% filter(event_type == current_event)\n  coords_matrix &lt;- st_coordinates(battle_quartered_sf)\n  x_coords &lt;- coords_matrix[, \"X\"]\n  y_coords &lt;- coords_matrix[, \"Y\"]\n  \n  stpp_data &lt;- as.3dpoints(x_coords,y_coords, battle_quartered_sf$t)\n  \n  Kst_result &lt;- stpp::STIKhat(xyt=stpp_data)\n\n  write_rds(Kst_result, \"data/rds/battle_stik_result.rds\")\n\n\n\nplotK(battle_stik_result)\nplotK(battle_stik_result, type =\"persp\")\nplotK(battle_stik_result, type =\"image\")\n\n\n\n\n\n\nBattle Projection\n\n\n\n\n\nBattle Perspective\n\n\n\n\n\n\n\nBattle Image\n\n\n\n\n\n\n\n\nClick to view the code\n  current_event &lt;- \"Explosions/Remote violence\"\n\n  explosions_quartered_sf &lt;- grped_acled_quartered_sf %&gt;% filter(event_type == current_event)\n  \n  coords_matrix &lt;- st_coordinates(explosions_quartered_sf)\n  x_coords &lt;- coords_matrix[, \"X\"]\n  y_coords &lt;- coords_matrix[, \"Y\"]\n  \n  stpp_data &lt;- as.3dpoints(x_coords,y_coords, explosions_quartered_sf$t)\n  \n  Kst_result &lt;- stpp::STIKhat(xyt=stpp_data)\n\n  write_rds(Kst_result, \"data/rds/explosions_stik_result.rds\")\n\n\n\nplotK(explosions_stik_result)\nplotK(explosions_stik_result, type =\"persp\")\nplotK(explosions_stik_result, type =\"image\")\n\n\n\n\n\n\nExplosions Projection\n\n\n\n\n\nExplosions Perspective\n\n\n\n\n\n\n\nExplosions Image\n\n\n\n\n\n\n\n\nClick to view the code\n  current_event &lt;- \"Strategic developments\"\n\n  strategic_quartered_sf &lt;- grped_acled_quartered_sf %&gt;% filter(event_type == current_event)\n  \n  coords_matrix &lt;- st_coordinates(strategic_quartered_sf)\n  x_coords &lt;- coords_matrix[, \"X\"]\n  y_coords &lt;- coords_matrix[, \"Y\"]\n  \n  stpp_data &lt;- as.3dpoints(x_coords,y_coords, strategic_quartered_sf$t)\n  \n  Kst_result &lt;- stpp::STIKhat(xyt=stpp_data)\n\n  write_rds(Kst_result, \"data/rds/strategic_stik_result.rds\")\n\n\n\nplotK(strategic_stik_result)\nplotK(strategic_stik_result, type =\"persp\")\nplotK(strategic_stik_result, type =\"image\")\n\n\n\n\n\n\nStrategic Projection\n\n\n\n\n\nStrategic Perspective\n\n\n\n\n\n\n\nStrategic Image\n\n\n\n\n\n\n\n\nClick to view the code\n  current_event &lt;- \"Violence against civilians\"\n  violence_quartered_sf &lt;- grped_acled_quartered_sf %&gt;% filter(event_type == current_event)\n  \n  coords_matrix &lt;- st_coordinates(violence_quartered_sf)\n  x_coords &lt;- coords_matrix[, \"X\"]\n  y_coords &lt;- coords_matrix[, \"Y\"]\n  \n  stpp_data &lt;- as.3dpoints(x_coords,y_coords, violence_quartered_sf$t)\n  \n  Kst_result &lt;- stpp::STIKhat(xyt=stpp_data)\n\n  write_rds(Kst_result, \"data/rds/violence_stik_result.rds\")\n\n\n\nplotK(violence_stik_result)\nplotK(violence_stik_result, type =\"persp\")\nplotK(violence_stik_result, type =\"image\")\n\n\n\n\n\n\nViolence Projection\n\n\n\n\n\nViolence Perspective\n\n\n\n\n\n\n\nViolence Image"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#interpreting",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#interpreting",
    "title": "Take-Home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "7.3 Interpreting",
    "text": "7.3 Interpreting\nDifference between the plots\n\nThe projection plot shows contours of intensity over space and time.\nThe image plot provides a heatmap view, with darker blue indicating higher intensity.\nThe perspective plot gives a 3D visualization of the intensity surface.\n\nOverall Insights\n\nAll four types of event show spatiotemporal clustering, indicating non-random patterns.\nThere’s a general trend of increasing intensity over time for all event types, which could suggest an escalation of conflict.\nThe similar intensity scales across all event types suggest that no single type of event dominates the conflict in terms of frequency or intensity."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello",
    "section": "",
    "text": "Welcome to IS415 Geospatial Analytics and Applications. In this website, you will find my coursework prepared for this course.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On Exercise 10: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2024\n\n\nGeorgia Ng\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On Exercise 9: Geographical Segmentation with Spatially Constrained Clustering Techniques Part 2\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2024\n\n\nGeorgia Ng\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On Exercise 8: Geographical Segmentation with Spatially Constrained Clustering Techniques\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2024\n\n\nGeorgia Ng\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On Exercise 6: Global & Local Measures of Spatial Autocorrelation\n\n\n\n\n\n\n\n\n\n\n\nSep 21, 2024\n\n\nGeorgia Ng\n\n\n\n\n\n\n\n\n\n\n\n\nHands On Exercise 5: Spatial Weights and Applications\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2024\n\n\nGeorgia Ng\n\n\n\n\n\n\n\n\n\n\n\n\nHands On Exercise 4: Spatio-Temporal Point Patterns Analysis\n\n\n\n\n\n\n\n\n\n\n\nSep 10, 2024\n\n\nGeorgia Ng\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On Exercise 3: 1st Order Spatial Point Patterns Analysis Methods\n\n\n\n\n\n\n\n\n\n\n\nAug 27, 2024\n\n\nGeorgia Ng\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On Exercise 2: Thematic Mapping and GeoVisualisation with R\n\n\n\n\n\n\n\n\n\n\n\nAug 23, 2024\n\n\nGeorgia Ng\n\n\n\n\n\n\n\n\n\n\n\n\nHands-On Exercise 1: Geospatial Data Science with R\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2024\n\n\nGeorgia Ng\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "title": "Hands-On Exercise 1: Geospatial Data Science with R",
    "section": "",
    "text": "In this exercise, I learn to handle geospatial data files and some basic data science tasks with sf.\nUse pacman::p_load to install and load sf and tidyverse"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#overview",
    "title": "Hands-On Exercise 1: Geospatial Data Science with R",
    "section": "",
    "text": "In this exercise, I learn to handle geospatial data files and some basic data science tasks with sf.\nUse pacman::p_load to install and load sf and tidyverse"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#data-acquisition",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#data-acquisition",
    "title": "Hands-On Exercise 1: Geospatial Data Science with R",
    "section": "1.2 Data Acquisition",
    "text": "1.2 Data Acquisition\nData are key to data analytics including geospatial analytics. Hence, before analysing, we need to assemble the necessary data.\n\nMaster Plan 2014 Subzone Boundary (Web) from data.gov.sg\nPre-Schools Location from data.gov.sg\nCycling Path from LTADataMall\nLatest version of Singapore Airbnb listing data from Inside Airbnb"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#getting-started",
    "title": "Hands-On Exercise 1: Geospatial Data Science with R",
    "section": "1.3 Getting Started",
    "text": "1.3 Getting Started\nIn this hands-on exercise, two R packages will be used. They are:\n\nsf for importing, managing, and processing geospatial data, and\ntidyverse for performing data science tasks such as importing, wrangling and visualising data.\n\nTidyverse consists of a family of R packages. In this hands-on exercise, the following packages will be used:\n\nreadr for importing csv data,\nreadxl for importing Excel worksheet,\ntidyr for manipulating data,\ndplyr for transforming data, and\nggplot2 for visualising data\n\nThe code chunk below uses p_load() of pacman package to check if tidyverse packages are installed in the computer. If they are, then they will be launched into R.\n\npacman::p_load(sf,tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-geospatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-geospatial-data",
    "title": "Hands-On Exercise 1: Geospatial Data Science with R",
    "section": "1.4 Importing Geospatial Data",
    "text": "1.4 Importing Geospatial Data\nWe will be using st_read() of sf package:\n\nMP14_SUBZONE_WEB_PL, a polygon feature layer in ESRI shapefile format,\nCyclingPath, a line feature layer in ESRI shapefile format, and\nPreSchool, a point feature layer in kml file format.\n\n\n1.4.1 Importing polygon feature data in shapefile format\nThe code chunk below uses st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a polygon feature data frame. Note that when the input geospatial data is in shapefile format, two arguments will be used, namely: dsn to define the data path and layer to provide the shapefile name. Also note that no extension such as .shp, .dbf, .prj and .shx are needed.\n\nmpsz = st_read(dsn = \"data/geospatial/MasterPlan2014SubzoneBoundaryWebSHP\", \n                  layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/georgiaxng/georgiaxng/is415-handson/Hands-on_Ex/Hands-on_Ex01/data/geospatial/MasterPlan2014SubzoneBoundaryWebSHP' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nFrom the output message, we can see that in our mpsz simple feature data frame, there are 323 multipolygon features, 15 fields and is in the svy21 projected coordinates system.\n\n\n1.4.2 Importing polyline feature data in shapefile form\nThe code chunk below uses st_read() function of sf package to import CyclingPath shapefile into R as line feature data frame.\n\ncyclingpath = st_read(dsn = \"data/geospatial/CyclingPath_Jul2024\", \n                         layer = \"CyclingPathGazette\")\n\nReading layer `CyclingPathGazette' from data source \n  `/Users/georgiaxng/georgiaxng/is415-handson/Hands-on_Ex/Hands-on_Ex01/data/geospatial/CyclingPath_Jul2024' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3138 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42644.17 ymax: 48948.15\nProjected CRS: SVY21\n\n\nThe message above reveals that there are a total of 3138 features and 2 fields in cyclingpath linestring feature data frame and it is in svy21 projected coordinates system too.\n\n\n1.4.3 Importing GIS data in kml format\n\npreschool = st_read(\"data/geospatial/PreSchoolsLocation.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `/Users/georgiaxng/georgiaxng/is415-handson/Hands-on_Ex/Hands-on_Ex01/data/geospatial/PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nThe message above reveals that preschool is a point feature data frame. There are a total of 2290 features and 2 fields. Different from the previous two simple feature data frame, preschool is in wgs84 coordinates system."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#checking-the-content-of-a-simple-feature-data-frame",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#checking-the-content-of-a-simple-feature-data-frame",
    "title": "Hands-On Exercise 1: Geospatial Data Science with R",
    "section": "1.5 Checking the Content of A Simple Feature Data Frame",
    "text": "1.5 Checking the Content of A Simple Feature Data Frame\nIn this sub-section, we will use different ways to retrieve information related to the content of a simple feature data frame.\n\n1.5.1 st_geometry()\nThe column in the sf data.frame that contains the geometries is a list, of class sfc. We can retrieve the geometry list-column in this case by mpsz$geom or mpsz[[1]], but the more general way uses st_geometry() as shown in the code chunk below.\nst_geometry prints basic info on the feature class, displaying basic information of the feature class such as type of geometry, the geographic extent of the features and the coordinate system of the data\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\n\n\n1.5.2 glimpse()\nglimpse reveals the data type of each fields\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\n\n\n1.5.3 head()\nSometimes we would like to reveal complete information of a feature object, this is the job of head() of Base R\n\nhead(mpsz, n=5)  \n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30..."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#plotting-the-geospatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#plotting-the-geospatial-data",
    "title": "Hands-On Exercise 1: Geospatial Data Science with R",
    "section": "1.6 Plotting the Geospatial Data",
    "text": "1.6 Plotting the Geospatial Data\nIn geospatial data science, it is not enough to just look at the feature information. We are also interested to visualise the geospatial features. This is where plot() of R Graphic comes in very handy as shown in the code chunk below.\n\nplot(mpsz)\n\n\n\n\n\n\n\n\nThe default plot of an sf object is a multi-plot of all attributes, up to a reasonable maximum as shown above. We can, however, choose to plot only the geometry by using the code chunk below.\n\nplot(st_geometry(mpsz))\n\n\n\n\n\n\n\n\nAlternatively, we can also choose the plot the sf object by using a specific attribute as shown in the code chunk below.\n\nplot(mpsz[\"PLN_AREA_N\"])\n\n\n\n\n\n\n\n\n\nNote: plot() is mean for plotting the geospatial object for quick look. For high cartographic quality plot, other R package such as tmap should be used."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-projection",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-projection",
    "title": "Hands-On Exercise 1: Geospatial Data Science with R",
    "section": "1.7 Working with Projection",
    "text": "1.7 Working with Projection\nMap projection is an important property of a geospatial data. In order to perform geoprocessing using two geospatial data, we need to ensure that both geospatial data are projected using similar coordinate system.\nIn this section, we will learn how to project a simple feature data frame from one coordinate system to another coordinate system. The technical term of this process is called projection transformation.\n\n1.7.1 Assigning EPSG code to a simple feature data frame\nOne of the common issue that can happen during importing geospatial data into R is that the coordinate system of the source data was either missing (such as due to missing .proj for ESRI shapefile) or wrongly assigned during the importing process.\nThis is an example the coordinate system of mpsz simple feature data frame by using st_crs() of sf package as shown in the code chunk below.\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nAlthough mpsz data frame is projected in svy21 but when we read until the end of the print, it indicates that the EPSG is 9001. This is a wrong EPSG code because the correct EPSG code for svy21 should be 3414.\nIn order to assign the correct EPSG code to mpsz data frame, st_set_crs() of sf package is used as shown in the code chunk below.\n\nmpsz3414 &lt;- st_set_crs(mpsz, 3414)\n\nNow, let us check the CSR again by using the code chunk below.\n\nst_crs(mpsz3414)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nNotice that the EPSG code is 3414 now.\n\n\n1.7.2 Transforming the projection of preschool from wgs84 to svy21\nIn geospatial analytics, it is very common for us to transform the original data from geographic coordinate system to projected coordinate system. This is because geographic coordinate system is not appropriate if the analysis need to use distance or/and area measurements.\nLet us take preschool simple feature data frame as an example. The print below reveals that it is in wgs84 coordinate system.\n\nst_geometry(preschool)\n\nGeometry set for 2290 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\nThis is a scenario that st_set_crs() is not appropriate and st_transform() of sf package should be used. This is because we need to reproject preschool from one coordinate system to another coordinate system mathemetically.\nLet us perform the projection transformation by using the code chunk below.\n\npreschool3414 &lt;- st_transform(preschool, \n                              crs = 3414)\n\n\nNote: In practice, we need find out the appropriate project coordinate system to use before performing the projection transformation.\n\nNext, let us display the content of preschool3414 sf data frame as shown below.\n\nst_geometry(preschool3414)\n\nGeometry set for 2290 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 11810.03 ymin: 25596.33 xmax: 45404.24 ymax: 49300.88\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:\n\n\n\nNotice that it is in svy21 projected coordinate system now. Furthermore, if you refer to Bounding box:, the values are greater than 0-360 range of decimal degree commonly used by most of the geographic coordinate systems."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-and-converting-an-aspatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-and-converting-an-aspatial-data",
    "title": "Hands-On Exercise 1: Geospatial Data Science with R",
    "section": "1.8 Importing and Converting An Aspatial Data",
    "text": "1.8 Importing and Converting An Aspatial Data\nAn example of aspatial data would be listing of Inside Airbnb. This is because it is not a geospatial data but among the data fields, there are two fields that capture the x- and y-coordinates of the data points.\nIn this section, we will be importing an aspatial data into R environment and save it as a tibble data frame. Next, we will convert it into a simple feature data frame.\n\n1.8.1 Importing the aspatial data\n\nlistings &lt;- read_csv(\"data/aspatial/listings.csv\")\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly.\nThe code chunk below shows list() of Base R instead of glimpse() is used to do the job.\n\nlist(listings) \n\n[[1]]\n# A tibble: 3,540 × 18\n       id name      host_id host_name neighbourhood_group neighbourhood latitude\n    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;               &lt;chr&gt;            &lt;dbl&gt;\n 1  71609 Ensuite …  367042 Belinda   East Region         Tampines          1.35\n 2  71896 B&B  Roo…  367042 Belinda   East Region         Tampines          1.35\n 3  71903 Room 2-n…  367042 Belinda   East Region         Tampines          1.35\n 4 275343 10min wa… 1439258 Kay       Central Region      Bukit Merah       1.29\n 5 275344 15 mins … 1439258 Kay       Central Region      Bukit Merah       1.29\n 6 289234 Booking …  367042 Belinda   East Region         Tampines          1.34\n 7 294281 5 mins w… 1521514 Elizabeth Central Region      Newton            1.31\n 8 324945 Comforta… 1439258 Kay       Central Region      Bukit Merah       1.29\n 9 330095 Relaxing… 1439258 Kay       Central Region      Bukit Merah       1.29\n10 344803 Budget s…  367042 Belinda   East Region         Tampines          1.35\n# ℹ 3,530 more rows\n# ℹ 11 more variables: longitude &lt;dbl&gt;, room_type &lt;chr&gt;, price &lt;dbl&gt;,\n#   minimum_nights &lt;dbl&gt;, number_of_reviews &lt;dbl&gt;, last_review &lt;date&gt;,\n#   reviews_per_month &lt;dbl&gt;, calculated_host_listings_count &lt;dbl&gt;,\n#   availability_365 &lt;dbl&gt;, number_of_reviews_ltm &lt;dbl&gt;, license &lt;chr&gt;\n\n\n\n\n1.8.2 Creating a simple feature data frame from an aspatial data frame\nThe code chunk below converts listing data frame into a simple feature data frame by using st_as_sf() of sf packages\n\nlistings_sf &lt;- st_as_sf(listings, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %&gt;%\n  st_transform(crs = 3414)\n\nThings to learn from the arguments above:\n\ncoords argument requires you to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates.\ncrs argument requires you to provide the coordinates system in epsg format. EPSG: 4326 is wgs84 Geographic Coordinate System and EPSG: 3414 is Singapore SVY21 Projected Coordinate System. You can search for other country’s epsg code by referring to epsg.io.\n%&gt;% is used to nest st_transform() to transform the newly created simple feature data frame into svy21 projected coordinates system.\n\nLet us examine the content of this newly created simple feature data frame.\n\nglimpse(listings_sf)\n\nRows: 3,540\nColumns: 17\n$ id                             &lt;dbl&gt; 71609, 71896, 71903, 275343, 275344, 28…\n$ name                           &lt;chr&gt; \"Ensuite Room (Room 1 & 2) near EXPO\", …\n$ host_id                        &lt;dbl&gt; 367042, 367042, 367042, 1439258, 143925…\n$ host_name                      &lt;chr&gt; \"Belinda\", \"Belinda\", \"Belinda\", \"Kay\",…\n$ neighbourhood_group            &lt;chr&gt; \"East Region\", \"East Region\", \"East Reg…\n$ neighbourhood                  &lt;chr&gt; \"Tampines\", \"Tampines\", \"Tampines\", \"Bu…\n$ room_type                      &lt;chr&gt; \"Private room\", \"Private room\", \"Privat…\n$ price                          &lt;dbl&gt; NA, 80, 80, 50, 50, NA, 85, 65, 45, 54,…\n$ minimum_nights                 &lt;dbl&gt; 92, 92, 92, 180, 180, 92, 92, 180, 180,…\n$ number_of_reviews              &lt;dbl&gt; 19, 24, 46, 20, 16, 12, 131, 17, 5, 60,…\n$ last_review                    &lt;date&gt; 2020-01-17, 2019-10-13, 2020-01-09, 20…\n$ reviews_per_month              &lt;dbl&gt; 0.12, 0.15, 0.29, 0.15, 0.11, 0.08, 0.8…\n$ calculated_host_listings_count &lt;dbl&gt; 6, 6, 6, 49, 49, 6, 7, 49, 49, 6, 7, 7,…\n$ availability_365               &lt;dbl&gt; 89, 148, 90, 62, 0, 88, 365, 0, 0, 365,…\n$ number_of_reviews_ltm          &lt;dbl&gt; 0, 0, 0, 0, 2, 0, 0, 1, 1, 1, 0, 0, 0, …\n$ license                        &lt;chr&gt; NA, NA, NA, \"S0399\", \"S0399\", NA, NA, \"…\n$ geometry                       &lt;POINT [m]&gt; POINT (41972.5 36390.05), POINT (…\n\n\nTable above shows the content of listing_sf. Notice that a new column called geometry has been added into the data frame. On the other hand, the longitude and latitude columns have been dropped from the data frame."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geoprocessing-with-sf-package",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geoprocessing-with-sf-package",
    "title": "Hands-On Exercise 1: Geospatial Data Science with R",
    "section": "1.9 Geoprocessing with sf package",
    "text": "1.9 Geoprocessing with sf package\nIn this section, we will perform two commonly used geoprocessing functions, namely buffering and point in polygon count.\n\n1.9.1 Buffering\nThe scenario:\nThe authority is planning to upgrade the exiting cycling path. To do so, they need to acquire 5 metres of reserved land on the both sides of the current cycling path. You are tasked to determine the extend of the land need to be acquired and their total area.\nThe solution:\nFirstly, st_buffer() of sf package is used to compute the 5-meter buffers around cycling paths\n\nbuffer_cycling &lt;- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 30)\n\nThis is followed by calculating the area of the buffers as shown in the code chunk below.\n\nbuffer_cycling$AREA &lt;- st_area(buffer_cycling)\n\nLastly, sum() of Base R will be used to derive the total land involved\n\nsum(buffer_cycling$AREA)\n\n2218855 [m^2]\n\n\n\n\n1.9.2 Point-in-polygon count\nThe scenario:\nA pre-school service group want to find out the numbers of pre-schools in each Planning Subzone.\nThe solution:\nThe code chunk below performs two operations at one go. Firstly, identify pre-schools located inside each Planning Subzone by using st_intersects(). Next, length() of Base R is used to calculate numbers of pre-schools that fall inside each planning subzone.\n\nmpsz3414$`PreSch Count`&lt;- lengths(st_intersects(mpsz3414, preschool3414))\n\nWe can check the summary statistics of the newly derived PreSch Count field by using summary() as shown in the code chunk below.\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    4.00    7.09   10.00   72.00 \n\n\nTo list the planning subzone with the most number of pre-school, the top_n() of dplyr package is used as shown in the code chunk below.\n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 39655.33 ymin: 35966 xmax: 42940.57 ymax: 38622.37\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO     SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      189          2 TAMPINES EAST    TMSZ02      N   TAMPINES         TM\n     REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng\n1 EAST REGION       ER 21658EAAF84F4D8D 2014-12-05 41122.55 37392.39   10180.62\n  SHAPE_Area                       geometry PreSch Count\n1    4339824 MULTIPOLYGON (((42196.76 38...           72\n\n\nCalculating Density of Preschool by planning subzone\nFirstly, the code chunk below uses st_area() of sf package to derive the area of each planning subzone.\n\nmpsz3414$Area &lt;- mpsz3414 %&gt;%\n  st_area()\n\nNext, mutate() of dplyr package is used to compute the density by using the code chunk below.\n\nmpsz3414 &lt;- mpsz3414 %&gt;%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#exploratory-data-analysis-eda",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#exploratory-data-analysis-eda",
    "title": "Hands-On Exercise 1: Geospatial Data Science with R",
    "section": "1.10 Exploratory Data Analysis (EDA)",
    "text": "1.10 Exploratory Data Analysis (EDA)\nIn practice, many geospatial analytics start with Exploratory Data Analysis. In this section, you will learn how to use appropriate ggplot2 functions to create functional and yet truthful statistical graphs for EDA purposes.\nFirstly, we will plot a histogram to reveal the distribution of PreSch Density. Conventionally, hist() of R Graphics will be used as shown in the code chunk below.\n\nhist(mpsz3414$`PreSch Density`)\n\n\n\n\n\n\n\n\nAlthough the syntax is very easy to use however the output is far from meeting publication quality. Furthermore, the function has limited room for further customisation.\nIn the code chunk below, appropriate ggplot2 functions will be used.\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\n\n\n\n\n\n\n\nUsing ggplot2 method to plot a scatterplot showing the relationship between Pre-school Density and Pre-school Count:\n\nggplot(data=mpsz3414, \n       aes(y = `PreSch Count`, \n           x= as.numeric(`PreSch Density`)))+\n  geom_point(color=\"black\", \n             fill=\"light blue\") +\n  xlim(0, 40) +\n  ylim(0, 40) +\n  labs(title = \"\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Pre-school count\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html",
    "title": "Hands-On Exercise 8: Geographical Segmentation with Spatially Constrained Clustering Techniques",
    "section": "",
    "text": "In this hands-on exercise, you will gain hands-on experience on how to delineate homogeneous region by using geographically referenced multivariate data. There are two major analysis, namely:\n\nhierarchical cluster analysis; and\nspatially constrained cluster analysis.\n\n\n\nBy the end of this hands-on exercise, you will able:\n\nto convert GIS polygon data into R’s simple feature data.frame by using appropriate functions of sf package of R;\nto convert simple feature data.frame into R’s SpatialPolygonDataFrame object by using appropriate sf of package of R;\nto perform custer analysis by using hclust() of Base R;\nto perform spatially constrained cluster analysis using skater() of Base R; and\nto visualise the analysis output by using ggplot2 and tmap package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#overview",
    "title": "Hands-On Exercise 8: Geographical Segmentation with Spatially Constrained Clustering Techniques",
    "section": "",
    "text": "In this hands-on exercise, you will gain hands-on experience on how to delineate homogeneous region by using geographically referenced multivariate data. There are two major analysis, namely:\n\nhierarchical cluster analysis; and\nspatially constrained cluster analysis.\n\n\n\nBy the end of this hands-on exercise, you will able:\n\nto convert GIS polygon data into R’s simple feature data.frame by using appropriate functions of sf package of R;\nto convert simple feature data.frame into R’s SpatialPolygonDataFrame object by using appropriate sf of package of R;\nto perform custer analysis by using hclust() of Base R;\nto perform spatially constrained cluster analysis using skater() of Base R; and\nto visualise the analysis output by using ggplot2 and tmap package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#getting-started",
    "title": "Hands-On Exercise 8: Geographical Segmentation with Spatially Constrained Clustering Techniques",
    "section": "8.2 Getting Started",
    "text": "8.2 Getting Started\n\n8.2.1 The analytical question\nIn geobusiness and spatial policy, it is a common practice to delineate the market or planning area into homogeneous regions by using multivariate data. In this hands-on exercise, we are interested to delineate Shan State, Myanmar into homogeneous regions by using multiple Information and Communication technology (ICT) measures, namely: Radio, Television, Land line phone, Mobile phone, Computer, and Internet at home."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#the-data",
    "title": "Hands-On Exercise 8: Geographical Segmentation with Spatially Constrained Clustering Techniques",
    "section": "8.3 The data",
    "text": "8.3 The data\nTwo data sets will be used in this study. They are:\n\nMyanmar Township Boundary Data (i.e. myanmar_township_boundaries) : This is a GIS data in ESRI shapefile format. It consists of township boundary information of Myanmar. The spatial data are captured in polygon features.\nShan-ICT.csv: This is an extract of The 2014 Myanmar Population and Housing Census Myanmar at the township level.\n\nBoth data sets are download from Myanmar Information Management Unit (MIMU)\n\n8.3.1 Installing and loading R packages\nBefore we get started, it is important for us to install the necessary R packages into R and launch these R packages into R environment.\nThe R packages needed for this exercise are as follows:\n\nSpatial data handling\n\nsf, rgdal and spdep\n\nAttribute data handling\n\ntidyverse, especially readr, ggplot2 and dplyr\n\nChoropleth mapping\n\ntmap\n\nMultivariate data visualisation and analysis\n\ncoorplot, ggpubr, and heatmaply\n\nCluster analysis\n\ncluster\nClustGeo\n\n\nThe code chunks below installs and launches these R packages into R environment.\n\npacman::p_load(spdep, tmap, sf, ClustGeo, \n               ggpubr, cluster, factoextra, NbClust,\n               heatmaply, corrplot, psych, tidyverse, GGally)\n\nNote: With tidyverse, we do not have to install readr, ggplot2 and dplyr packages separately. In fact, tidyverse also installs other very useful R packages such as tidyr."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#data-import-and-prepatation",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#data-import-and-prepatation",
    "title": "Hands-On Exercise 8: Geographical Segmentation with Spatially Constrained Clustering Techniques",
    "section": "8.4 Data Import and Prepatation",
    "text": "8.4 Data Import and Prepatation\n\n8.4.1 Importing geospatial data into R environment\nIn this section, you will import Myanmar Township Boundary GIS data and its associated attrbiute table into R environment.\nThe Myanmar Township Boundary GIS data is in ESRI shapefile format. It will be imported into R environment by using the st_read() function of sf.\nThe code chunks used are shown below:\n\nshan_sf &lt;- st_read(dsn = \"data/geospatial\", \n                   layer = \"myanmar_township_boundaries\") %&gt;%\n  filter(ST %in% c(\"Shan (East)\", \"Shan (North)\", \"Shan (South)\")) %&gt;%\n  select(c(2:7))\n\nReading layer `myanmar_township_boundaries' from data source \n  `/Users/georgiaxng/georgiaxng/is415-handson/Hands-on_Ex/Hands-on_Ex08/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 330 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.17275 ymin: 9.671252 xmax: 101.1699 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\nThe imported township boundary object is called shan_sf. It is saved in simple feature data.frame format. We can view the content of the newly created shan_sf simple features data.frame by using the code chunk below.\n\nshan_sf\n\nSimple feature collection with 55 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 96.15107 ymin: 19.29932 xmax: 101.1699 ymax: 24.15907\nGeodetic CRS:  WGS 84\nFirst 10 features:\n             ST ST_PCODE       DT   DT_PCODE        TS  TS_PCODE\n1  Shan (North)   MMR015  Mongmit MMR015D008   Mongmit MMR015017\n2  Shan (South)   MMR014 Taunggyi MMR014D001   Pindaya MMR014006\n3  Shan (South)   MMR014 Taunggyi MMR014D001   Ywangan MMR014007\n4  Shan (South)   MMR014 Taunggyi MMR014D001  Pinlaung MMR014009\n5  Shan (North)   MMR015  Mongmit MMR015D008    Mabein MMR015018\n6  Shan (South)   MMR014 Taunggyi MMR014D001     Kalaw MMR014005\n7  Shan (South)   MMR014 Taunggyi MMR014D001     Pekon MMR014010\n8  Shan (South)   MMR014 Taunggyi MMR014D001  Lawksawk MMR014008\n9  Shan (North)   MMR015  Kyaukme MMR015D003 Nawnghkio MMR015013\n10 Shan (North)   MMR015  Kyaukme MMR015D003   Kyaukme MMR015012\n                         geometry\n1  MULTIPOLYGON (((96.96001 23...\n2  MULTIPOLYGON (((96.7731 21....\n3  MULTIPOLYGON (((96.78483 21...\n4  MULTIPOLYGON (((96.49518 20...\n5  MULTIPOLYGON (((96.66306 24...\n6  MULTIPOLYGON (((96.49518 20...\n7  MULTIPOLYGON (((97.14738 19...\n8  MULTIPOLYGON (((96.94981 22...\n9  MULTIPOLYGON (((96.75648 22...\n10 MULTIPOLYGON (((96.95498 22...\n\n\nNotice that sf.data.frame is conformed to Hardy Wickham’s tidy framework.\nSince shan_sf is conformed to tidy framework, we can also glimpse() to reveal the data type of it’s fields.\n\nglimpse(shan_sf)\n\nRows: 55\nColumns: 7\n$ ST       &lt;chr&gt; \"Shan (North)\", \"Shan (South)\", \"Shan (South)\", \"Shan (South)…\n$ ST_PCODE &lt;chr&gt; \"MMR015\", \"MMR014\", \"MMR014\", \"MMR014\", \"MMR015\", \"MMR014\", \"…\n$ DT       &lt;chr&gt; \"Mongmit\", \"Taunggyi\", \"Taunggyi\", \"Taunggyi\", \"Mongmit\", \"Ta…\n$ DT_PCODE &lt;chr&gt; \"MMR015D008\", \"MMR014D001\", \"MMR014D001\", \"MMR014D001\", \"MMR0…\n$ TS       &lt;chr&gt; \"Mongmit\", \"Pindaya\", \"Ywangan\", \"Pinlaung\", \"Mabein\", \"Kalaw…\n$ TS_PCODE &lt;chr&gt; \"MMR015017\", \"MMR014006\", \"MMR014007\", \"MMR014009\", \"MMR01501…\n$ geometry &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((96.96001 23..., MULTIPOLYGON (((…\n\n\n\n\n8.4.2 Importing aspatial data into R environment\nThe csv file will be import using read_csv function of readr package.\nThe code chunks used are shown below:\n\nict &lt;- read_csv (\"data/aspatial/Shan-ICT.csv\")\n\nThe imported InfoComm variables are extracted from The 2014 Myanmar Population and Housing Census Myanmar. The attribute data set is called ict. It is saved in R’s * tibble data.frame* format.\nThe code chunk below reveal the summary statistics of ict data.frame.\n\nsummary(ict)\n\n District Pcode     District Name      Township Pcode     Township Name     \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n Total households     Radio         Television    Land line phone \n Min.   : 3318    Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711    1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685    Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369    Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471    3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604    Max.   :30176   Max.   :62388   Max.   :6736.0  \n  Mobile phone      Computer      Internet at home\n Min.   :  150   Min.   :  20.0   Min.   :   8.0  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0  \n Median : 3559   Median : 244.0   Median : 316.0  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0  \n\n\nThere are a total of eleven fields and 55 observation in the tibble data.frame.\n\n\n8.4.3 Derive new variables using dplyr package\nThe unit of measurement of the values are number of household. Using these values directly will be bias by the underlying total number of households. In general, the townships with relatively higher total number of households will also have higher number of households owning radio, TV, etc.\nIn order to overcome this problem, we will derive the penetration rate of each ICT variable by using the code chunk below.\n\nict_derived &lt;- ict %&gt;%\n  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %&gt;%\n  mutate(`TV_PR` = `Television`/`Total households`*1000) %&gt;%\n  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %&gt;%\n  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %&gt;%\n  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %&gt;%\n  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %&gt;%\n  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,\n         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,\n         `TT_HOUSEHOLDS`=`Total households`,\n         `RADIO`=`Radio`, `TV`=`Television`, \n         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,\n         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`) \n\nLet us review the summary statistics of the newly derived penetration rates using the code chunk below.\n\nsummary(ict_derived)\n\n   DT_PCODE              DT              TS_PCODE              TS           \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n TT_HOUSEHOLDS       RADIO             TV           LLPHONE      \n Min.   : 3318   Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711   1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685   Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369   Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471   3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604   Max.   :30176   Max.   :62388   Max.   :6736.0  \n     MPHONE         COMPUTER         INTERNET         RADIO_PR     \n Min.   :  150   Min.   :  20.0   Min.   :   8.0   Min.   : 21.05  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0   1st Qu.:138.95  \n Median : 3559   Median : 244.0   Median : 316.0   Median :210.95  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2   Mean   :215.68  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5   3rd Qu.:268.07  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0   Max.   :484.52  \n     TV_PR         LLPHONE_PR       MPHONE_PR       COMPUTER_PR    \n Min.   :116.0   Min.   :  2.78   Min.   : 36.42   Min.   : 3.278  \n 1st Qu.:450.2   1st Qu.: 22.84   1st Qu.:190.14   1st Qu.:11.832  \n Median :517.2   Median : 37.59   Median :305.27   Median :18.970  \n Mean   :509.5   Mean   : 51.09   Mean   :314.05   Mean   :24.393  \n 3rd Qu.:606.4   3rd Qu.: 69.72   3rd Qu.:428.43   3rd Qu.:29.897  \n Max.   :842.5   Max.   :181.49   Max.   :735.43   Max.   :92.402  \n  INTERNET_PR     \n Min.   :  1.041  \n 1st Qu.:  8.617  \n Median : 22.829  \n Mean   : 30.644  \n 3rd Qu.: 41.281  \n Max.   :117.985  \n\n\nNotice that six new fields have been added into the data.frame. They are RADIO_PR, TV_PR, LLPHONE_PR, MPHONE_PR, COMPUTER_PR, and INTERNET_PR."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#exploratory-data-analysis-eda",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#exploratory-data-analysis-eda",
    "title": "Hands-On Exercise 8: Geographical Segmentation with Spatially Constrained Clustering Techniques",
    "section": "8.5 Exploratory Data Analysis (EDA)",
    "text": "8.5 Exploratory Data Analysis (EDA)\n\n8.5.1 EDA using statistical graphics\nWe can plot the distribution of the variables (i.e. Number of households with radio) by using appropriate Exploratory Data Analysis (EDA) as shown in the code chunk below.\nHistogram is useful to identify the overall distribution of the data values (i.e. left skew, right skew or normal distribution)\n\nggplot(data=ict_derived, \n       aes(x=`RADIO`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\n\n\n\n\n\n\n\nBoxplot is useful to detect if there are outliers.\n\nggplot(data=ict_derived, \n       aes(x=`RADIO`)) +\n  geom_boxplot(color=\"black\", \n               fill=\"light blue\")\n\n\n\n\n\n\n\n\nNext, we will also plotting the distribution of the newly derived variables (i.e. Radio penetration rate) by using the code chunk below.\n\nggplot(data=ict_derived, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\n\n\n\n\n\n\n\n\nggplot(data=ict_derived, \n       aes(x=`RADIO_PR`)) +\n  geom_boxplot(color=\"black\", \n               fill=\"light blue\")\n\n\n\n\n\n\n\n\nWhat can you observed from the distributions reveal in the histogram and boxplot.\nIn the figure below, multiple histograms are plotted to reveal the distribution of the selected variables in the ict_derived data.frame.\n\n\n\n\n\n\n\n\n\nThe code chunks below are used to create the data visualisation. They consist of two main parts. First, we will create the individual histograms using the code chunk below.\n\nradio &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ntv &lt;- ggplot(data=ict_derived, \n             aes(x= `TV_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\nllphone &lt;- ggplot(data=ict_derived, \n             aes(x= `LLPHONE_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\nmphone &lt;- ggplot(data=ict_derived, \n             aes(x= `MPHONE_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ncomputer &lt;- ggplot(data=ict_derived, \n             aes(x= `COMPUTER_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ninternet &lt;- ggplot(data=ict_derived, \n             aes(x= `INTERNET_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\nNext, the ggarrange() function of ggpubr package is used to group these histograms together.\n\nggarrange(radio, tv, llphone, mphone, computer, internet, \n          ncol = 3, \n          nrow = 2)\n\n\n\n8.5.2 EDA using choropleth map\n\n8.5.2.1 Joining geospatial data with aspatial data\nBefore we can prepare the choropleth map, we need to combine both the geospatial data object (i.e. shan_sf) and aspatial data.frame object (i.e. ict_derived) into one. This will be performed by using the left_join function of dplyr package. The shan_sf simple feature data.frame will be used as the base data object and the ict_derived data.frame will be used as the join table.\nThe code chunks below is used to perform the task. The unique identifier used to join both data objects is TS_PCODE.\n\nshan_sf &lt;- left_join(shan_sf, \n                     ict_derived, by=c(\"TS_PCODE\"=\"TS_PCODE\"))\n  \nwrite_rds(shan_sf, \"data/rds/shan_sf.rds\")\n\nThe message above shows that TS_CODE field is the common field used to perform the left-join.\nIt is important to note that there is no new output data been created. Instead, the data fields from ict_derived data frame are now updated into the data frame of shan_sf.\n\nshan_sf &lt;- read_rds(\"data/rds/shan_sf.rds\")\n\n\n\n8.5.2.2 Preparing a choropleth map\nTo have a quick look at the distribution of Radio penetration rate of Shan State at township level, a choropleth map will be prepared.\nThe code chunks below are used to prepare the choroplethby using the qtm() function of tmap package.\n\nqtm(shan_sf, \"RADIO_PR\")\n\n\n\n\n\n\n\n\nIn order to reveal the distribution shown in the choropleth map above are bias to the underlying total number of households at the townships, we will create two choropleth maps, one for the total number of households (i.e. TT_HOUSEHOLDS.map) and one for the total number of household with Radio (RADIO.map) by using the code chunk below.\n\nTT_HOUSEHOLDS.map &lt;- tm_shape(shan_sf) + \n  tm_fill(col = \"TT_HOUSEHOLDS\",\n          n = 5,\n          style = \"jenks\", \n          title = \"Total households\") + \n  tm_borders(alpha = 0.5) \n\nRADIO.map &lt;- tm_shape(shan_sf) + \n  tm_fill(col = \"RADIO\",\n          n = 5,\n          style = \"jenks\",\n          title = \"Number Radio \") + \n  tm_borders(alpha = 0.5) \n\ntmap_arrange(TT_HOUSEHOLDS.map, RADIO.map,\n             asp=NA, ncol=2)\n\n\n\n\n\n\n\n\nNotice that the choropleth maps above clearly show that townships with relatively larger number ot households are also showing relatively higher number of radio ownership.\nNow let us plot the choropleth maps showing the dsitribution of total number of households and Radio penetration rate by using the code chunk below.\n\ntm_shape(shan_sf) +\n    tm_polygons(c(\"TT_HOUSEHOLDS\", \"RADIO_PR\"),\n                style=\"jenks\") +\n    tm_facets(sync = TRUE, ncol = 2) +\n  tm_legend(legend.position = c(\"right\", \"bottom\"))+\n  tm_layout(outer.margins=0, asp=0)\n\n\n\n\n\n\n\n\nCan you identify the differences?"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#correlation-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#correlation-analysis",
    "title": "Hands-On Exercise 8: Geographical Segmentation with Spatially Constrained Clustering Techniques",
    "section": "8.6 Correlation Analysis",
    "text": "8.6 Correlation Analysis\nBefore we perform cluster analysis, it is important for us to ensure that the cluster variables are not highly correlated.\nIn this section, you will learn how to use corrplot.mixed() function of corrplot package to visualise and analyse the correlation of the input variables.\n\ncluster_vars.cor = cor(ict_derived[,12:17])\ncorrplot.mixed(cluster_vars.cor,\n         lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\nThe correlation plot above shows that COMPUTER_PR and INTERNET_PR are highly correlated. This suggest that only one of them should be used in the cluster analysis instead of both."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#hierarchy-cluster-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#hierarchy-cluster-analysis",
    "title": "Hands-On Exercise 8: Geographical Segmentation with Spatially Constrained Clustering Techniques",
    "section": "8.7 Hierarchy Cluster Analysis",
    "text": "8.7 Hierarchy Cluster Analysis\nIn this section, you will learn how to perform hierarchical cluster analysis. The analysis consists of four major steps:\n\n8.7.1 Extracting clustering variables\nThe code chunk below will be used to extract the clustering variables from the shan_sf simple feature object into data.frame.\n\ncluster_vars &lt;- shan_sf %&gt;%\n  st_set_geometry(NULL) %&gt;%\n  select(\"TS.x\", \"RADIO_PR\", \"TV_PR\", \"LLPHONE_PR\", \"MPHONE_PR\", \"COMPUTER_PR\")\nhead(cluster_vars,10)\n\n        TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\n1    Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\n2    Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\n3    Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\n4   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\n5     Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\n6      Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\n7      Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\n8   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\n9  Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\n10   Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nNotice that the final clustering variables list does not include variable INTERNET_PR because it is highly correlated with variable COMPUTER_PR.\nNext, we need to change the rows by township name instead of row number by using the code chunk below\n\nrow.names(cluster_vars) &lt;- cluster_vars$\"TS.x\"\nhead(cluster_vars,10)\n\n               TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit     Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya     Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan     Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\nMabein       Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw         Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\nPekon         Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme     Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nNotice that the row number has been replaced into the township name.\nNow, we will delete the TS.x field by using the code chunk below.\n\nshan_ict &lt;- select(cluster_vars, c(2:6))\nhead(shan_ict, 10)\n\n          RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit   286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya   417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan   484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung  231.6499 541.7189   28.54454  249.4903    13.76255\nMabein    449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw     280.7624 611.6204   42.06478  408.7951    29.63160\nPekon     318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk  387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme   210.9548 601.1773   39.58267  372.4930    30.94709\n\n\n\nshan_ict &lt;- read_rds(\"data/rds/shan_ict.rds\")\n\n\n\n8.7.2 Data Standardisation\nIn general, multiple variables will be used in cluster analysis. It is not unusual their values range are different. In order to avoid the cluster analysis result is baised to clustering variables with large values, it is useful to standardise the input variables before performing cluster analysis.\n\n\n8.7.3 Min-Max standardisation\nIn the code chunk below, normalize() of heatmaply package is used to stadardisation the clustering variables by using Min-Max method. The summary() is then used to display the summary statistics of the standardised clustering variables.\n\nshan_ict.std &lt;- normalize(shan_ict)\nsummary(shan_ict.std)\n\n    RADIO_PR          TV_PR          LLPHONE_PR       MPHONE_PR     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.2544   1st Qu.:0.4600   1st Qu.:0.1123   1st Qu.:0.2199  \n Median :0.4097   Median :0.5523   Median :0.1948   Median :0.3846  \n Mean   :0.4199   Mean   :0.5416   Mean   :0.2703   Mean   :0.3972  \n 3rd Qu.:0.5330   3rd Qu.:0.6750   3rd Qu.:0.3746   3rd Qu.:0.5608  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n  COMPUTER_PR     \n Min.   :0.00000  \n 1st Qu.:0.09598  \n Median :0.17607  \n Mean   :0.23692  \n 3rd Qu.:0.29868  \n Max.   :1.00000  \n\n\nNotice that the values range of the Min-max standardised clustering variables are 0-1 now.\n\n\n8.7.4 Z-score standardisation\nZ-score standardisation can be performed easily by using scale() of Base R. The code chunk below will be used to stadardisation the clustering variables by using Z-score method.\n\nshan_ict.z &lt;- scale(shan_ict)\ndescribe(shan_ict.z)\n\n            vars  n mean sd median trimmed  mad   min  max range  skew kurtosis\nRADIO_PR       1 55    0  1  -0.04   -0.06 0.94 -1.85 2.55  4.40  0.48    -0.27\nTV_PR          2 55    0  1   0.05    0.04 0.78 -2.47 2.09  4.56 -0.38    -0.23\nLLPHONE_PR     3 55    0  1  -0.33   -0.15 0.68 -1.19 3.20  4.39  1.37     1.49\nMPHONE_PR      4 55    0  1  -0.05   -0.06 1.01 -1.58 2.40  3.98  0.48    -0.34\nCOMPUTER_PR    5 55    0  1  -0.26   -0.18 0.64 -1.03 3.31  4.34  1.80     2.96\n              se\nRADIO_PR    0.13\nTV_PR       0.13\nLLPHONE_PR  0.13\nMPHONE_PR   0.13\nCOMPUTER_PR 0.13\n\n\nNotice the mean and standard deviation of the Z-score standardised clustering variables are 0 and 1 respectively.\nNote: describe() of psych package is used here instead of summary() of Base R because the earlier provides standard deviation.\nWarning: Z-score standardisation method should only be used if we would assume all variables come from some normal distribution.\n\n\n8.7.5 Visualising the standardised clustering variables\nBeside reviewing the summary statistics of the standardised clustering variables, it is also a good practice to visualise their distribution graphical.\nThe code chunk below plot the scaled Radio_PR field.\n\nr &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  ggtitle(\"Raw values without standardisation\")\n\nshan_ict_s_df &lt;- as.data.frame(shan_ict.std)\ns &lt;- ggplot(data=shan_ict_s_df, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  ggtitle(\"Min-Max Standardisation\")\n\nshan_ict_z_df &lt;- as.data.frame(shan_ict.z)\nz &lt;- ggplot(data=shan_ict_z_df, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  ggtitle(\"Z-score Standardisation\")\n\nggarrange(r, s, z,\n          ncol = 3,\n          nrow = 1)\n\n\n\n\n\n\n\n\n\nWhat statistical conclusion can you draw from the histograms above?\n\n\nr &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"light blue\") +\n  ggtitle(\"Raw values without standardisation\")\n\nshan_ict_s_df &lt;- as.data.frame(shan_ict.std)\ns &lt;- ggplot(data=shan_ict_s_df, \n       aes(x=`RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"light blue\") +\n  ggtitle(\"Min-Max Standardisation\")\n\nshan_ict_z_df &lt;- as.data.frame(shan_ict.z)\nz &lt;- ggplot(data=shan_ict_z_df, \n       aes(x=`RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"light blue\") +\n  ggtitle(\"Z-score Standardisation\")\n\nggarrange(r, s, z,\n          ncol = 3,\n          nrow = 1)\n\n\n\n\n\n\n\n\n\n\n8.7.6 Computing proximity matrix\nIn R, many packages provide functions to calculate distance matrix. We will compute the proximity matrix by using dist() of R.\ndist() supports six distance proximity calculations, they are: euclidean, maximum, manhattan, canberra, binary and minkowski. The default is euclidean proximity matrix.\nThe code chunk below is used to compute the proximity matrix using euclidean method.\n\nproxmat &lt;- dist(shan_ict, method = 'euclidean')\n\nThe code chunk below can then be used to list the content of proxmat for visual inspection.\n\nproxmat\n\n\n\n8.7.7 Computing hierarchical clustering\nIn R, there are several packages provide hierarchical clustering function. In this hands-on exercise, hclust() of R stats will be used.\nhclust() employed agglomeration method to compute the cluster. Eight clustering algorithms are supported, they are: ward.D, ward.D2, single, complete, average(UPGMA), mcquitty(WPGMA), median(WPGMC) and centroid(UPGMC).\nThe code chunk below performs hierarchical cluster analysis using ward.D method. The hierarchical clustering output is stored in an object of class hclust which describes the tree produced by the clustering process.\n\nhclust_ward &lt;- hclust(proxmat, method = 'ward.D')\n\nWe can then plot the tree by using plot() of R Graphics as shown in the code chunk below.\n\nplot(hclust_ward, cex = 0.6)\n\n\n\n\n\n\n\n\n\n\n8.7.8 Selecting the optimal clustering algorithm\nOne of the challenge in performing hierarchical clustering is to identify stronger clustering structures. The issue can be solved by using use agnes() function of cluster package. It functions like hclus(), however, with the agnes() function you can also get the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).\nThe code chunk below will be used to compute the agglomerative coefficients of all hierarchical clustering algorithms.\n\nm &lt;- c( \"average\", \"single\", \"complete\", \"ward\")\nnames(m) &lt;- c( \"average\", \"single\", \"complete\", \"ward\")\n\nac &lt;- function(x) {\n  agnes(shan_ict, method = x)$ac\n}\n\nmap_dbl(m, ac)\n\n  average    single  complete      ward \n0.8131144 0.6628705 0.8950702 0.9427730 \n\n\nWith reference to the output above, we can see that Ward’s method provides the strongest clustering structure among the four methods assessed. Hence, in the subsequent analysis, only Ward’s method will be used.\n\n\n8.7.9 Determining Optimal Clusters\nAnother technical challenge face by data analyst in performing clustering analysis is to determine the optimal clusters to retain.\nThere are three commonly used methods to determine the optimal clusters, they are:\n\nElbow Method\nAverage Silhouette Method\nGap Statistic Method\n\n\n8.7.9.1 Gap Statistic Method\nThe gap statistic compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic (i.e., that yields the largest gap statistic). This means that the clustering structure is far away from the random uniform distribution of points.\nTo compute the gap statistic, clusGap() of cluster package will be used.\n\nset.seed(12345)\ngap_stat &lt;- clusGap(shan_ict, \n                    FUN = hcut, \n                    nstart = 25, \n                    K.max = 10, \n                    B = 50)\n# Print the result\nprint(gap_stat, method = \"firstmax\")\n\nClustering Gap statistic [\"clusGap\"] from call:\nclusGap(x = shan_ict, FUNcluster = hcut, K.max = 10, B = 50, nstart = 25)\nB=50 simulated reference sets, k = 1..10; spaceH0=\"scaledPCA\"\n --&gt; Number of clusters (method 'firstmax'): 1\n          logW   E.logW       gap     SE.sim\n [1,] 8.407129 8.680794 0.2736651 0.04460994\n [2,] 8.130029 8.350712 0.2206824 0.03880130\n [3,] 7.992265 8.202550 0.2102844 0.03362652\n [4,] 7.862224 8.080655 0.2184311 0.03784781\n [5,] 7.756461 7.978022 0.2215615 0.03897071\n [6,] 7.665594 7.887777 0.2221833 0.03973087\n [7,] 7.590919 7.806333 0.2154145 0.04054939\n [8,] 7.526680 7.731619 0.2049390 0.04198644\n [9,] 7.458024 7.660795 0.2027705 0.04421874\n[10,] 7.377412 7.593858 0.2164465 0.04540947\n\n\nAlso note that the hcut function used is from factoextra package.\nNext, we can visualise the plot by using fviz_gap_stat() of factoextra package.\n\nfviz_gap_stat(gap_stat)\n\n\n\n\n\n\n\n\nWith reference to the gap statistic graph above, the recommended number of cluster to retain is 1. However, it is not logical to retain only one cluster. By examine the gap statistic graph, the 6-cluster gives the largest gap statistic and should be the next best cluster to pick.\nNote: In addition to these commonly used approaches, the NbClust package, published by Charrad et al., 2014, provides 30 indices for determining the relevant number of clusters and proposes to users the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods.\n\n\n\n8.7.10 Interpreting the dendrograms\nIn the dendrogram displayed above, each leaf corresponds to one observation. As we move up the tree, observations that are similar to each other are combined into branches, which are themselves fused at a higher height.\nThe height of the fusion, provided on the vertical axis, indicates the (dis)similarity between two observations. The higher the height of the fusion, the less similar the observations are. Note that, conclusions about the proximity of two observations can be drawn only based on the height where branches containing those two observations first are fused. We cannot use the proximity of two observations along the horizontal axis as a criteria of their similarity.\nIt’s also possible to draw the dendrogram with a border around the selected clusters by using rect.hclust() of R stats. The argument border is used to specify the border colors for the rectangles.\n\nplot(hclust_ward, cex = 0.6)\nrect.hclust(hclust_ward, \n            k = 6, \n            border = 2:5)\n\n\n\n\n\n\n\n\n\n\n8.7.11 Visually-driven hierarchical clustering analysis\nIn this section, we will learn how to perform visually-driven hiearchical clustering analysis by using heatmaply package.\nWith heatmaply, we are able to build both highly interactive cluster heatmap or static cluster heatmap.\n\n8.7.11.1 Transforming the data frame into a matrix\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform shan_ict data frame into a data matrix.\n\nshan_ict_mat &lt;- data.matrix(shan_ict)\n\n\n\n8.7.11.2 Plotting interactive cluster heatmap using heatmaply()\nIn the code chunk below, the heatmaply() of heatmaply package is used to build an interactive cluster heatmap.\n\nheatmaply(normalize(shan_ict_mat),\n          Colv=NA,\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\",\n          seriate = \"OLO\",\n          colors = Blues,\n          k_row = 6,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"Geographic Segmentation of Shan State by ICT indicators\",\n          xlab = \"ICT Indicators\",\n          ylab = \"Townships of Shan State\"\n          )\n\n\n\n\n\n\n\n\n8.7.12 Mapping the clusters formed\nWith closed examination of the dendragram above, we have decided to retain six clusters.\ncutree() of R Base will be used in the code chunk below to derive a 6-cluster model.\n\ngroups &lt;- as.factor(cutree(hclust_ward, k=6))\n\nThe output is called groups. It is a list object.\nIn order to visualise the clusters, the groups object need to be appended onto shan_sf simple feature object.\nThe code chunk below form the join in three steps:\n\nthe groups list object will be converted into a matrix;\ncbind() is used to append groups matrix onto shan_sf to produce an output simple feature object called shan_sf_cluster; and\nrename of dplyr package is used to rename as.matrix.groups field as CLUSTER.\n\n\nshan_sf_cluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER`=`as.matrix.groups.`)\n\nNext, qtm() of tmap package is used to plot the choropleth map showing the cluster formed.\n\nqtm(shan_sf_cluster, \"CLUSTER\")\n\n\n\n\n\n\n\n\nThe choropleth map above reveals the clusters are very fragmented. The is one of the major limitation when non-spatial clustering algorithm such as hierarchical cluster analysis method is used."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "title": "Hands On Exercise 5: Spatial Weights and Applications",
    "section": "",
    "text": "In this hands-on exercise, I will be computing spatial weights using R. By the end to this hands-on exercise, we would have been able to:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute spatial weights using appropriate functions of spdep package, and\ncalculate spatially lagged variables using appropriate functions of spdep package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#overview",
    "title": "Hands On Exercise 5: Spatial Weights and Applications",
    "section": "",
    "text": "In this hands-on exercise, I will be computing spatial weights using R. By the end to this hands-on exercise, we would have been able to:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute spatial weights using appropriate functions of spdep package, and\ncalculate spatially lagged variables using appropriate functions of spdep package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#the-study-area-and-data",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#the-study-area-and-data",
    "title": "Hands On Exercise 5: Spatial Weights and Applications",
    "section": "8.2 The Study Area and Data",
    "text": "8.2 The Study Area and Data\nTwo data sets will be used in this hands-on exercise, they are:\n\nHunan county boundary layer. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n8.2.1 Getting Started\nBefore we get started, we need to ensure that spdep, sf, tmap and tidyverse packages of R are currently installed in your R.\n\npacman::p_load(sf, spdep, tmap, tidyverse, knitr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#getting-the-data-into-r-environment",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#getting-the-data-into-r-environment",
    "title": "Hands On Exercise 5: Spatial Weights and Applications",
    "section": "8.3 Getting the Data Into R Environment",
    "text": "8.3 Getting the Data Into R Environment\nIn this section, we will be bringing a geospatial data and its associated attribute table into R environment. The geospatial data is in ESRI shapefile format and the attribute table is in csv fomat.\n\n8.3.1 Import shapefile into r environment\nThe code chunk below uses st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/georgiaxng/georgiaxng/is415-handson/Hands-on_Ex/Hands-on_Ex05/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n8.3.2 Import csv file into r environment\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package. The output is R dataframe class.\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n8.3.3 Performing relational join\nHere is the list of columns for hunan.\n\ncolnames(hunan)\n\n[1] \"NAME_2\"     \"ID_3\"       \"NAME_3\"     \"ENGTYPE_3\"  \"Shape_Leng\"\n[6] \"Shape_Area\" \"County\"     \"geometry\"  \n\n\nHere is the list of columns for hunan2012.\n\ncolnames(hunan2012)\n\n [1] \"County\"      \"City\"        \"avg_wage\"    \"deposite\"    \"FAI\"        \n [6] \"Gov_Rev\"     \"Gov_Exp\"     \"GDP\"         \"GDPPC\"       \"GIO\"        \n[11] \"Loan\"        \"NIPCR\"       \"Bed\"         \"Emp\"         \"EmpR\"       \n[16] \"EmpRT\"       \"Pri_Stu\"     \"Sec_Stu\"     \"Household\"   \"Household_R\"\n[21] \"NOIP\"        \"Pop_R\"       \"RSCG\"        \"Pop_T\"       \"Agri\"       \n[26] \"Service\"     \"Disp_Inc\"    \"RORP\"        \"ROREmp\"     \n\n\nThis part of the code performs a left join between two data frames: hunan and hunan2012. In a left join:\n\nThe result will include all rows from hunan (the left data frame).\nIt will also include the columns from hunan2012 (the right data frame), but only for the rows that match on the common key columns (usually one or more columns with the same name in both data frames).\nIf there is no match for a row in hunan2012, the corresponding columns from hunan2012 will have NA values.\n\n\ntest&lt;-left_join(hunan,hunan2012)\n\nAs such, we will get these columns.\n\ncolnames(test)\n\n [1] \"NAME_2\"      \"ID_3\"        \"NAME_3\"      \"ENGTYPE_3\"   \"Shape_Leng\" \n [6] \"Shape_Area\"  \"County\"      \"City\"        \"avg_wage\"    \"deposite\"   \n[11] \"FAI\"         \"Gov_Rev\"     \"Gov_Exp\"     \"GDP\"         \"GDPPC\"      \n[16] \"GIO\"         \"Loan\"        \"NIPCR\"       \"Bed\"         \"Emp\"        \n[21] \"EmpR\"        \"EmpRT\"       \"Pri_Stu\"     \"Sec_Stu\"     \"Household\"  \n[26] \"Household_R\" \"NOIP\"        \"Pop_R\"       \"RSCG\"        \"Pop_T\"      \n[31] \"Agri\"        \"Service\"     \"Disp_Inc\"    \"RORP\"        \"ROREmp\"     \n[36] \"geometry\"   \n\n\nHowever, we only want selected columns for our purpose, thus we will left join the two datasets and use select() to filter the desired columns out.\n\nhunan &lt;- left_join(hunan,hunan2012)%&gt;%\n  select(1:4, 7, 15)\n\n\ncolnames(hunan)\n\n[1] \"NAME_2\"    \"ID_3\"      \"NAME_3\"    \"ENGTYPE_3\" \"County\"    \"GDPPC\"    \n[7] \"geometry\""
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#visualising-regional-development-indicator",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#visualising-regional-development-indicator",
    "title": "Hands On Exercise 5: Spatial Weights and Applications",
    "section": "8.4 Visualising Regional Development Indicator",
    "text": "8.4 Visualising Regional Development Indicator\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\nbasemap &lt;- tm_shape(hunan) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size=0.5)\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\ntmap_arrange(basemap, gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#computing-contiguity-spatial-weights",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#computing-contiguity-spatial-weights",
    "title": "Hands On Exercise 5: Spatial Weights and Applications",
    "section": "8.5 Computing Contiguity Spatial Weights",
    "text": "8.5 Computing Contiguity Spatial Weights\nIn this section, I’ll show you how to use the poly2nb() function from the spdep package to calculate contiguity weight matrices for our study area. This function creates a list of neighboring regions based on shared boundaries. When I looked at the documentation, I noticed that I can pass a queen argument, which can be set to TRUE or FALSE. If I don’t specify it, the default is TRUE, meaning the function will return a list of first-order neighbors using the Queen contiguity criterion unless I explicitly set queen = FALSE.\n\n8.5.1 Computing (QUEEN) contiguity based neighbours\nThe code chunk below is used to compute Queen contiguity weight matrix.\n\nwm_q &lt;- poly2nb(hunan, queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one heighbours.\nFor each polygon in our polygon object, wm_q lists all neighboring polygons. For example, to see the neighbors for the first polygon in the object, type:\n\nwm_q[[1]]\n\n[1]  2  3  4 57 85\n\n\nPolygon 1 has 5 neighbors. The numbers represent the polygon IDs as stored in hunan SpatialPolygonsDataFrame class.\nWe can retrieve the county name of Polygon ID=1 by using the code chunk below:\n\nhunan$County[1]\n\n[1] \"Anxiang\"\n\n\nThe output reveals that Polygon ID=1 is Anxiang county.\nTo reveal the county names of the five neighboring polygons, the code chunk will be used:\n\nhunan$NAME_3[c(2,3,4,57,85)]\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\nWe can retrieve the GDPPC of these five countries by using the code chunk below.\n\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n[1] 20981 34592 24473 21311 22879\n\n\nThe printed output above shows that the GDPPC of the five nearest neighbours based on Queen’s method are 20981, 34592, 24473, 21311 and 22879 respectively.\nWe can display the complete weight matrix by using str().\n\nstr(wm_q)\n\nList of 88\n $ : int [1:5] 2 3 4 57 85\n $ : int [1:5] 1 57 58 78 85\n $ : int [1:4] 1 4 5 85\n $ : int [1:4] 1 3 5 6\n $ : int [1:4] 3 4 6 85\n $ : int [1:5] 4 5 69 75 85\n $ : int [1:4] 67 71 74 84\n $ : int [1:7] 9 46 47 56 78 80 86\n $ : int [1:6] 8 66 68 78 84 86\n $ : int [1:8] 16 17 19 20 22 70 72 73\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:3] 11 15 17\n $ : int [1:4] 13 14 17 83\n $ : int [1:5] 10 17 22 72 83\n $ : int [1:7] 10 11 14 15 16 72 83\n $ : int [1:5] 20 22 23 77 83\n $ : int [1:6] 10 20 21 73 74 86\n $ : int [1:7] 10 18 19 21 22 23 82\n $ : int [1:5] 19 20 35 82 86\n $ : int [1:5] 10 16 18 20 83\n $ : int [1:7] 18 20 38 41 77 79 82\n $ : int [1:5] 25 28 31 32 54\n $ : int [1:5] 24 28 31 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:3] 26 29 42\n $ : int [1:5] 24 25 33 49 54\n $ : int [1:3] 27 37 42\n $ : int 33\n $ : int [1:8] 24 25 32 36 39 40 56 81\n $ : int [1:8] 24 31 50 54 55 56 75 85\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 47 80 82 86\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:4] 29 42 43 44\n $ : int [1:4] 23 44 77 79\n $ : int [1:5] 31 40 42 43 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:7] 26 27 29 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:4] 37 38 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:3] 8 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:5] 32 48 52 54 55\n $ : int [1:3] 48 49 52\n $ : int [1:5] 48 49 50 51 54\n $ : int [1:3] 48 55 75\n $ : int [1:6] 24 28 32 49 50 52\n $ : int [1:5] 32 48 50 53 75\n $ : int [1:7] 8 31 32 36 78 80 85\n $ : int [1:6] 1 2 58 64 76 85\n $ : int [1:5] 2 57 68 76 78\n $ : int [1:4] 60 61 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:7] 12 59 60 62 63 77 87\n $ : int [1:3] 61 77 87\n $ : int [1:4] 12 61 77 83\n $ : int [1:2] 57 76\n $ : int 76\n $ : int [1:5] 9 67 68 76 84\n $ : int [1:4] 7 66 76 84\n $ : int [1:5] 9 58 66 76 78\n $ : int [1:3] 6 75 85\n $ : int [1:3] 10 72 73\n $ : int [1:3] 7 73 74\n $ : int [1:5] 10 11 16 17 70\n $ : int [1:5] 10 19 70 71 74\n $ : int [1:6] 7 19 71 73 84 86\n $ : int [1:6] 6 32 53 55 69 85\n $ : int [1:7] 57 58 64 65 66 67 68\n $ : int [1:7] 18 23 38 61 62 63 83\n $ : int [1:7] 2 8 9 56 58 68 85\n $ : int [1:7] 23 38 40 41 43 44 45\n $ : int [1:8] 8 34 35 36 41 45 47 56\n $ : int [1:6] 25 26 31 33 39 42\n $ : int [1:5] 20 21 23 35 41\n $ : int [1:9] 12 13 15 16 17 18 22 63 77\n $ : int [1:6] 7 9 66 67 74 86\n $ : int [1:11] 1 2 3 5 6 32 56 57 69 75 ...\n $ : int [1:9] 8 9 19 21 35 46 47 74 84\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language poly2nb(pl = hunan, queen = TRUE)\n - attr(*, \"type\")= chr \"queen\"\n - attr(*, \"sym\")= logi TRUE\n\n\n\n\n8.5.2 Creating (ROOK) contiguity based neighbours\nThe code chunk below is used to compute Rook contiguity weight matrix.\n\nwm_r &lt;- poly2nb(hunan, queen=FALSE)\nsummary(wm_r)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connect area unit has 10 neighbours. There are two area units with only one heighbours.\n\n\n8.5.3 Visualising contiguity weights\nA connectivity graph takes a point and displays a line to each neighboring point. We are working with polygons at the moment, so we will need to get points in order to make our connectivity graphs. The most typically method for this will be polygon centroids. We will calculate these in the sf package before moving onto the graphs. Getting Latitude and Longitude of Polygon Centroids\nWe will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid on the sf object: us.bound. We need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of us.bound. Our function will be st_centroid. We will be using map_dbl variation of map from the purrr package. For more documentation, check out map documentation\nTo get our longitude values we map the st_centroid function over the geometry column of us.bound and access the longitude value through double bracket notation [[]] and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\ncoords &lt;- cbind(longitude, latitude)\n\nWe check the first few observations to see if things are formatted correctly.\n\nhead(coords)\n\n     longitude latitude\n[1,]  112.1531 29.44362\n[2,]  112.0372 28.86489\n[3,]  111.8917 29.47107\n[4,]  111.7031 29.74499\n[5,]  111.6138 29.49258\n[6,]  111.0341 29.79863\n\n\n\n8.5.3.1 Plotting Queen contiguity based neighbours map\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\")\n\n\n\n\n\n\n\n\n\n\n8.5.3.2 Plotting Rook contiguity based neighbours map\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n8.5.3.3 Plotting both Queen and Rook contiguity based neighbours maps\n\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"Queen Contiguity\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\")\nplot(hunan$geometry, border=\"lightgrey\", main=\"Rook Contiguity\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#computing-distance-based-neighbours",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#computing-distance-based-neighbours",
    "title": "Hands On Exercise 5: Spatial Weights and Applications",
    "section": "8.6 Computing distance based neighbours",
    "text": "8.6 Computing distance based neighbours\nIn this section, I’ll guide you through how to derive distance-based weight matrices using the dnearneigh() function from the spdep package. This function identifies neighboring regions based on Euclidean distance, with a distance band defined by lower (d1) and upper (d2) bounds, which I can control using the bounds= argument. If I’m working with unprojected coordinates—either specified in the coordinates object x or with x as a two-column matrix—and I set longlat=TRUE, the function will calculate great circle distances in kilometers using the WGS84 reference ellipsoid.\n\n8.6.1 Determine the cut-off distance\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n#coords &lt;- coordinates(hunan)\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n8.6.2 Computing fixed distance weight matrix\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nThe number 3.681818 refers to the average number of neighboring regions (or “links”) that each of the 88 regions has within the specified distance range (from 0 to 62 km in this case, since longlat = TRUE indicates that great circle distances are being used).\nIn simpler terms, on average, each region is connected to approximately 3.68 other regions within the 62 km distance band. This value is calculated by dividing the total number of links (324) by the total number of regions (88).\nNext, we will use str() to display the content of wm_d62 weight matrix.\n\nstr(wm_d62)\n\nList of 88\n $ : int [1:5] 3 4 5 57 64\n $ : int [1:4] 57 58 78 85\n $ : int [1:4] 1 4 5 57\n $ : int [1:3] 1 3 5\n $ : int [1:4] 1 3 4 85\n $ : int 69\n $ : int [1:2] 67 84\n $ : int [1:4] 9 46 47 78\n $ : int [1:4] 8 46 68 84\n $ : int [1:4] 16 22 70 72\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:2] 11 17\n $ : int 13\n $ : int [1:4] 10 17 22 83\n $ : int [1:3] 11 14 16\n $ : int [1:3] 20 22 63\n $ : int [1:5] 20 21 73 74 82\n $ : int [1:5] 18 19 21 22 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:4] 10 16 18 20\n $ : int [1:3] 41 77 82\n $ : int [1:4] 25 28 31 54\n $ : int [1:4] 24 28 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:2] 26 29\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:2] 27 37\n $ : int 33\n $ : int [1:2] 24 36\n $ : int 50\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:5] 31 34 45 56 80\n $ : int [1:2] 29 42\n $ : int [1:3] 44 77 79\n $ : int [1:4] 40 42 43 81\n $ : int [1:3] 39 45 79\n $ : int [1:5] 23 35 45 79 82\n $ : int [1:5] 26 37 39 43 81\n $ : int [1:3] 39 42 44\n $ : int [1:2] 38 43\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:5] 8 9 35 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:4] 48 49 50 52\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:2] 48 55\n $ : int [1:5] 24 28 49 50 52\n $ : int [1:4] 48 50 53 75\n $ : int 36\n $ : int [1:5] 1 2 3 58 64\n $ : int [1:5] 2 57 64 66 68\n $ : int [1:3] 60 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:5] 12 60 62 63 87\n $ : int [1:4] 61 63 77 87\n $ : int [1:5] 12 18 61 62 83\n $ : int [1:4] 1 57 58 76\n $ : int 76\n $ : int [1:5] 58 67 68 76 84\n $ : int [1:2] 7 66\n $ : int [1:4] 9 58 66 84\n $ : int [1:2] 6 75\n $ : int [1:3] 10 72 73\n $ : int [1:2] 73 74\n $ : int [1:3] 10 11 70\n $ : int [1:4] 19 70 71 74\n $ : int [1:5] 19 21 71 73 86\n $ : int [1:2] 55 69\n $ : int [1:3] 64 65 66\n $ : int [1:3] 23 38 62\n $ : int [1:2] 2 8\n $ : int [1:4] 38 40 41 45\n $ : int [1:5] 34 35 36 45 47\n $ : int [1:5] 25 26 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:4] 12 13 16 63\n $ : int [1:4] 7 9 66 68\n $ : int [1:2] 2 5\n $ : int [1:4] 21 46 47 74\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language dnearneigh(x = coords, d1 = 0, d2 = 62, longlat = TRUE)\n - attr(*, \"dnn\")= num [1:2] 0 62\n - attr(*, \"bounds\")= chr [1:2] \"GE\" \"LE\"\n - attr(*, \"nbtype\")= chr \"distance\"\n - attr(*, \"sym\")= logi TRUE\n\n\nAnother way to display the structure of the weight matrix is to combine table() and card() of spdep.\n\ntable(hunan$County, card(wm_d62))\n\n               \n                1 2 3 4 5 6\n  Anhua         1 0 0 0 0 0\n  Anren         0 0 0 1 0 0\n  Anxiang       0 0 0 0 1 0\n  Baojing       0 0 0 0 1 0\n  Chaling       0 0 1 0 0 0\n  Changning     0 0 1 0 0 0\n  Changsha      0 0 0 1 0 0\n  Chengbu       0 1 0 0 0 0\n  Chenxi        0 0 0 1 0 0\n  Cili          0 1 0 0 0 0\n  Dao           0 0 0 1 0 0\n  Dongan        0 0 1 0 0 0\n  Dongkou       0 0 0 1 0 0\n  Fenghuang     0 0 0 1 0 0\n  Guidong       0 0 1 0 0 0\n  Guiyang       0 0 0 1 0 0\n  Guzhang       0 0 0 0 0 1\n  Hanshou       0 0 0 1 0 0\n  Hengdong      0 0 0 0 1 0\n  Hengnan       0 0 0 0 1 0\n  Hengshan      0 0 0 0 0 1\n  Hengyang      0 0 0 0 0 1\n  Hongjiang     0 0 0 0 1 0\n  Huarong       0 0 0 1 0 0\n  Huayuan       0 0 0 1 0 0\n  Huitong       0 0 0 1 0 0\n  Jiahe         0 0 0 0 1 0\n  Jianghua      0 0 1 0 0 0\n  Jiangyong     0 1 0 0 0 0\n  Jingzhou      0 1 0 0 0 0\n  Jinshi        0 0 0 1 0 0\n  Jishou        0 0 0 0 0 1\n  Lanshan       0 0 0 1 0 0\n  Leiyang       0 0 0 1 0 0\n  Lengshuijiang 0 0 1 0 0 0\n  Li            0 0 1 0 0 0\n  Lianyuan      0 0 0 0 1 0\n  Liling        0 1 0 0 0 0\n  Linli         0 0 0 1 0 0\n  Linwu         0 0 0 1 0 0\n  Linxiang      1 0 0 0 0 0\n  Liuyang       0 1 0 0 0 0\n  Longhui       0 0 1 0 0 0\n  Longshan      0 1 0 0 0 0\n  Luxi          0 0 0 0 1 0\n  Mayang        0 0 0 0 0 1\n  Miluo         0 0 0 0 1 0\n  Nan           0 0 0 0 1 0\n  Ningxiang     0 0 0 1 0 0\n  Ningyuan      0 0 0 0 1 0\n  Pingjiang     0 1 0 0 0 0\n  Qidong        0 0 1 0 0 0\n  Qiyang        0 0 1 0 0 0\n  Rucheng       0 1 0 0 0 0\n  Sangzhi       0 1 0 0 0 0\n  Shaodong      0 0 0 0 1 0\n  Shaoshan      0 0 0 0 1 0\n  Shaoyang      0 0 0 1 0 0\n  Shimen        1 0 0 0 0 0\n  Shuangfeng    0 0 0 0 0 1\n  Shuangpai     0 0 0 1 0 0\n  Suining       0 0 0 0 1 0\n  Taojiang      0 1 0 0 0 0\n  Taoyuan       0 1 0 0 0 0\n  Tongdao       0 1 0 0 0 0\n  Wangcheng     0 0 0 1 0 0\n  Wugang        0 0 1 0 0 0\n  Xiangtan      0 0 0 1 0 0\n  Xiangxiang    0 0 0 0 1 0\n  Xiangyin      0 0 0 1 0 0\n  Xinhua        0 0 0 0 1 0\n  Xinhuang      1 0 0 0 0 0\n  Xinning       0 1 0 0 0 0\n  Xinshao       0 0 0 0 0 1\n  Xintian       0 0 0 0 1 0\n  Xupu          0 1 0 0 0 0\n  Yanling       0 0 1 0 0 0\n  Yizhang       1 0 0 0 0 0\n  Yongshun      0 0 0 1 0 0\n  Yongxing      0 0 0 1 0 0\n  You           0 0 0 1 0 0\n  Yuanjiang     0 0 0 0 1 0\n  Yuanling      1 0 0 0 0 0\n  Yueyang       0 0 1 0 0 0\n  Zhijiang      0 0 0 0 1 0\n  Zhongfang     0 0 0 1 0 0\n  Zhuzhou       0 0 0 0 1 0\n  Zixing        0 0 1 0 0 0\n\n\n\nn_comp &lt;- n.comp.nb(wm_d62)\nn_comp$nc\n\n[1] 1\n\n\n\ntable(n_comp$comp.id)\n\n\n 1 \n88 \n\n\n\n8.6.2.1 Plotting fixed distance weight matrix\nNext, we will plot the distance weight matrix by using the code chunk below.\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_d62, coords, add=TRUE)\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\n\n\n\n\n\n\n\n\nThe red lines show the links of 1st nearest neighbours and the black lines show the links of neighbours within the cut-off distance of 62km.\nAlternatively, we can plot both of them next to each other by using the code chunk below.\n\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"1st nearest neighbours\")\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\nplot(hunan$geometry, border=\"lightgrey\", main=\"Distance link\")\nplot(wm_d62, coords, add=TRUE, pch = 19, cex = 0.6)\n\n\n\n\n\n\n\n\n\n\n\n8.6.3 Computing adaptive distance weight matrix\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\nknn6 &lt;- knn2nb(knearneigh(coords, k=6))\nknn6\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 528 \nPercentage nonzero weights: 6.818182 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\nSimilarly, we can display the content of the matrix by using str().\n\nstr(knn6)\n\nList of 88\n $ : int [1:6] 2 3 4 5 57 64\n $ : int [1:6] 1 3 57 58 78 85\n $ : int [1:6] 1 2 4 5 57 85\n $ : int [1:6] 1 3 5 6 69 85\n $ : int [1:6] 1 3 4 6 69 85\n $ : int [1:6] 3 4 5 69 75 85\n $ : int [1:6] 9 66 67 71 74 84\n $ : int [1:6] 9 46 47 78 80 86\n $ : int [1:6] 8 46 66 68 84 86\n $ : int [1:6] 16 19 22 70 72 73\n $ : int [1:6] 10 14 16 17 70 72\n $ : int [1:6] 13 15 60 61 63 83\n $ : int [1:6] 12 15 60 61 63 83\n $ : int [1:6] 11 15 16 17 72 83\n $ : int [1:6] 12 13 14 17 60 83\n $ : int [1:6] 10 11 17 22 72 83\n $ : int [1:6] 10 11 14 16 72 83\n $ : int [1:6] 20 22 23 63 77 83\n $ : int [1:6] 10 20 21 73 74 82\n $ : int [1:6] 18 19 21 22 23 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:6] 10 16 18 19 20 83\n $ : int [1:6] 18 20 41 77 79 82\n $ : int [1:6] 25 28 31 52 54 81\n $ : int [1:6] 24 28 31 33 54 81\n $ : int [1:6] 25 27 29 33 42 81\n $ : int [1:6] 26 29 30 37 42 81\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:6] 26 27 37 42 43 81\n $ : int [1:6] 26 27 28 33 49 81\n $ : int [1:6] 24 25 36 39 40 54\n $ : int [1:6] 24 31 50 54 55 56\n $ : int [1:6] 25 26 28 30 49 81\n $ : int [1:6] 36 40 41 45 56 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:6] 26 27 29 42 43 44\n $ : int [1:6] 23 43 44 62 77 79\n $ : int [1:6] 25 40 42 43 44 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:6] 26 27 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:6] 37 38 39 42 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:6] 8 9 35 47 78 86\n $ : int [1:6] 8 21 35 46 80 86\n $ : int [1:6] 49 50 51 52 53 55\n $ : int [1:6] 28 33 48 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:6] 28 48 49 50 52 54\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:6] 48 50 51 52 55 75\n $ : int [1:6] 24 28 49 50 51 52\n $ : int [1:6] 32 48 50 52 53 75\n $ : int [1:6] 32 34 36 78 80 85\n $ : int [1:6] 1 2 3 58 64 68\n $ : int [1:6] 2 57 64 66 68 78\n $ : int [1:6] 12 13 60 61 87 88\n $ : int [1:6] 12 13 59 61 63 87\n $ : int [1:6] 12 13 60 62 63 87\n $ : int [1:6] 12 38 61 63 77 87\n $ : int [1:6] 12 18 60 61 62 83\n $ : int [1:6] 1 3 57 58 68 76\n $ : int [1:6] 58 64 66 67 68 76\n $ : int [1:6] 9 58 67 68 76 84\n $ : int [1:6] 7 65 66 68 76 84\n $ : int [1:6] 9 57 58 66 78 84\n $ : int [1:6] 4 5 6 32 75 85\n $ : int [1:6] 10 16 19 22 72 73\n $ : int [1:6] 7 19 73 74 84 86\n $ : int [1:6] 10 11 14 16 17 70\n $ : int [1:6] 10 19 21 70 71 74\n $ : int [1:6] 19 21 71 73 84 86\n $ : int [1:6] 6 32 50 53 55 69\n $ : int [1:6] 58 64 65 66 67 68\n $ : int [1:6] 18 23 38 61 62 63\n $ : int [1:6] 2 8 9 46 58 68\n $ : int [1:6] 38 40 41 43 44 45\n $ : int [1:6] 34 35 36 41 45 47\n $ : int [1:6] 25 26 28 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:6] 12 13 15 16 22 63\n $ : int [1:6] 7 9 66 68 71 74\n $ : int [1:6] 2 3 4 5 56 69\n $ : int [1:6] 8 9 21 46 47 74\n $ : int [1:6] 59 60 61 62 63 88\n $ : int [1:6] 59 60 61 62 63 87\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language knearneigh(x = coords, k = 6)\n - attr(*, \"sym\")= logi FALSE\n - attr(*, \"type\")= chr \"knn\"\n - attr(*, \"knn-k\")= num 6\n - attr(*, \"class\")= chr \"nb\"\n\n\nNotice that each county has six neighbours, no less no more!\n\n8.6.3.1 Plotting distance based neighbours\nWe can plot the weight matrix using the code chunk below.\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(knn6, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#weights-based-on-idw",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#weights-based-on-idw",
    "title": "Hands On Exercise 5: Spatial Weights and Applications",
    "section": "8.7 Weights based on IDW",
    "text": "8.7 Weights based on IDW\nIn this section, you will learn how to derive a spatial weight matrix based on Inversed Distance method.\nFirst, we will compute the distances between areas by using nbdists() of spdep.\n\ndist &lt;- nbdists(wm_q, coords, longlat = TRUE)\nids &lt;- lapply(dist, function(x) 1/(x))\nids\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n[[2]]\n[1] 0.01535405 0.01764308 0.01925924 0.02323898 0.01719350\n\n[[3]]\n[1] 0.03916350 0.02822040 0.03695795 0.01395765\n\n[[4]]\n[1] 0.01820896 0.02822040 0.03414741 0.01539065\n\n[[5]]\n[1] 0.03695795 0.03414741 0.01524598 0.01618354\n\n[[6]]\n[1] 0.015390649 0.015245977 0.021748129 0.011883901 0.009810297\n\n[[7]]\n[1] 0.01708612 0.01473997 0.01150924 0.01872915\n\n[[8]]\n[1] 0.02022144 0.03453056 0.02529256 0.01036340 0.02284457 0.01500600 0.01515314\n\n[[9]]\n[1] 0.02022144 0.01574888 0.02109502 0.01508028 0.02902705 0.01502980\n\n[[10]]\n[1] 0.02281552 0.01387777 0.01538326 0.01346650 0.02100510 0.02631658 0.01874863\n[8] 0.01500046\n\n[[11]]\n[1] 0.01882869 0.02243492 0.02247473\n\n[[12]]\n[1] 0.02779227 0.02419652 0.02333385 0.02986130 0.02335429\n\n[[13]]\n[1] 0.02779227 0.02650020 0.02670323 0.01714243\n\n[[14]]\n[1] 0.01882869 0.01233868 0.02098555\n\n[[15]]\n[1] 0.02650020 0.01233868 0.01096284 0.01562226\n\n[[16]]\n[1] 0.02281552 0.02466962 0.02765018 0.01476814 0.01671430\n\n[[17]]\n[1] 0.01387777 0.02243492 0.02098555 0.01096284 0.02466962 0.01593341 0.01437996\n\n[[18]]\n[1] 0.02039779 0.02032767 0.01481665 0.01473691 0.01459380\n\n[[19]]\n[1] 0.01538326 0.01926323 0.02668415 0.02140253 0.01613589 0.01412874\n\n[[20]]\n[1] 0.01346650 0.02039779 0.01926323 0.01723025 0.02153130 0.01469240 0.02327034\n\n[[21]]\n[1] 0.02668415 0.01723025 0.01766299 0.02644986 0.02163800\n\n[[22]]\n[1] 0.02100510 0.02765018 0.02032767 0.02153130 0.01489296\n\n[[23]]\n[1] 0.01481665 0.01469240 0.01401432 0.02246233 0.01880425 0.01530458 0.01849605\n\n[[24]]\n[1] 0.02354598 0.01837201 0.02607264 0.01220154 0.02514180\n\n[[25]]\n[1] 0.02354598 0.02188032 0.01577283 0.01949232 0.02947957\n\n[[26]]\n[1] 0.02155798 0.01745522 0.02212108 0.02220532\n\n[[27]]\n[1] 0.02155798 0.02490625 0.01562326\n\n[[28]]\n[1] 0.01837201 0.02188032 0.02229549 0.03076171 0.02039506\n\n[[29]]\n[1] 0.02490625 0.01686587 0.01395022\n\n[[30]]\n[1] 0.02090587\n\n[[31]]\n[1] 0.02607264 0.01577283 0.01219005 0.01724850 0.01229012 0.01609781 0.01139438\n[8] 0.01150130\n\n[[32]]\n[1] 0.01220154 0.01219005 0.01712515 0.01340413 0.01280928 0.01198216 0.01053374\n[8] 0.01065655\n\n[[33]]\n[1] 0.01949232 0.01745522 0.02229549 0.02090587 0.01979045\n\n[[34]]\n[1] 0.03113041 0.03589551 0.02882915\n\n[[35]]\n[1] 0.01766299 0.02185795 0.02616766 0.02111721 0.02108253 0.01509020\n\n[[36]]\n[1] 0.01724850 0.03113041 0.01571707 0.01860991 0.02073549 0.01680129\n\n[[37]]\n[1] 0.01686587 0.02234793 0.01510990 0.01550676\n\n[[38]]\n[1] 0.01401432 0.02407426 0.02276151 0.01719415\n\n[[39]]\n[1] 0.01229012 0.02172543 0.01711924 0.02629732 0.01896385\n\n[[40]]\n[1] 0.01609781 0.01571707 0.02172543 0.01506473 0.01987922 0.01894207\n\n[[41]]\n[1] 0.02246233 0.02185795 0.02205991 0.01912542 0.01601083 0.01742892\n\n[[42]]\n[1] 0.02212108 0.01562326 0.01395022 0.02234793 0.01711924 0.01836831 0.01683518\n\n[[43]]\n[1] 0.01510990 0.02629732 0.01506473 0.01836831 0.03112027 0.01530782\n\n[[44]]\n[1] 0.01550676 0.02407426 0.03112027 0.01486508\n\n[[45]]\n[1] 0.03589551 0.01860991 0.01987922 0.02205991 0.02107101 0.01982700\n\n[[46]]\n[1] 0.03453056 0.04033752 0.02689769\n\n[[47]]\n[1] 0.02529256 0.02616766 0.04033752 0.01949145 0.02181458\n\n[[48]]\n[1] 0.02313819 0.03370576 0.02289485 0.01630057 0.01818085\n\n[[49]]\n[1] 0.03076171 0.02138091 0.02394529 0.01990000\n\n[[50]]\n[1] 0.01712515 0.02313819 0.02551427 0.02051530 0.02187179\n\n[[51]]\n[1] 0.03370576 0.02138091 0.02873854\n\n[[52]]\n[1] 0.02289485 0.02394529 0.02551427 0.02873854 0.03516672\n\n[[53]]\n[1] 0.01630057 0.01979945 0.01253977\n\n[[54]]\n[1] 0.02514180 0.02039506 0.01340413 0.01990000 0.02051530 0.03516672\n\n[[55]]\n[1] 0.01280928 0.01818085 0.02187179 0.01979945 0.01882298\n\n[[56]]\n[1] 0.01036340 0.01139438 0.01198216 0.02073549 0.01214479 0.01362855 0.01341697\n\n[[57]]\n[1] 0.028079221 0.017643082 0.031423501 0.029114131 0.013520292 0.009903702\n\n[[58]]\n[1] 0.01925924 0.03142350 0.02722997 0.01434859 0.01567192\n\n[[59]]\n[1] 0.01696711 0.01265572 0.01667105 0.01785036\n\n[[60]]\n[1] 0.02419652 0.02670323 0.01696711 0.02343040\n\n[[61]]\n[1] 0.02333385 0.01265572 0.02343040 0.02514093 0.02790764 0.01219751 0.02362452\n\n[[62]]\n[1] 0.02514093 0.02002219 0.02110260\n\n[[63]]\n[1] 0.02986130 0.02790764 0.01407043 0.01805987\n\n[[64]]\n[1] 0.02911413 0.01689892\n\n[[65]]\n[1] 0.02471705\n\n[[66]]\n[1] 0.01574888 0.01726461 0.03068853 0.01954805 0.01810569\n\n[[67]]\n[1] 0.01708612 0.01726461 0.01349843 0.01361172\n\n[[68]]\n[1] 0.02109502 0.02722997 0.03068853 0.01406357 0.01546511\n\n[[69]]\n[1] 0.02174813 0.01645838 0.01419926\n\n[[70]]\n[1] 0.02631658 0.01963168 0.02278487\n\n[[71]]\n[1] 0.01473997 0.01838483 0.03197403\n\n[[72]]\n[1] 0.01874863 0.02247473 0.01476814 0.01593341 0.01963168\n\n[[73]]\n[1] 0.01500046 0.02140253 0.02278487 0.01838483 0.01652709\n\n[[74]]\n[1] 0.01150924 0.01613589 0.03197403 0.01652709 0.01342099 0.02864567\n\n[[75]]\n[1] 0.011883901 0.010533736 0.012539774 0.018822977 0.016458383 0.008217581\n\n[[76]]\n[1] 0.01352029 0.01434859 0.01689892 0.02471705 0.01954805 0.01349843 0.01406357\n\n[[77]]\n[1] 0.014736909 0.018804247 0.022761507 0.012197506 0.020022195 0.014070428\n[7] 0.008440896\n\n[[78]]\n[1] 0.02323898 0.02284457 0.01508028 0.01214479 0.01567192 0.01546511 0.01140779\n\n[[79]]\n[1] 0.01530458 0.01719415 0.01894207 0.01912542 0.01530782 0.01486508 0.02107101\n\n[[80]]\n[1] 0.01500600 0.02882915 0.02111721 0.01680129 0.01601083 0.01982700 0.01949145\n[8] 0.01362855\n\n[[81]]\n[1] 0.02947957 0.02220532 0.01150130 0.01979045 0.01896385 0.01683518\n\n[[82]]\n[1] 0.02327034 0.02644986 0.01849605 0.02108253 0.01742892\n\n[[83]]\n[1] 0.023354289 0.017142433 0.015622258 0.016714303 0.014379961 0.014593799\n[7] 0.014892965 0.018059871 0.008440896\n\n[[84]]\n[1] 0.01872915 0.02902705 0.01810569 0.01361172 0.01342099 0.01297994\n\n[[85]]\n [1] 0.011451133 0.017193502 0.013957649 0.016183544 0.009810297 0.010656545\n [7] 0.013416965 0.009903702 0.014199260 0.008217581 0.011407794\n\n[[86]]\n[1] 0.01515314 0.01502980 0.01412874 0.02163800 0.01509020 0.02689769 0.02181458\n[8] 0.02864567 0.01297994\n\n[[87]]\n[1] 0.01667105 0.02362452 0.02110260 0.02058034\n\n[[88]]\n[1] 0.01785036 0.02058034"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#row-standardised-weights-matrix",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#row-standardised-weights-matrix",
    "title": "Hands On Exercise 5: Spatial Weights and Applications",
    "section": "8.8 Row-standardised Weights Matrix",
    "text": "8.8 Row-standardised Weights Matrix\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\nrswm_q &lt;- nb2listw(wm_q, style=\"W\", zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nThe zero.policy=TRUE option allows for lists of non-neighbors. This should be used with caution since the user may not be aware of missing neighbors in their dataset however, a zero.policy of FALSE would return an error.\nTo see the weight of the first polygon’s eight neighbors type:\n\nrswm_q$weights[10]\n\n[[1]]\n[1] 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125\n\n\nEach neighbor is assigned a 0.125 of the total weight. This means that when R computes the average neighboring income values, each neighbor’s income will be multiplied by 0.125 before being tallied.\nUsing the same method, we can also derive a row standardised distance weight matrix by using the code chunk below.\n\nrswm_ids &lt;- nb2listw(wm_q, glist=ids, style=\"B\", zero.policy=TRUE)\nrswm_ids\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn       S0        S1     S2\nB 88 7744 8.786867 0.3776535 3.8137\n\n\n\nrswm_ids$weights[1]\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n\n\nsummary(unlist(rswm_ids$weights))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.008218 0.015088 0.018739 0.019614 0.022823 0.040338"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#application-of-spatial-weight-matrix",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#application-of-spatial-weight-matrix",
    "title": "Hands On Exercise 5: Spatial Weights and Applications",
    "section": "8.9 Application of Spatial Weight Matrix",
    "text": "8.9 Application of Spatial Weight Matrix\nIn this section, you will learn how to create four different spatial lagged variables, they are:\n\nspatial lag with row-standardized weights,\nspatial lag as a sum of neighbouring values,\nspatial window average, and\nspatial window sum.\n\n\n8.9.1 Spatial lag with row-standardized weights\nFinally, we’ll compute the average neighbor GDPPC value for each polygon. These values are often referred to as spatially lagged values.\n\nGDPPC.lag &lt;- lag.listw(rswm_q, hunan$GDPPC)\nGDPPC.lag\n\n [1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80 43747.00 33582.71\n [9] 45651.17 32027.62 32671.00 20810.00 25711.50 30672.33 33457.75 31689.20\n[17] 20269.00 23901.60 25126.17 21903.43 22718.60 25918.80 20307.00 20023.80\n[25] 16576.80 18667.00 14394.67 19848.80 15516.33 20518.00 17572.00 15200.12\n[33] 18413.80 14419.33 24094.50 22019.83 12923.50 14756.00 13869.80 12296.67\n[41] 15775.17 14382.86 11566.33 13199.50 23412.00 39541.00 36186.60 16559.60\n[49] 20772.50 19471.20 19827.33 15466.80 12925.67 18577.17 14943.00 24913.00\n[57] 25093.00 24428.80 17003.00 21143.75 20435.00 17131.33 24569.75 23835.50\n[65] 26360.00 47383.40 55157.75 37058.00 21546.67 23348.67 42323.67 28938.60\n[73] 25880.80 47345.67 18711.33 29087.29 20748.29 35933.71 15439.71 29787.50\n[81] 18145.00 21617.00 29203.89 41363.67 22259.09 44939.56 16902.00 16930.00\n\n\nRecalled in the previous section, we retrieved the GDPPC of these five countries by using the code chunk below.\n\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n[1] 20981 34592 24473 21311 22879\n\n\nWe can append the spatially lag GDPPC values onto hunan sf data frame by using the code chunk below.\n\nlag.list &lt;- list(hunan$NAME_3, lag.listw(rswm_q, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag.list)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag GDPPC\")\nhunan &lt;- left_join(hunan,lag.res)\n\nThe following table shows the average neighboring income values (stored in the Inc.lag object) for each county.\n\nhead(hunan)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC lag GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667  24847.20\n2 Changde 21100 Hanshou      County Hanshou 20981  22724.80\n3 Changde 21101  Jinshi County City  Jinshi 34592  24143.25\n4 Changde 21102      Li      County      Li 24473  27737.50\n5 Changde 21103   Linli      County   Linli 25554  27270.25\n6 Changde 21104  Shimen      County  Shimen 27137  21248.80\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675...\n\n\nNext, we will plot both the GDPPC and spatial lag GDPPC for comparison using the code chunk below.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_gdppc &lt;- qtm(hunan, \"lag GDPPC\")\ntmap_arrange(gdppc, lag_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n8.9.2 Spatial lag as a sum of neighboring values\nWe can calculate spatial lag as a sum of neighboring values by assigning binary weights. This requires us to go back to our neighbors list, then apply a function that will assign binary weights, then we use glist = in the nb2listw function to explicitly assign these weights.\nWe start by applying a function that will assign a value of 1 per each neighbor. This is done with lapply, which we have been using to manipulate the neighbors structure throughout the past notebooks. Basically it applies a function across each value in the neighbors structure.\n\nb_weights &lt;- lapply(wm_q, function(x) 0*x + 1)\nb_weights2 &lt;- nb2listw(wm_q, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\nWith the proper weights assigned, we can use lag.listw to compute a lag variable from our weight and GDPPC.\n\nlag_sum &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag_sum)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag_sum GDPPC\")\n\nFirst, let us examine the result by using the code chunk below.\n\nlag_sum\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 124236 113624  96573 110950 109081 106244 174988 235079 273907 256221\n[11]  98013 104050 102846  92017 133831 158446 141883 119508 150757 153324\n[21] 113593 129594 142149 100119  82884  74668  43184  99244  46549  20518\n[31] 140576 121601  92069  43258 144567 132119  51694  59024  69349  73780\n[41]  94651 100680  69398  52798 140472 118623 180933  82798  83090  97356\n[51]  59482  77334  38777 111463  74715 174391 150558 122144  68012  84575\n[61] 143045  51394  98279  47671  26360 236917 220631 185290  64640  70046\n[71] 126971 144693 129404 284074 112268 203611 145238 251536 108078 238300\n[81] 108870 108085 262835 248182 244850 404456  67608  33860\n\n\nNext, we will append the lag_sum GDPPC field into hunan sf data frame by using the code chunk below.\n\nhunan &lt;- left_join(hunan, lag.res)\n\nNow, We can plot both the GDPPC and Spatial Lag Sum GDPPC for comparison using the code chunk below.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_sum_gdppc &lt;- qtm(hunan, \"lag_sum GDPPC\")\ntmap_arrange(gdppc, lag_sum_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n8.9.3 Spatial window average\nThe spatial window average uses row-standardized weights and includes the diagonal element. To do this in R, we need to go back to the neighbors structure and add the diagonal element before assigning weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\nwm_qs &lt;- include.self(wm_q)\n\nNotice that the Number of nonzero links, Percentage nonzero weights and Average number of links are 536, 6.921488 and 6.090909 respectively as compared to wm_q of 448, 5.785124 and 5.090909\nLet us take a good look at the neighbour list of area [1] by using the code chunk below.\n\nwm_qs[[1]]\n\n[1]  1  2  3  4 57 85\n\n\nNotice that now [1] has six neighbours instead of five.\nNow we obtain weights with nb2listw()\n\nwm_qs &lt;- nb2listw(wm_qs)\nwm_qs\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 30.90265 357.5308\n\n\nAgain, we use nb2listw() and glist() to explicitly assign weight values.\nLastly, we just need to create the lag variable from our weight structure and GDPPC variable.\n\nlag_w_avg_gpdpc &lt;- lag.listw(wm_qs, \n                             hunan$GDPPC)\nlag_w_avg_gpdpc\n\n [1] 24650.50 22434.17 26233.00 27084.60 26927.00 22230.17 47621.20 37160.12\n [9] 49224.71 29886.89 26627.50 22690.17 25366.40 25825.75 30329.00 32682.83\n[17] 25948.62 23987.67 25463.14 21904.38 23127.50 25949.83 20018.75 19524.17\n[25] 18955.00 17800.40 15883.00 18831.33 14832.50 17965.00 17159.89 16199.44\n[33] 18764.50 26878.75 23188.86 20788.14 12365.20 15985.00 13764.83 11907.43\n[41] 17128.14 14593.62 11644.29 12706.00 21712.29 43548.25 35049.00 16226.83\n[49] 19294.40 18156.00 19954.75 18145.17 12132.75 18419.29 14050.83 23619.75\n[57] 24552.71 24733.67 16762.60 20932.60 19467.75 18334.00 22541.00 26028.00\n[65] 29128.50 46569.00 47576.60 36545.50 20838.50 22531.00 42115.50 27619.00\n[73] 27611.33 44523.29 18127.43 28746.38 20734.50 33880.62 14716.38 28516.22\n[81] 18086.14 21244.50 29568.80 48119.71 22310.75 43151.60 17133.40 17009.33\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\nlag.list.wm_qs &lt;- list(hunan$NAME_3, lag.listw(wm_qs, hunan$GDPPC))\nlag_wm_qs.res &lt;- as.data.frame(lag.list.wm_qs)\ncolnames(lag_wm_qs.res) &lt;- c(\"NAME_3\", \"lag_window_avg GDPPC\")\n\nNote: The third command line on the code chunk above renames the field names of lag_wm_q1.res object into NAME_3 and lag_window_avg GDPPC respectively.\nNext, the code chunk below will be used to append lag_window_avg GDPPC values onto hunan sf data.frame by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan, lag_wm_qs.res)\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\nhunan %&gt;%\n  select(\"County\", \n         \"lag GDPPC\", \n         \"lag_window_avg GDPPC\") %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag GDPPC\nlag_window_avg GDPPC\ngeometry\n\n\n\n\nAnxiang\n24847.20\n24650.50\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n22724.80\n22434.17\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n24143.25\n26233.00\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n27737.50\n27084.60\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n27270.25\n26927.00\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n21248.80\n22230.17\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n43747.00\n47621.20\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n33582.71\n37160.12\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n45651.17\n49224.71\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n32027.62\n29886.89\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n32671.00\n26627.50\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n20810.00\n22690.17\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n25711.50\n25366.40\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n30672.33\n25825.75\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n33457.75\n30329.00\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n31689.20\n32682.83\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n20269.00\n25948.62\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n23901.60\n23987.67\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n25126.17\n25463.14\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n21903.43\n21904.38\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n22718.60\n23127.50\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n25918.80\n25949.83\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n20307.00\n20018.75\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n20023.80\n19524.17\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n16576.80\n18955.00\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n18667.00\n17800.40\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n14394.67\n15883.00\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n19848.80\n18831.33\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n15516.33\n14832.50\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518.00\n17965.00\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n17572.00\n17159.89\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n15200.12\n16199.44\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n18413.80\n18764.50\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n14419.33\n26878.75\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n24094.50\n23188.86\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n22019.83\n20788.14\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n12923.50\n12365.20\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n14756.00\n15985.00\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n13869.80\n13764.83\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n12296.67\n11907.43\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n15775.17\n17128.14\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n14382.86\n14593.62\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n11566.33\n11644.29\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n13199.50\n12706.00\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n23412.00\n21712.29\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n39541.00\n43548.25\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n36186.60\n35049.00\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n16559.60\n16226.83\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n20772.50\n19294.40\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n19471.20\n18156.00\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n19827.33\n19954.75\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n15466.80\n18145.17\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n12925.67\n12132.75\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n18577.17\n18419.29\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n14943.00\n14050.83\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n24913.00\n23619.75\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n25093.00\n24552.71\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n24428.80\n24733.67\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n17003.00\n16762.60\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n21143.75\n20932.60\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n20435.00\n19467.75\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n17131.33\n18334.00\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n24569.75\n22541.00\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n23835.50\n26028.00\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360.00\n29128.50\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n47383.40\n46569.00\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n55157.75\n47576.60\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n37058.00\n36545.50\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n21546.67\n20838.50\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n23348.67\n22531.00\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n42323.67\n42115.50\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n28938.60\n27619.00\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n25880.80\n27611.33\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n47345.67\n44523.29\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n18711.33\n18127.43\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n29087.29\n28746.38\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n20748.29\n20734.50\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n35933.71\n33880.62\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n15439.71\n14716.38\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n29787.50\n28516.22\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n18145.00\n18086.14\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n21617.00\n21244.50\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n29203.89\n29568.80\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n41363.67\n48119.71\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n22259.09\n22310.75\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n44939.56\n43151.60\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n16902.00\n17133.40\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n16930.00\n17009.33\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nLastly, qtm() of tmap package is used to plot the lag_gdppc and w_ave_gdppc maps next to each other for quick comparison.\n\nw_avg_gdppc &lt;- qtm(hunan, \"lag_window_avg GDPPC\")\ntmap_arrange(lag_gdppc, w_avg_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\nNote: For more effective comparison, it is advicible to use the core tmap mapping functions.\n\n\n8.9.4 Spatial window sum\nThe spatial window sum is the counter part of the window average, but without using row-standardized weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\nwm_qs &lt;- include.self(wm_q)\nwm_qs\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\nNext, we will assign binary weights to the neighbour structure that includes the diagonal element.\n\nb_weights &lt;- lapply(wm_qs, function(x) 0*x + 1)\nb_weights[1]\n\n[[1]]\n[1] 1 1 1 1 1 1\n\n\nNotice that now [1] has six neighbours instead of five.\nAgain, we use nb2listw() and glist() to explicitly assign weight values.\n\nb_weights2 &lt;- nb2listw(wm_qs, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 536 1072 14160\n\n\nWith our new weight structure, we can compute the lag variable with lag.listw().\n\nw_sum_gdppc &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nw_sum_gdppc\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 147903 134605 131165 135423 134635 133381 238106 297281 344573 268982\n[11] 106510 136141 126832 103303 151645 196097 207589 143926 178242 175235\n[21] 138765 155699 160150 117145 113730  89002  63532 112988  59330  35930\n[31] 154439 145795 112587 107515 162322 145517  61826  79925  82589  83352\n[41] 119897 116749  81510  63530 151986 174193 210294  97361  96472 108936\n[51]  79819 108871  48531 128935  84305 188958 171869 148402  83813 104663\n[61] 155742  73336 112705  78084  58257 279414 237883 219273  83354  90124\n[71] 168462 165714 165668 311663 126892 229971 165876 271045 117731 256646\n[81] 126603 127467 295688 336838 267729 431516  85667  51028\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\nw_sum_gdppc.res &lt;- as.data.frame(w_sum_gdppc)\ncolnames(w_sum_gdppc.res) &lt;- c(\"NAME_3\", \"w_sum GDPPC\")\n\nNote: The second command line on the code chunk above renames the field names of w_sum_gdppc.res object into NAME_3 and w_sum GDPPC respectively.\nNext, the code chunk below will be used to append w_sum GDPPC values onto hunan sf data.frame by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan, w_sum_gdppc.res)\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\nhunan %&gt;%\n  select(\"County\", \"lag_sum GDPPC\", \"w_sum GDPPC\") %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag_sum GDPPC\nw_sum GDPPC\ngeometry\n\n\n\n\nAnxiang\n124236\n147903\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n113624\n134605\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n96573\n131165\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n110950\n135423\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n109081\n134635\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n106244\n133381\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n174988\n238106\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n235079\n297281\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n273907\n344573\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n256221\n268982\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n98013\n106510\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n104050\n136141\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n102846\n126832\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n92017\n103303\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n133831\n151645\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n158446\n196097\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n141883\n207589\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n119508\n143926\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n150757\n178242\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n153324\n175235\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n113593\n138765\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n129594\n155699\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n142149\n160150\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n100119\n117145\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n82884\n113730\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n74668\n89002\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n43184\n63532\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n99244\n112988\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n46549\n59330\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518\n35930\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n140576\n154439\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n121601\n145795\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n92069\n112587\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n43258\n107515\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n144567\n162322\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n132119\n145517\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n51694\n61826\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n59024\n79925\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n69349\n82589\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n73780\n83352\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n94651\n119897\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n100680\n116749\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n69398\n81510\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n52798\n63530\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n140472\n151986\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n118623\n174193\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n180933\n210294\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n82798\n97361\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n83090\n96472\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n97356\n108936\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n59482\n79819\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n77334\n108871\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n38777\n48531\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n111463\n128935\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n74715\n84305\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n174391\n188958\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n150558\n171869\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n122144\n148402\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n68012\n83813\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n84575\n104663\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n143045\n155742\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n51394\n73336\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n98279\n112705\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n47671\n78084\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360\n58257\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n236917\n279414\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n220631\n237883\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n185290\n219273\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n64640\n83354\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n70046\n90124\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n126971\n168462\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n144693\n165714\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n129404\n165668\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n284074\n311663\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n112268\n126892\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n203611\n229971\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n145238\n165876\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n251536\n271045\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n108078\n117731\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n238300\n256646\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n108870\n126603\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n108085\n127467\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n262835\n295688\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n248182\n336838\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n244850\n267729\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n404456\n431516\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n67608\n85667\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n33860\n51028\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nLastly, qtm() of tmap package is used to plot the lag_sum GDPPC and w_sum_gdppc maps next to each other for quick comparison.\n\nw_sum_gdppc &lt;- qtm(hunan, \"w_sum GDPPC\")\ntmap_arrange(lag_sum_gdppc, w_sum_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\nNote: For more effective comparison, it is advicible to use the core tmap mapping functions."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#references",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#references",
    "title": "Hands On Exercise 5: Spatial Weights and Applications",
    "section": "8.10 References",
    "text": "8.10 References\n\nCreating Neighbours using sf objects"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html",
    "title": "Hands-On Exercise 3: 1st Order Spatial Point Patterns Analysis Methods",
    "section": "",
    "text": "Spatial Point Pattern Analysis is the evaluation of the pattern or distribution, of a set of points on a surface. The point can be location of:\n\nevents such as crime, traffic accident and disease onset, or\nbusiness services (coffee and fast food outlets) or facilities such as childcare and eldercare.\n\nUsing appropriate functions of spatstat, this hands-on exercise aims to discover the spatial point processes of childecare centres in Singapore.\nThe specific questions we would like to answer are as follows:\n\nare the childcare centres in Singapore randomly distributed throughout the country?\nif the answer is not, then the next logical question is where are the locations with higher concentration of childcare centres?"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#overview",
    "title": "Hands-On Exercise 3: 1st Order Spatial Point Patterns Analysis Methods",
    "section": "",
    "text": "Spatial Point Pattern Analysis is the evaluation of the pattern or distribution, of a set of points on a surface. The point can be location of:\n\nevents such as crime, traffic accident and disease onset, or\nbusiness services (coffee and fast food outlets) or facilities such as childcare and eldercare.\n\nUsing appropriate functions of spatstat, this hands-on exercise aims to discover the spatial point processes of childecare centres in Singapore.\nThe specific questions we would like to answer are as follows:\n\nare the childcare centres in Singapore randomly distributed throughout the country?\nif the answer is not, then the next logical question is where are the locations with higher concentration of childcare centres?"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#the-data",
    "title": "Hands-On Exercise 3: 1st Order Spatial Point Patterns Analysis Methods",
    "section": "3.2 The Data",
    "text": "3.2 The Data\nTo provide answers to the questions above, three data sets will be used. They are:\n\nCHILDCARE, a point feature data providing both location and attribute information of childcare centres. It was downloaded from Data.gov.sg and is in geojson format.\nMP14_SUBZONE_WEB_PL, a polygon feature data providing information of URA 2014 Master Plan Planning Subzone boundary data. It is in ESRI shapefile format. This data set was also downloaded from Data.gov.sg.\nCostalOutline, a polygon feature data showing the national boundary of Singapore. It is provided by SLA and is in ESRI shapefile format."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#installing-and-loading-the-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#installing-and-loading-the-r-packages",
    "title": "Hands-On Exercise 3: 1st Order Spatial Point Patterns Analysis Methods",
    "section": "3.3 Installing and Loading the R packages",
    "text": "3.3 Installing and Loading the R packages\nIn this hands-on exercise, five R packages will be used, they are:\n\nsf, a relatively new R package specially designed to import, manage and process vector-based geospatial data in R.\nspatstat, which has a wide range of useful functions for point pattern analysis. In this hands-on exercise, it will be used to perform 1st- and 2nd-order spatial point patterns analysis and derive kernel density estimation (KDE) layer.\nraster which reads, writes, manipulates, analyses and model of gridded spatial data (i.e. raster). In this hands-on exercise, it will be used to convert image output generate by spatstat into raster format.\nmaptools which provides a set of tools for manipulating geographic data. In this hands-on exercise, we mainly use it to convert Spatial objects into ppp format of spatstat.\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\npacman::p_load(sf, raster, spatstat, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#spatial-data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#spatial-data-wrangling",
    "title": "Hands-On Exercise 3: 1st Order Spatial Point Patterns Analysis Methods",
    "section": "3.4 Spatial Data Wrangling",
    "text": "3.4 Spatial Data Wrangling\n\n3.4.1 Importing the spatial data\nIn this section, st_read() of sf package will be used to import these three geospatial data sets into R. st_transform is used in this case to project the data to the appropriate Singapore projection system after we have used st_crs to check the respective coordinate system of the data.\n\nchildcare_sf &lt;- st_read(\"data/child-care-services-geojson.geojson\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `child-care-services-geojson' from data source \n  `/Users/georgiaxng/georgiaxng/is415-handson/Hands-on_Ex/Hands-on_Ex03/data/child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1545 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\nsg_sf &lt;- st_read(dsn = \"data\", layer=\"CostalOutline\")\n\nReading layer `CostalOutline' from data source \n  `/Users/georgiaxng/georgiaxng/is415-handson/Hands-on_Ex/Hands-on_Ex03/data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21\n\n\n\nmpsz_sf &lt;- st_read(dsn = \"data\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/georgiaxng/georgiaxng/is415-handson/Hands-on_Ex/Hands-on_Ex03/data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\n3.4.2 Mapping the geospatial data sets\nAfter checking the referencing system of each geospatial data data frame, it is also useful for us to plot a map to show their spatial patterns.\n\nThe code below creates a static map using the tmap package in R. It first plots a layer of polygons (likely administrative boundaries) from the mpsz_sf dataset and then overlays it with points representing the locations of childcare centers from the childcare_sf dataset. This visualization helps to see the distribution of childcare centers within specific regions.\n\n\ntmap_mode(\"plot\")\ntm_shape(mpsz_sf) +\n  tm_polygons() + \n  tm_shape(childcare_sf) + \n  tm_dots() \n\n\n\n\n\n\n\n\nNotice that all the geospatial layers are within the same map extend. This shows that their referencing system and coordinate values are referred to similar spatial context. This is very important in any geospatial analysis.\nAlternatively, we can also prepare a pin map by using the code chunk below.\n\ntmap_mode(\"view\")\ntm_shape(childcare_sf) + \n  tm_dots() \n\n\n\n\n\nNotice that at the interactive mode, tmap is using leaflet for R API. The advantage of this interactive pin map is it allows us to navigate and zoom around the map freely. We can also query the information of each simple feature (i.e. the point) by clicking of them. Last but not least, you can also change the background of the internet map layer. Currently, three internet map layers are provided. They are: ESRI.WorldGrayCanvas, OpenStreetMap, and ESRI.WorldTopoMap. The default is ESRI.WorldGrayCanvas.\n\nReminder: Always remember to switch back to plot mode after the interactive map. This is because, each interactive mode will consume a connection. You should also avoid displaying ecessive numbers of interactive maps (i.e. not more than 10) in one RMarkdown document when publish on Netlify."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#geospatial-data-wrangling",
    "title": "Hands-On Exercise 3: 1st Order Spatial Point Patterns Analysis Methods",
    "section": "3.5 Geospatial Data wrangling",
    "text": "3.5 Geospatial Data wrangling\nAlthough simple feature data frame is gaining popularity again sp’s Spatial* classes, there are, however, many geospatial analysis packages require the input geospatial data in sp’s Spatial* classes. In this section, you will learn how to convert simple feature data frame to sp’s Spatial* class.\n\n3.5.1 Converting sf data frames to sp’s Spatial* class\nThe code chunk below uses as_Spatial() of sf package to convert the three geospatial data from simple feature data frame to sp’s Spatial* class.\n\nchildcare &lt;- as_Spatial(childcare_sf)\nmpsz &lt;- as_Spatial(mpsz_sf)\nsg &lt;- as_Spatial(sg_sf)\n\n\nchildcare\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1545 \nextent      : 11203.01, 45404.24, 25667.6, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 2\nnames       :    Name,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Description \nmin values  :   kml_1, &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;018989&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;1, MARINA BOULEVARD, #B1 - 01, ONE MARINA BOULEVARD, SINGAPORE 018989&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;THE LITTLE SKOOL-HOUSE INTERNATIONAL PTE. LTD.&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;08F73931F4A691F4&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt; \nmax values  : kml_999,                  &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;829646&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;200, PONGGOL SEVENTEENTH AVENUE, SINGAPORE 829646&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;RAFFLES KIDZ @ PUNGGOL PTE LTD&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;379D017BF244B0FA&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20200826094036&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt; \n\n\n\nmpsz\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 323 \nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +datum=WGS84 +units=m +no_defs \nvariables   : 15\nnames       : OBJECTID, SUBZONE_NO, SUBZONE_N, SUBZONE_C, CA_IND, PLN_AREA_N, PLN_AREA_C,       REGION_N, REGION_C,          INC_CRC, FMEL_UPD_D,     X_ADDR,     Y_ADDR,    SHAPE_Leng,    SHAPE_Area \nmin values  :        1,          1, ADMIRALTY,    AMSZ01,      N, ANG MO KIO,         AM, CENTRAL REGION,       CR, 00F5E30B5C9B7AD8,      16409,  5092.8949,  19579.069, 871.554887798, 39437.9352703 \nmax values  :      323,         17,    YUNNAN,    YSSZ09,      Y,     YISHUN,         YS,    WEST REGION,       WR, FFCCF172717C2EAF,      16409, 50424.7923, 49552.7904, 68083.9364708,  69748298.792 \n\n\n\nsg\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +datum=WGS84 +units=m +no_defs \nvariables   : 4\nnames       : GDO_GID, MSLINK, MAPID,              COSTAL_NAM \nmin values  :       1,      1,     0,             ISLAND LINK \nmax values  :      60,     67,     0, SINGAPORE - MAIN ISLAND \n\n\n\ntmap_mode(\"plot\")\ntm_shape(sg_sf) +\n  tm_polygons()\n\n\n\n\n\n\n\n\n\n\n3.5.2 Converting the Spatial* class into generic sp format\nspatstat requires the analytical data in ppp object form. There is no direct way to convert a Spatial* classes into ppp object. We need to convert the Spatial classes* into Spatial object first.\nThe codes chunk below converts the Spatial* classes into generic sp objects.\n\nchildcare_sp &lt;- as(childcare, \"SpatialPoints\")\nsg_sp &lt;- as(sg, \"SpatialPolygons\")\n\n\nchildcare_sp\n\nclass       : SpatialPoints \nfeatures    : 1545 \nextent      : 11203.01, 45404.24, 25667.6, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\n\n\nsg_sp\n\nclass       : SpatialPolygons \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +datum=WGS84 +units=m +no_defs \n\n\n\n\n3.5.3 Converting the generic sp format into spatstat’s ppp format\nNow, we will use as.ppp() function of spatstat to convert the spatial data into spatstat’s ppp object format.\n\nchildcare_ppp &lt;- as.ppp(st_coordinates(childcare_sf), st_bbox(childcare_sf))\nchildcare_ppp\n\nMarked planar point pattern: 1545 points\nmarks are numeric, of storage type  'double'\nwindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n\n\nNow, let us plot childcare_ppp and examine the difference.\n\nplot(childcare_ppp)\n\n\n\n\n\n\n\n\nWe can also get a quick overview of the statistics of the newly created ppp object by using the code chunk below.\n\nsummary(childcare_ppp)\n\nMarked planar point pattern:  1545 points\nAverage intensity 1.91145e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 11 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0       0       0       0 \n\nWindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n                    (34200 x 23630 units)\nWindow area = 808287000 square units\n\n\n\n\n3.5.4 Handling duplicated points\nWe can check the duplication in a ppp object by using the code chunk below. In this case, there exists duplicated data points in our data.\n\nany(duplicated(childcare_ppp))\n\n[1] TRUE\n\n\nTo count the number of co-indicence point, we will use the multiplicity() function as shown in the code chunk below.\n\nmultiplicity(childcare_ppp)\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n   1    1    1    3    1    1    1    1    2    1    1    1    1    1    1    1 \n  17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32 \n   1    1    1    1    1    1    1    1    1    1    9    1    1    1    1    1 \n  33   34   35   36   37   38   39   40   41   42   43   44   45   46   47   48 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n  49   50   51   52   53   54   55   56   57   58   59   60   61   62   63   64 \n   1    1    1    1    1    1    2    1    1    3    1    1    1    1    1    1 \n  65   66   67   68   69   70   71   72   73   74   75   76   77   78   79   80 \n   1    1    1    1    1    2    1    1    1    1    1    2    1    1    1    1 \n  81   82   83   84   85   86   87   88   89   90   91   92   93   94   95   96 \n   1    1    1    3    1    1    1    1    1    1    1    1    1    1    1    1 \n  97   98   99  100  101  102  103  104  105  106  107  108  109  110  111  112 \n   1    1    1    1    1    1    1    1    2    1    1    1    1    1    1    1 \n 113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128 \n   1    1    1    1    1    1    2    1    1    1    3    1    1    1    2    1 \n 129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144 \n   1    1    1    1    1    2    1    1    1    1    1    1    1    1    3    2 \n 145  146  147  148  149  150  151  152  153  154  155  156  157  158  159  160 \n   1    2    1    1    1    2    2    3    1    5    1    5    1    1    1    2 \n 161  162  163  164  165  166  167  168  169  170  171  172  173  174  175  176 \n   1    1    1    1    2    1    1    1    1    1    1    2    1    1    1    1 \n 177  178  179  180  181  182  183  184  185  186  187  188  189  190  191  192 \n   1    4    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 193  194  195  196  197  198  199  200  201  202  203  204  205  206  207  208 \n   1    1    1    1    1    2    2    1    1    1    1    2    1    4    1    1 \n 209  210  211  212  213  214  215  216  217  218  219  220  221  222  223  224 \n   2    1    1    1    1    1    1    1    1    1    1    1    2    1    1    1 \n 225  226  227  228  229  230  231  232  233  234  235  236  237  238  239  240 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 241  242  243  244  245  246  247  248  249  250  251  252  253  254  255  256 \n   1    1    1    1    2    1    1    1    1    1    1    1    1    1    1    1 \n 257  258  259  260  261  262  263  264  265  266  267  268  269  270  271  272 \n   1    1    1    1    1    1    1    1    1    1    2    1    1    1    1    3 \n 273  274  275  276  277  278  279  280  281  282  283  284  285  286  287  288 \n   1    1    1    1    1    1    3    1    1    1    1    1    1    1    1    1 \n 289  290  291  292  293  294  295  296  297  298  299  300  301  302  303  304 \n   1    1    1    1    1    1    1    9    1    1    2    1    1    1    1    1 \n 305  306  307  308  309  310  311  312  313  314  315  316  317  318  319  320 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 321  322  323  324  325  326  327  328  329  330  331  332  333  334  335  336 \n   1    1    1    5    1    1    1    1    1    2    1    1    2    2    1    1 \n 337  338  339  340  341  342  343  344  345  346  347  348  349  350  351  352 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    2    2    1 \n 353  354  355  356  357  358  359  360  361  362  363  364  365  366  367  368 \n   1    1    1    1    9    1    1    1    1    1    1    1    1    1    1    1 \n 369  370  371  372  373  374  375  376  377  378  379  380  381  382  383  384 \n   1    3    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 385  386  387  388  389  390  391  392  393  394  395  396  397  398  399  400 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 401  402  403  404  405  406  407  408  409  410  411  412  413  414  415  416 \n   1    1    2    1    1    1    1    1    1    1    2    1    1    1    1    1 \n 417  418  419  420  421  422  423  424  425  426  427  428  429  430  431  432 \n   1    1    1    1    1    1    1    2    1    1    2    1    1    1    1    1 \n 433  434  435  436  437  438  439  440  441  442  443  444  445  446  447  448 \n   1    1    1    1    2    1    1    1    1    1    1    1    1    1    1    1 \n 449  450  451  452  453  454  455  456  457  458  459  460  461  462  463  464 \n   1    1    9    9    1    1    1    1    1    1    1    1    1    1    2    1 \n 465  466  467  468  469  470  471  472  473  474  475  476  477  478  479  480 \n   2    1    1    1    1    1    1    1    1    1    1    1    2    2    1    1 \n 481  482  483  484  485  486  487  488  489  490  491  492  493  494  495  496 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 497  498  499  500  501  502  503  504  505  506  507  508  509  510  511  512 \n   1    1    1    1    1    1    2    1    1    1    1    1    1    1    1    2 \n 513  514  515  516  517  518  519  520  521  522  523  524  525  526  527  528 \n   1    1    1    1    1    1    1    1    1    1    1    2    1    1    3    1 \n 529  530  531  532  533  534  535  536  537  538  539  540  541  542  543  544 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 545  546  547  548  549  550  551  552  553  554  555  556  557  558  559  560 \n   1    1    1    1    1    1    1    1    1    3    1    1    1    1    1    1 \n 561  562  563  564  565  566  567  568  569  570  571  572  573  574  575  576 \n   2    2    2    1    1    1    1    2    1    1    2    1    1    1    2    1 \n 577  578  579  580  581  582  583  584  585  586  587  588  589  590  591  592 \n   1    2    1    1    1    1    1    9    1    4    1    2    1    1    1    1 \n 593  594  595  596  597  598  599  600  601  602  603  604  605  606  607  608 \n   2    1    1    1    1    1    1    1    2    1    2    1    1    1    1    1 \n 609  610  611  612  613  614  615  616  617  618  619  620  621  622  623  624 \n   1    1    1    1    1    1    1    1    1    2    1    2    1    1    1    1 \n 625  626  627  628  629  630  631  632  633  634  635  636  637  638  639  640 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 641  642  643  644  645  646  647  648  649  650  651  652  653  654  655  656 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    4 \n 657  658  659  660  661  662  663  664  665  666  667  668  669  670  671  672 \n   1    1    1    1    1    1    1    3    1    1    1    1    1    1    1    1 \n 673  674  675  676  677  678  679  680  681  682  683  684  685  686  687  688 \n   1    1    1    1    1    4    1    1    1    1    1    4    1    1    1    1 \n 689  690  691  692  693  694  695  696  697  698  699  700  701  702  703  704 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 705  706  707  708  709  710  711  712  713  714  715  716  717  718  719  720 \n   1    1    2    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 721  722  723  724  725  726  727  728  729  730  731  732  733  734  735  736 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 737  738  739  740  741  742  743  744  745  746  747  748  749  750  751  752 \n   1    2    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 753  754  755  756  757  758  759  760  761  762  763  764  765  766  767  768 \n   1    1    1    1    1    2    1    1    1    1    1    1    1    1    1    1 \n 769  770  771  772  773  774  775  776  777  778  779  780  781  782  783  784 \n   1    1    1    1    1    1    1    1    1    4    1    1    1    1    1    1 \n 785  786  787  788  789  790  791  792  793  794  795  796  797  798  799  800 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 801  802  803  804  805  806  807  808  809  810  811  812  813  814  815  816 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 817  818  819  820  821  822  823  824  825  826  827  828  829  830  831  832 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 833  834  835  836  837  838  839  840  841  842  843  844  845  846  847  848 \n   1    1    1    1    1    1    1    2    1    1    1    1    1    1    1    1 \n 849  850  851  852  853  854  855  856  857  858  859  860  861  862  863  864 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 865  866  867  868  869  870  871  872  873  874  875  876  877  878  879  880 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    2 \n 881  882  883  884  885  886  887  888  889  890  891  892  893  894  895  896 \n   3    1    1    1    2    1    1    1    3    1    1    3    1    1    1    1 \n 897  898  899  900  901  902  903  904  905  906  907  908  909  910  911  912 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 913  914  915  916  917  918  919  920  921  922  923  924  925  926  927  928 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 929  930  931  932  933  934  935  936  937  938  939  940  941  942  943  944 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 945  946  947  948  949  950  951  952  953  954  955  956  957  958  959  960 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    2 \n 961  962  963  964  965  966  967  968  969  970  971  972  973  974  975  976 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 977  978  979  980  981  982  983  984  985  986  987  988  989  990  991  992 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 993  994  995  996  997  998  999 1000 1001 1002 1003 1004 1005 1006 1007 1008 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 \n   1    1    1    1    1    1    1    1    1    2    2    1    1    1    1    1 \n1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 \n   1    1    1    1    1    2    1    1    1    1    1    1    1    1    1    1 \n1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 \n   1    1    1    1    1    1    1    1    2    2    1    1    1    5    1    1 \n1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    1 \n1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 \n   1    1    1    1    1    1    1    1    1    1    2    1    1    1    1    1 \n1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 \n   1    9    1    2    2    1    1    1    2    1    1    1    1    1    1    1 \n1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 \n   1    1    1    1    2    1    1    1    3    1    1    1    1    1    1    1 \n1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 \n   9    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 \n   1    1    1    2    1    1    1    1    1    1    1    1    1    1    1    1 \n1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    2 \n1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 \n   1    1    1    2    1    2    1    1    1    2    2    2    1    1    1    1 \n1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 \n   1    1    2    1    1    1    1    1    1    1    1    1    2    1    1    1 \n1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 \n   1    1    1    1    3    1    1    1    1    1    1    1    1    1    1    1 \n1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 \n   1    1    1    1    1    1    1    1    4    1    1    1    1    1    2    1 \n1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 \n   1    1    1    1    1    1    1    1    1    9    1    1    1    1    1    1 \n1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    2    1 \n1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 \n   1    2    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 \n   1    1    1    1    1    1    1    1    1    1    2    1    1    1    1    1 \n1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 \n   1    1    1    1    1    1    2    1    1    1    1    1    1    1    1    1 \n1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 \n   1    1    1    1    1    1    1    1    1    1    5    1    1    1    1    1 \n1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 \n   1    1    1    1    1    2    1    1    1    1    2    1    1    1    1    3 \n1537 1538 1539 1540 1541 1542 1543 1544 1545 \n   1    1    1    1    1    1    2    1    1 \n\n\n]If we want to know how many locations have more than one point event, we can use the code chunk below.\n\nsum(multiplicity(childcare_ppp) &gt; 1)\n\n[1] 128\n\n\nThe output shows that there are 128 duplicated point events.\nTo view the locations of these duplicate point events, we will plot childcare data by using the code chunk below.\n\ntmap_mode('view')\ntm_shape(childcare) +\n  tm_dots(alpha=0.4, \n          size=0.05)\n\n\n\n\n\n\ntmap_mode('plot')\n\n3 Ways to Handle Duplicate points\n\nDelete the duplicates (easiest solution, but may result in losing some useful point events).\nUse jittering to add a small perturbation to the duplicate points so that they do not occupy the exact same space.\nMake each point unique and attach duplicates to the patterns as marks or attributes, then use analytical techniques that consider these marks.\n\nBelow is the code chunk implementing the jittering approach:\n\nchildcare_ppp_jit &lt;- rjitter(childcare_ppp, \n                             retry=TRUE, \n                             nsim=1, \n                             drop=TRUE)\n\nNow to check if there are still any duplicated points in this geospatial data.\n\nany(duplicated(childcare_ppp_jit))\n\n[1] FALSE\n\n\n\n\n3.5.5 Creating owin object\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical area like Singapore boundary. In spatstat, an object called owin is specially designed to represent this polygonal region.\nThe code chunk below is used to covert sg SpatialPolygon object into owin object of spatstat.\n\nsg_owin &lt;- as.owin(sg_sf)\n\nThe ouput object can be displayed by using plot() function.\n\nplot(sg_owin)\n\n\n\n\n\n\n\n\nAnd with the summary() function we can view an overview of sg_owin.\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\n\n\n3.5.6 Combining point events object and owin object\nIn this last step of geospatial data wrangling, we will extract childcare events that are located within Singapore by using the code chunk below.\n\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\nsummary(childcareSG_ppp)\n\nMarked planar point pattern:  1545 points\nAverage intensity 2.129929e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 11 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0       0       0       0 \n\nWindow: polygonal boundary\n50 separate polygons (1 hole)\n                 vertices         area relative.area\npolygon 1 (hole)       30     -7081.18     -9.76e-06\npolygon 2              55     82537.90      1.14e-04\npolygon 3              90    415092.00      5.72e-04\npolygon 4              49     16698.60      2.30e-05\npolygon 5              38     24249.20      3.34e-05\npolygon 6             976  23344700.00      3.22e-02\npolygon 7             721   1927950.00      2.66e-03\npolygon 8            1992   9992170.00      1.38e-02\npolygon 9             330   1118960.00      1.54e-03\npolygon 10            175    925904.00      1.28e-03\npolygon 11            115    928394.00      1.28e-03\npolygon 12             24      6352.39      8.76e-06\npolygon 13            190    202489.00      2.79e-04\npolygon 14             37     10170.50      1.40e-05\npolygon 15             25     16622.70      2.29e-05\npolygon 16             10      2145.07      2.96e-06\npolygon 17             66     16184.10      2.23e-05\npolygon 18           5195 636837000.00      8.78e-01\npolygon 19             76    312332.00      4.31e-04\npolygon 20            627  31891300.00      4.40e-02\npolygon 21             20     32842.00      4.53e-05\npolygon 22             42     55831.70      7.70e-05\npolygon 23             67   1313540.00      1.81e-03\npolygon 24            734   4690930.00      6.47e-03\npolygon 25             16      3194.60      4.40e-06\npolygon 26             15      4872.96      6.72e-06\npolygon 27             15      4464.20      6.15e-06\npolygon 28             14      5466.74      7.54e-06\npolygon 29             37      5261.94      7.25e-06\npolygon 30            111    662927.00      9.14e-04\npolygon 31             69     56313.40      7.76e-05\npolygon 32            143    145139.00      2.00e-04\npolygon 33            397   2488210.00      3.43e-03\npolygon 34             90    115991.00      1.60e-04\npolygon 35             98     62682.90      8.64e-05\npolygon 36            165    338736.00      4.67e-04\npolygon 37            130     94046.50      1.30e-04\npolygon 38             93    430642.00      5.94e-04\npolygon 39             16      2010.46      2.77e-06\npolygon 40            415   3253840.00      4.49e-03\npolygon 41             30     10838.20      1.49e-05\npolygon 42             53     34400.30      4.74e-05\npolygon 43             26      8347.58      1.15e-05\npolygon 44             74     58223.40      8.03e-05\npolygon 45            327   2169210.00      2.99e-03\npolygon 46            177    467446.00      6.44e-04\npolygon 47             46    699702.00      9.65e-04\npolygon 48              6     16841.00      2.32e-05\npolygon 49             13     70087.30      9.66e-05\npolygon 50              4      9459.63      1.30e-05\nenclosing rectangle: [2663.93, 56047.79] x [16357.98, 50244.03] units\n                     (53380 x 33890 units)\nWindow area = 725376000 square units\nFraction of frame area: 0.401\n\n\nBelow is the plot of the newly derived childcareSG_ppp.\n\nplot(childcareSG_ppp)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#first-order-spatial-point-patterns-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#first-order-spatial-point-patterns-analysis",
    "title": "Hands-On Exercise 3: 1st Order Spatial Point Patterns Analysis Methods",
    "section": "3.6 First-order Spatial Point Patterns Analysis",
    "text": "3.6 First-order Spatial Point Patterns Analysis\nIn this section, we will be performing first-order SPPA by using spatstat package. The hands-on exercise will focus on:\n\nderiving kernel density estimation (KDE) layer for visualising and exploring the intensity of point processes,\nperforming Confirmatory Spatial Point Patterns Analysis by using Nearest Neighbour statistics.\n\n\n3.6.1 Kernel Density Estimation\nIn this section, you will learn how to compute the kernel density estimation (KDE) of childcare services in Singapore.\n\n3.6.1.1 Computing kernel density estimation using automatic bandwidth selection method\nThe code chunk below computes a kernel density by using the following configurations of density() of spatstat:\n\nbw.diggle() automatic bandwidth selection method. Other recommended methods are bw.CvL(), bw.scott() or bw.ppl().\nThe smoothing kernel used is gaussian, which is the default. Other smoothing methods are: “epanechnikov”, “quartic” or “disc”.\nThe intensity estimate is corrected for edge effect bias by using method described by Jones (1993) and Diggle (2010, equation 18.9). The default is FALSE.\n\n\nkde_childcareSG_bw &lt;- density(childcareSG_ppp,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                            kernel=\"gaussian\") \n\nThe plot() function of Base R is then used to display the kernel density derived.\n\nplot(kde_childcareSG_bw)\n\n\n\n\n\n\n\n\n\nThe density values of the output range from 0 to 0.000035 which is way too small to comprehend. This is because the default unit of measurement of svy21 is in meter. As a result, the density values computed is in “number of points per square meter”.\n\nBefore we move on to next section, it is good to know that we can retrieve the bandwidth used to compute the kde layer by using the code chunk below.\n\nbw &lt;- bw.diggle(childcareSG_ppp)\nbw\n\n   sigma \n298.4095 \n\n\n\n\n3.6.1.2 Rescalling KDE values\nIn the code chunk below, rescale.ppp() is used to covert the unit of measurement from meter to kilometer.\n\nchildcareSG_ppp.km &lt;- rescale.ppp(childcareSG_ppp, 1000, \"km\")\n\nNow, we can re-run density() using the resale data set and plot the output kde map.\n\nkde_childcareSG.bw &lt;- density(childcareSG_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw)\n\n\n\n\n\n\n\n\n\nWhile the output image looks identical to the earlier version, the only changes in the data values (refer to the legend).\n\n\n\n\n3.6.2 Working with different automatic badwidth methods\nBeside bw.diggle(), there are three other spatstat functions can be used to determine the bandwidth, they are: bw.CvL(), bw.scott(), and bw.ppl().\n\nbw.diggle(): Used in point pattern analysis to select a bandwidth that balances smoothing and detail.\nbw.CvL(): Utilized in spatial density estimation to choose the bandwidth that best fits the data through cross-validated likelihood.\nbw.scott(): Commonly applied in kernel density estimation to determine a fixed bandwidth based on Scott’s rule.\nbw.ppl(): Used in spatial point process analysis to find a bandwidth that minimizes the pseudolikelihood cross-validation score.\n\nLet us take a look at the bandwidth return by these automatic bandwidth calculation methods by using the code chunk below.\n\n bw.CvL(childcareSG_ppp.km)\n\n   sigma \n4.543278 \n\n\n\nbw.scott(childcareSG_ppp.km)\n\n sigma.x  sigma.y \n2.224898 1.450966 \n\n\n\nbw.ppl(childcareSG_ppp.km)\n\n    sigma \n0.3897114 \n\n\n\nbw.diggle(childcareSG_ppp.km)\n\n    sigma \n0.2984095 \n\n\nBaddeley et. (2016) suggested the use of the bw.ppl() algorithm because in their experience it tends to produce the more appropriate values when the pattern consists predominantly of tight clusters. But they also insist that if the purpose of once study is to detect a single tight cluster in the midst of random noise then the bw.diggle() method seems to work best.\nThe code chunk beow will be used to compare the output of using bw.diggle and bw.ppl methods.\n\nkde_childcareSG.ppl &lt;- density(childcareSG_ppp.km, \n                               sigma=bw.ppl, \n                               edge=TRUE,\n                               kernel=\"gaussian\")\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"bw.diggle\")\nplot(kde_childcareSG.ppl, main = \"bw.ppl\")\n\n\n\n\n\n\n\n\n\n\n3.6.3 Working with different kernel methods\nBy default, the kernel method used in density.ppp() is gaussian. But there are three other options, namely: Epanechnikov, Quartic and Dics.\nThe code chunk below will be used to compute three more kernel density estimations by using these three kernel function.\n\npar(mfrow=c(2,2))\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"gaussian\"), \n     main=\"Gaussian\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"epanechnikov\"), \n     main=\"Epanechnikov\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"quartic\"), \n     main=\"Quartic\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"disc\"), \n     main=\"Disc\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#fixed-and-adaptive-kde",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#fixed-and-adaptive-kde",
    "title": "Hands-On Exercise 3: 1st Order Spatial Point Patterns Analysis Methods",
    "section": "3.7 Fixed and Adaptive KDE",
    "text": "3.7 Fixed and Adaptive KDE\n\n3.7.1 Computing KDE by using fixed bandwidth\nNext, we will compute a KDE layer by defining a bandwidth of 600 meter. Notice that in the code chunk below, the sigma value used is 0.6. This is because the unit of measurement of childcareSG_ppp.km object is in kilometer, hence the 600m is 0.6km.\n\nkde_childcareSG_600 &lt;- density(childcareSG_ppp.km, sigma=0.6, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG_600)\n\n\n\n\n\n\n\n\n\n\n3.7.2 Computing KDE by using adaptive bandwidth\nFixed bandwidth method is very sensitive to highly skew distribution of spatial point patterns over geographical units for example urban versus rural. One way to overcome this problem is by using adaptive bandwidth instead.\nIn this section, you will learn how to derive adaptive kernel density estimation by using density.adaptive() of spatstat.\n\nkde_childcareSG_adaptive &lt;- adaptive.density(childcareSG_ppp.km, method=\"kernel\")\nplot(kde_childcareSG_adaptive)\n\n\n\n\n\n\n\n\nWe can compare the fixed and adaptive kernel density estimation outputs by using the code chunk below.\n\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"Fixed bandwidth\")\nplot(kde_childcareSG_adaptive, main = \"Adaptive bandwidth\")\n\n\n\n\n\n\n\n\n\n\n3.7.3 Converting KDE output into grid object.\n\ngridded_kde_childcareSG_bw &lt;- as(kde_childcareSG.bw, \"SpatialGridDataFrame\")\nspplot(gridded_kde_childcareSG_bw)\n\n\n\n\n\n\n\n\n\n3.7.3.1 Converting gridded output into raster\nNext, we will convert the gridded kernal density objects into RasterLayer object by using raster() of raster package.\n\nkde_childcareSG_bw_raster &lt;- raster(kde_childcareSG.bw)\n\nLet us take a look at the properties of kde_childcareSG_bw_raster RasterLayer.\n\nkde_childcareSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4170614, 0.2647348  (x, y)\nextent     : 2.663926, 56.04779, 16.35798, 50.24403  (xmin, xmax, ymin, ymax)\ncrs        : NA \nsource     : memory\nnames      : layer \nvalues     : -1.005814e-14, 28.51831  (min, max)\n\n\n\nNotice that the crs property is NA.\n\n\n\n3.7.3.2 Assigning projection systems\nThe code chunk below will be used to include the CRS information on kde_childcareSG_bw_raster RasterLayer.\n\nprojection(kde_childcareSG_bw_raster) &lt;- CRS(\"+init=EPSG:3414\")\nkde_childcareSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4170614, 0.2647348  (x, y)\nextent     : 2.663926, 56.04779, 16.35798, 50.24403  (xmin, xmax, ymin, ymax)\ncrs        : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +units=m +no_defs \nsource     : memory\nnames      : layer \nvalues     : -1.005814e-14, 28.51831  (min, max)\n\n\n\nNotice that the crs property is completed.\n\n\n\n\n3.7.4 Visualising the output in tmap\nFinally, we will display the raster in cartographic quality map using tmap package.\n\ntm_shape(kde_childcareSG_bw_raster) + \n  tm_raster(\"layer\", palette = \"viridis\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame = FALSE)\n\n\n\n\n\n\n\n\nNotice that the raster values are encoded explicitly onto the raster pixel using the values in “v”” field.\n\n\n3.7.5 Comparing Spatial Point Patterns using KDE\nIn this section, you will learn how to compare KDE of childcare at Ponggol, Tampines, Chua Chu Kang and Jurong West planning areas.\n\n3.7.5.1 Extracting study area\nThe code chunk below will be used to extract the target planning areas\n\npg &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"PUNGGOL\")\ntm &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"TAMPINES\")\nck &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"CHOA CHU KANG\")\njw &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"JURONG WEST\")\n\nPlotting target planning areas\n\npar(mfrow=c(2,2))\nplot(pg, main = \"Punggol\")\n\n\n\n\n\n\n\n\n\nplot(tm, main = \"Tampines\")\n\n\n\n\n\n\n\n\n\nplot(ck, main = \"Choa Chu Kang\")\n\n\n\n\n\n\n\n\n\nplot(jw, main = \"Jurong West\")\n\n\n\n\n\n\n\n\n\n\n3.7.5.2 Creating owin object\nNow, we will convert these sf objects into owin objects that is required by spatstat.\n\npg_owin = as.owin(pg)\ntm_owin = as.owin(tm)\nck_owin = as.owin(ck)\njw_owin = as.owin(jw)\n\n\n\n3.7.5.3 Combining childcare points and the study area\nBy using the code chunk below, we are able to extract childcare that is within the specific region to do our analysis later on.\n\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_ck_ppp = childcare_ppp_jit[ck_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\nNext, rescale.ppp() function is used to trasnform the unit of measurement from metre to kilometre.\n\nchildcare_pg_ppp.km = rescale.ppp(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale.ppp(childcare_tm_ppp, 1000, \"km\")\nchildcare_ck_ppp.km = rescale.ppp(childcare_ck_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale.ppp(childcare_jw_ppp, 1000, \"km\")\n\nThe code chunk below is used to plot these four study areas and the locations of the childcare centres.\n\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\nplot(childcare_tm_ppp.km, main=\"Tampines\")\nplot(childcare_ck_ppp.km, main=\"Choa Chu Kang\")\nplot(childcare_jw_ppp.km, main=\"Jurong West\")\n\n\n\n\n\n\n\n\n\n\n3.7.5.4 Computing KDE\nThe code chunk below will be used to compute the KDE of these four planning area. bw.diggle method is used to derive the bandwidth of each\n\npar(mfrow=c(2,2))\nplot(density(childcare_pg_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tempines\")\nplot(density(childcare_ck_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Choa Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"JUrong West\")\n\n\n\n\n\n\n\n\n\n\n3.7.5.5 Computing fixed bandwidth KDE\nFor comparison purposes, we will use 250m as the bandwidth.\n\npar(mfrow=c(2,2))\nplot(density(childcare_ck_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Chou Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"JUrong West\")\nplot(density(childcare_pg_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#nearest-neighbour-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#nearest-neighbour-analysis",
    "title": "Hands-On Exercise 3: 1st Order Spatial Point Patterns Analysis Methods",
    "section": "3.8 Nearest Neighbour Analysis",
    "text": "3.8 Nearest Neighbour Analysis\nIn this section, we will perform the Clark-Evans test of aggregation for a spatial point pattern by using clarkevans.test() of statspat.\nThe test hypotheses are:\nHo = The distribution of childcare services are randomly distributed.\nH1= The distribution of childcare services are not randomly distributed.\nThe 95% confident interval will be used.\n\n3.8.1 Testing spatial point patterns using Clark and Evans Test\n\nclarkevans.test(childcareSG_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcareSG_ppp\nR = 0.55631, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\n\nInterpretation\n\nR Value: The R value is less than 1, which suggests that the points are indeed clustered rather than randomly distributed or uniformly dispersed.\nP-Value: The very small p-value (less than 2.2e-16) indicates that the observed clustering is statistically significant. This means the likelihood of observing such clustering by chance alone is extremely low.\nTo conclude, the test result suggests that the points (e.g., childcare centers) are significantly clustered. The result is highly significant and supports the conclusion that the distribution of points is not random but rather shows a tendency to cluster together. This could imply that certain areas are more densely populated with childcare centers than would be expected by random chance.\n\n\n\n\n3.8.2 Clark and Evans Test: Choa Chu Kang planning area\nIn the code chunk below, clarkevans.test() of spatstat is used to performs Clark-Evans test of aggregation for childcare centre in Choa Chu Kang planning area.\n\nclarkevans.test(childcare_ck_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_ck_ppp\nR = 0.9428, p-value = 0.3928\nalternative hypothesis: two-sided\n\n\n\n\n3.8.3 Clark and Evans Test: Tampines planning area\nIn the code chunk below, the similar test is used to analyse the spatial point patterns of childcare centre in Tampines planning area.\n\nclarkevans.test(childcare_tm_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_tm_ppp\nR = 0.7773, p-value = 5.841e-05\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analysing-spatial-point-process-using",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analysing-spatial-point-process-using",
    "title": "Hands-On Exercise 3: 1st Order Spatial Point Patterns Analysis Methods",
    "section": "3.9 Analysing Spatial Point Process Using",
    "text": "3.9 Analysing Spatial Point Process Using\n\n3.9.1 Using G-Function\nThe G function measures the distribution of the distances from an arbitrary event to its nearest event.\nIn this section, we will learn how to compute G-function estimation by using Gest() of spatstat package. We will also monta carlo simulation test using envelope() of spatstat package.\n\n3.9.1.1 Choa Chu Kang planning area\nComputing G-function estimation\nThe code chunk below is used to compute G-function using Gest() of spatat package.\n\nG_CK = Gest(childcare_ck_ppp, correction = \"border\")\nplot(G_CK, xlim=c(0,500))\n\n\n\n\n\n\n\n\nPerforming Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with G-function\n\nG_CK.csr &lt;- envelope(childcare_ck_ppp, Gest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\nplot(G_CK.csr)\n\n\n\n\n\n\n\n\n\n\n3.9.1.2 Tampines planning area\nComputing G-function Estimate\n\nG_tm = Gest(childcare_tm_ppp, correction = \"best\")\nplot(G_tm)\n\n\n\n\n\n\n\n\nPerforming Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected is p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\nG_tm.csr &lt;- envelope(childcare_tm_ppp, Gest, correction = \"all\", nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\nplot(G_tm.csr)\n\n\n\n\n\n\n\n\n\n\n\n3.9.2 Using F-Function\nThe F function estimates the empty space function F(r) or its hazard rate h(r) from a point pattern in a window of arbitrary shape. In this section, you will learn how to compute F-function estimation by using Fest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package\n\n3.9.2.1 Choa Chu Kang planning area\nComputing F-function estimation\nThe code chunk below is used to compute F-function using Fest() of spatat package.\n\nF_CK = Fest(childcare_ck_ppp)\nplot(F_CK)\n\n\n\n\n\n\n\n\nPerforming Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with F-function\n\nF_CK.csr &lt;- envelope(childcare_ck_ppp, Fest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\nplot(F_CK.csr)\n\n\n\n\n\n\n\n\n\n\n3.9.2.2 Tampines planning area\nComputing F-function estimation\nMonte Carlo test with F-fucntion\n\nF_tm = Fest(childcare_tm_ppp, correction = \"best\")\nplot(F_tm)\n\n\n\n\n\n\n\n\nPerforming Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected is p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\nF_tm.csr &lt;- envelope(childcare_tm_ppp, Fest, correction = \"all\", nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\nplot(F_tm.csr)\n\n\n\n\n\n\n\n\n\n\n\n3.9.3 Using K-Function\n\n3.9.3.1 Choa Chu Kang planning area\nComputing K-function estimate\n\nK_ck = Kest(childcare_ck_ppp, correction = \"Ripley\")\nplot(K_ck, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\n\nPerforming Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\nK_ck.csr &lt;- envelope(childcare_ck_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\nplot(K_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"K(d)-r\")\n\n\n\n\n\n\n\n\n\n\n3.9.3.2 Tampines planning area\nComputing K-function estimation\n\nK_tm = Kest(childcare_tm_ppp, correction = \"Ripley\")\nplot(K_tm, . -r ~ r, \n     ylab= \"K(d)-r\", xlab = \"d(m)\", \n     xlim=c(0,1000))\n\n\n\n\n\n\n\n\nPerforming Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\nK_tm.csr &lt;- envelope(childcare_tm_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\nplot(K_tm.csr, . - r ~ r, \n     xlab=\"d\", ylab=\"K(d)-r\", xlim=c(0,500))\n\n\n\n\n\n\n\n\n\n\n\n3.9.4 Using L-Function\nIn this section, you will learn how to compute L-function estimation by using Lest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n3.9.4.1 Choa Chu Kang planning area\nComputing L Function estimation\n\nL_ck = Lest(childcare_ck_ppp, correction = \"Ripley\")\nplot(L_ck, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\n\nPerforming Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value if smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\nL_ck.csr &lt;- envelope(childcare_ck_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\nplot(L_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")\n\n\n\n\n\n\n\n\n\n\n3.9.4.2 Tampines planning area\nComputing L-function estimate\n\nL_tm = Lest(childcare_tm_ppp, correction = \"Ripley\")\nplot(L_tm, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\", \n     xlim=c(0,1000))\n\n\n\n\n\n\n\n\nPerforming Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nThe code chunk below will be used to perform the hypothesis testing.\n\nL_tm.csr &lt;- envelope(childcare_tm_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\nThen, plot the model output by using the code chunk below.\n\nplot(L_tm.csr, . - r ~ r, \n     xlab=\"d\", ylab=\"L(d)-r\", xlim=c(0,500))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html",
    "title": "Hands-On Exercise 10: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "",
    "text": "Geographically weighted regression (GWR) is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this hands-on exercise, you will learn how to build hedonic pricing models by using GWR methods. The dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#overview",
    "title": "Hands-On Exercise 10: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "",
    "text": "Geographically weighted regression (GWR) is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this hands-on exercise, you will learn how to build hedonic pricing models by using GWR methods. The dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#the-data",
    "title": "Hands-On Exercise 10: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "10.2 The Data",
    "text": "10.2 The Data\nTwo data sets will be used in this model building exercise, they are:\n\nURA Master Plan subzone boundary in shapefile format (i.e. MP14_SUBZONE_WEB_PL)\ncondo_resale_2015 in csv format (i.e. condo_resale_2015.csv)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#getting-started",
    "title": "Hands-On Exercise 10: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "10.3 Getting Started",
    "text": "10.3 Getting Started\nBefore we get started, it is important for us to install the necessary R packages into R and launch these R packages into R environment.\nThe R packages needed for this exercise are as follows:\n\nR package for building OLS and performing diagnostics tests\n\nolsrr\n\nR package for calibrating geographical weighted family of models\n\nGWmodel\n\nR package for multivariate data visualisation and analysis\n\ncorrplot\n\nSpatial data handling\n\nsf\n\nAttribute data handling\n\ntidyverse, especially readr, ggplot2 and dplyr\n\nChoropleth mapping\n\ntmap\n\n\nThe code chunks below installs and launches these R packages into R environment.\n\npacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#a-short-note-about-gwmodel",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#a-short-note-about-gwmodel",
    "title": "Hands-On Exercise 10: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "10.4 A short note about GWmodel",
    "text": "10.4 A short note about GWmodel\nGWmodel package provides a collection of localised spatial statistical methods, namely: GW summary statistics, GW principal components analysis, GW discriminant analysis and various forms of GW regression; some of which are provided in basic and robust (outlier resistant) forms. Commonly, outputs or parameters of the GWmodel are mapped to provide a useful exploratory tool, which can often precede (and direct) a more traditional or sophisticated statistical analysis."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#geospatial-data-wrangling",
    "title": "Hands-On Exercise 10: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "10.5 Geospatial Data Wrangling",
    "text": "10.5 Geospatial Data Wrangling\n\n10.5.1 Importing geospatial data\nThe geospatial data used in this hands-on exercise is called MP14_SUBZONE_WEB_PL. It is in ESRI shapefile format. The shapefile consists of URA Master Plan 2014’s planning subzone boundaries. Polygon features are used to represent these geographic boundaries. The GIS data is in svy21 projected coordinates systems.\nThe code chunk below is used to import MP_SUBZONE_WEB_PL shapefile by using st_read() of sf packages.\n\nmpsz = st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/georgiaxng/georgiaxng/is415-handson/Hands-on_Ex/Hands-on_Ex10/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nThe report above shows that the R object used to contain the imported MP14_SUBZONE_WEB_PL shapefile is called mpsz and it is a simple feature object. The geometry type is multipolygon. it is also important to note that mpsz simple feature object does not have EPSG information.\n\n\n10.5.2 Updating CRS information\nThe code chunk below updates the newly imported mpsz with the correct ESPG code (i.e. 3414)\n\nmpsz_svy21 &lt;- st_transform(mpsz, 3414)\n\nAfter transforming the projection metadata, you can varify the projection of the newly transformed mpsz_svy21 by using st_crs() of sf package.\nThe code chunk below will be used to varify the newly transformed mpsz_svy21.\n\nst_crs(mpsz_svy21)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nNotice that the EPSG: is indicated as 3414 now.\nNext, you will reveal the extent of mpsz_svy21 by using st_bbox() of sf package.\n\nst_bbox(mpsz_svy21) #view extent\n\n     xmin      ymin      xmax      ymax \n 2667.538 15748.721 56396.440 50256.334"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#aspatial-data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#aspatial-data-wrangling",
    "title": "Hands-On Exercise 10: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "10.6 Aspatial Data Wrangling",
    "text": "10.6 Aspatial Data Wrangling\n\n10.6.1 Importing the aspatial data\nThe condo_resale_2015 is in csv file format. The codes chunk below uses read_csv() function of readr package to import condo_resale_2015 into R as a tibble data frame called condo_resale.\n\ncondo_resale = read_csv(\"data/aspatial/Condo_resale_2015.csv\")\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly.\nThe codes chunks below uses glimpse() to display the data structure of will do the job.\n\nglimpse(condo_resale)\n\nRows: 1,436\nColumns: 23\n$ LATITUDE             &lt;dbl&gt; 1.287145, 1.328698, 1.313727, 1.308563, 1.321437,…\n$ LONGITUDE            &lt;dbl&gt; 103.7802, 103.8123, 103.7971, 103.8247, 103.9505,…\n$ POSTCODE             &lt;dbl&gt; 118635, 288420, 267833, 258380, 467169, 466472, 3…\n$ SELLING_PRICE        &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1320…\n$ AREA_SQM             &lt;dbl&gt; 309, 290, 248, 127, 145, 139, 218, 141, 165, 168,…\n$ AGE                  &lt;dbl&gt; 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22, 6,…\n$ PROX_CBD             &lt;dbl&gt; 7.941259, 6.609797, 6.898000, 4.038861, 11.783402…\n$ PROX_CHILDCARE       &lt;dbl&gt; 0.16597932, 0.28027246, 0.42922669, 0.39473543, 0…\n$ PROX_ELDERLYCARE     &lt;dbl&gt; 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.121…\n$ PROX_URA_GROWTH_AREA &lt;dbl&gt; 6.618741, 7.505109, 6.463887, 4.906512, 6.410632,…\n$ PROX_HAWKER_MARKET   &lt;dbl&gt; 1.76542207, 0.54507614, 0.37789301, 1.68259969, 0…\n$ PROX_KINDERGARTEN    &lt;dbl&gt; 0.05835552, 0.61592412, 0.14120309, 0.38200076, 0…\n$ PROX_MRT             &lt;dbl&gt; 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.528…\n$ PROX_PARK            &lt;dbl&gt; 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.116…\n$ PROX_PRIMARY_SCH     &lt;dbl&gt; 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.709…\n$ PROX_TOP_PRIMARY_SCH &lt;dbl&gt; 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.709…\n$ PROX_SHOPPING_MALL   &lt;dbl&gt; 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.307…\n$ PROX_SUPERMARKET     &lt;dbl&gt; 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.581…\n$ PROX_BUS_STOP        &lt;dbl&gt; 0.10336166, 0.28673408, 0.28504777, 0.29872340, 0…\n$ NO_Of_UNITS          &lt;dbl&gt; 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34, 3…\n$ FAMILY_FRIENDLY      &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\n\nhead(condo_resale$LONGITUDE) #see the data in XCOORD column\n\n[1] 103.7802 103.8123 103.7971 103.8247 103.9505 103.9386\n\nhead(condo_resale$LATITUDE) #see the data in YCOORD column\n\n[1] 1.287145 1.328698 1.313727 1.308563 1.321437 1.314198\n\n\nNext, summary() of base R is used to display the summary statistics of cond_resale tibble data frame.\n\nsummary(condo_resale)\n\n    LATITUDE       LONGITUDE        POSTCODE      SELLING_PRICE     \n Min.   :1.240   Min.   :103.7   Min.   : 18965   Min.   :  540000  \n 1st Qu.:1.309   1st Qu.:103.8   1st Qu.:259849   1st Qu.: 1100000  \n Median :1.328   Median :103.8   Median :469298   Median : 1383222  \n Mean   :1.334   Mean   :103.8   Mean   :440439   Mean   : 1751211  \n 3rd Qu.:1.357   3rd Qu.:103.9   3rd Qu.:589486   3rd Qu.: 1950000  \n Max.   :1.454   Max.   :104.0   Max.   :828833   Max.   :18000000  \n    AREA_SQM          AGE           PROX_CBD       PROX_CHILDCARE    \n Min.   : 34.0   Min.   : 0.00   Min.   : 0.3869   Min.   :0.004927  \n 1st Qu.:103.0   1st Qu.: 5.00   1st Qu.: 5.5574   1st Qu.:0.174481  \n Median :121.0   Median :11.00   Median : 9.3567   Median :0.258135  \n Mean   :136.5   Mean   :12.14   Mean   : 9.3254   Mean   :0.326313  \n 3rd Qu.:156.0   3rd Qu.:18.00   3rd Qu.:12.6661   3rd Qu.:0.368293  \n Max.   :619.0   Max.   :37.00   Max.   :19.1804   Max.   :3.465726  \n PROX_ELDERLYCARE  PROX_URA_GROWTH_AREA PROX_HAWKER_MARKET PROX_KINDERGARTEN \n Min.   :0.05451   Min.   :0.2145       Min.   :0.05182    Min.   :0.004927  \n 1st Qu.:0.61254   1st Qu.:3.1643       1st Qu.:0.55245    1st Qu.:0.276345  \n Median :0.94179   Median :4.6186       Median :0.90842    Median :0.413385  \n Mean   :1.05351   Mean   :4.5981       Mean   :1.27987    Mean   :0.458903  \n 3rd Qu.:1.35122   3rd Qu.:5.7550       3rd Qu.:1.68578    3rd Qu.:0.578474  \n Max.   :3.94916   Max.   :9.1554       Max.   :5.37435    Max.   :2.229045  \n    PROX_MRT         PROX_PARK       PROX_PRIMARY_SCH  PROX_TOP_PRIMARY_SCH\n Min.   :0.05278   Min.   :0.02906   Min.   :0.07711   Min.   :0.07711     \n 1st Qu.:0.34646   1st Qu.:0.26211   1st Qu.:0.44024   1st Qu.:1.34451     \n Median :0.57430   Median :0.39926   Median :0.63505   Median :1.88213     \n Mean   :0.67316   Mean   :0.49802   Mean   :0.75471   Mean   :2.27347     \n 3rd Qu.:0.84844   3rd Qu.:0.65592   3rd Qu.:0.95104   3rd Qu.:2.90954     \n Max.   :3.48037   Max.   :2.16105   Max.   :3.92899   Max.   :6.74819     \n PROX_SHOPPING_MALL PROX_SUPERMARKET PROX_BUS_STOP       NO_Of_UNITS    \n Min.   :0.0000     Min.   :0.0000   Min.   :0.001595   Min.   :  18.0  \n 1st Qu.:0.5258     1st Qu.:0.3695   1st Qu.:0.098356   1st Qu.: 188.8  \n Median :0.9357     Median :0.5687   Median :0.151710   Median : 360.0  \n Mean   :1.0455     Mean   :0.6141   Mean   :0.193974   Mean   : 409.2  \n 3rd Qu.:1.3994     3rd Qu.:0.7862   3rd Qu.:0.220466   3rd Qu.: 590.0  \n Max.   :3.4774     Max.   :2.2441   Max.   :2.476639   Max.   :1703.0  \n FAMILY_FRIENDLY     FREEHOLD      LEASEHOLD_99YR  \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.4868   Mean   :0.4227   Mean   :0.4882  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n\n\n\n\n10.6.2 Converting aspatial data frame into a sf object\nCurrently, the condo_resale tibble data frame is aspatial. We will convert it to a sf object. The code chunk below converts condo_resale data frame into a simple feature data frame by using st_as_sf() of sf packages.\n\ncondo_resale.sf &lt;- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs=4326) %&gt;%\n  st_transform(crs=3414)\n\nNotice that st_transform() of sf package is used to convert the coordinates from wgs84 (i.e. crs:4326) to svy21 (i.e. crs=3414).\nNext, head() is used to list the content of condo_resale.sf object.\n\nhead(condo_resale.sf)\n\nSimple feature collection with 6 features and 21 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 22085.12 ymin: 29951.54 xmax: 41042.56 ymax: 34546.2\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 6 × 22\n  POSTCODE SELLING_PRICE AREA_SQM   AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE\n     &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;\n1   118635       3000000      309    30     7.94          0.166            2.52 \n2   288420       3880000      290    32     6.61          0.280            1.93 \n3   267833       3325000      248    33     6.90          0.429            0.502\n4   258380       4250000      127     7     4.04          0.395            1.99 \n5   467169       1400000      145    28    11.8           0.119            1.12 \n6   466472       1320000      139    22    10.3           0.125            0.789\n# ℹ 15 more variables: PROX_URA_GROWTH_AREA &lt;dbl&gt;, PROX_HAWKER_MARKET &lt;dbl&gt;,\n#   PROX_KINDERGARTEN &lt;dbl&gt;, PROX_MRT &lt;dbl&gt;, PROX_PARK &lt;dbl&gt;,\n#   PROX_PRIMARY_SCH &lt;dbl&gt;, PROX_TOP_PRIMARY_SCH &lt;dbl&gt;,\n#   PROX_SHOPPING_MALL &lt;dbl&gt;, PROX_SUPERMARKET &lt;dbl&gt;, PROX_BUS_STOP &lt;dbl&gt;,\n#   NO_Of_UNITS &lt;dbl&gt;, FAMILY_FRIENDLY &lt;dbl&gt;, FREEHOLD &lt;dbl&gt;,\n#   LEASEHOLD_99YR &lt;dbl&gt;, geometry &lt;POINT [m]&gt;\n\n\nNotice that the output is in point feature data frame."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#exploratory-data-analysis-eda",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#exploratory-data-analysis-eda",
    "title": "Hands-On Exercise 10: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "10.7 Exploratory Data Analysis (EDA)",
    "text": "10.7 Exploratory Data Analysis (EDA)\nIn the section, you will learn how to use statistical graphics functions of ggplot2 package to perform EDA.\n\n10.7.1 EDA using statistical graphics\nWe can plot the distribution of SELLING_PRICE by using appropriate Exploratory Data Analysis (EDA) as shown in the code chunk below.\n\nggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\n\n\n\n\n\n\n\nThe figure above reveals a right skewed distribution. This means that more condominium units were transacted at relative lower prices.\nStatistically, the skewed dsitribution can be normalised by using log transformation. The code chunk below is used to derive a new variable called LOG_SELLING_PRICE by using a log transformation on the variable SELLING_PRICE. It is performed using mutate() of dplyr package.\n\ncondo_resale.sf &lt;- condo_resale.sf %&gt;%\n  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))\n\nNow, you can plot the LOG_SELLING_PRICE using the code chunk below.\n\nggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\n\n\n\n\n\n\n\nNotice that the distribution is relatively less skewed after the transformation.\n\n\n10.7.2 Multiple Histogram Plots distribution of variables\nIn this section, you will learn how to draw a small multiple histograms (also known as trellis plot) by using ggarrange() of ggpubr package.\nThe code chunk below is used to create 12 histograms. Then, ggarrange() is used to organised these histogram into a 3 columns by 4 rows small multiple plot.\n\nAREA_SQM &lt;- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nAGE &lt;- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CBD &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CHILDCARE &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_ELDERLYCARE &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_URA_GROWTH_AREA &lt;- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_URA_GROWTH_AREA`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_HAWKER_MARKET &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_KINDERGARTEN &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_MRT &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PARK &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PRIMARY_SCH &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_TOP_PRIMARY_SCH &lt;- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, \n          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,\n          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  \n          ncol = 3, nrow = 4)\n\n\n\n\n\n\n\n\n\n\n10.7.3 Drawing Statistical Point Map\nLastly, we want to reveal the geospatial distribution condominium resale prices in Singapore. The map will be prepared by using tmap package.\nFirst, we will turn on the interactive mode of tmap by using the code chunk below.\n\ntmap_mode(\"view\")\n\nNext, the code chunks below is used to create an interactive point symbol map.\n\ntm_shape(mpsz_svy21)+\n  tm_polygons() +\ntm_shape(condo_resale.sf) +  \n  tm_dots(col = \"SELLING_PRICE\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\nNotice that tm_dots() is used instead of tm_bubbles().\nset.zoom.limits argument of tm_view() sets the minimum and maximum zoom level to 11 and 14 respectively.\nBefore moving on to the next section, the code below will be used to turn R display into plot mode.\n\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#hedonic-pricing-modelling-in-r",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#hedonic-pricing-modelling-in-r",
    "title": "Hands-On Exercise 10: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "10.8 Hedonic Pricing Modelling in R",
    "text": "10.8 Hedonic Pricing Modelling in R\nIn this section, you will learn how to building hedonic pricing models for condominium resale units using lm() of R base.\n\n10.8.1 Simple Linear Regression Method\nFirst, we will build a simple linear regression model by using SELLING_PRICE as the dependent variable and AREA_SQM as the independent variable.\n\ncondo.slr &lt;- lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nlm() returns an object of class “lm” or for multiple responses of class c(“mlm”, “lm”).\nThe functions summary() and anova() can be used to obtain and print a summary and analysis of variance table of the results. The generic accessor functions coefficients, effects, fitted.values and residuals extract various useful features of the value returned by lm.\n\nsummary(condo.slr)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3695815  -391764   -87517   258900 13503875 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -258121.1    63517.2  -4.064 5.09e-05 ***\nAREA_SQM      14719.0      428.1  34.381  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 942700 on 1434 degrees of freedom\nMultiple R-squared:  0.4518,    Adjusted R-squared:  0.4515 \nF-statistic:  1182 on 1 and 1434 DF,  p-value: &lt; 2.2e-16\n\n\nThe output report reveals that the SELLING_PRICE can be explained by using the formula:\n      *y = -258121.1 + 14719x1*\nThe R-squared of 0.4518 reveals that the simple regression model built is able to explain about 45% of the resale prices.\nSince p-value is much smaller than 0.0001, we will reject the null hypothesis that mean is a good estimator of SELLING_PRICE. This will allow us to infer that simple linear regression model above is a good estimator of SELLING_PRICE.\nThe Coefficients: section of the report reveals that the p-values of both the estimates of the Intercept and ARA_SQM are smaller than 0.001. In view of this, the null hypothesis of the B0 and B1 are equal to 0 will be rejected. As a results, we will be able to infer that the B0 and B1 are good parameter estimates.\nTo visualise the best fit curve on a scatterplot, we can incorporate lm() as a method function in ggplot’s geometry as shown in the code chunk below.\n\nggplot(data=condo_resale.sf,  \n       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n\n\n\n\n\n\nFigure above reveals that there are a few statistical outliers with relatively high selling prices.\n\n\n10.8.2 Multiple Linear Regression Method\n\n10.8.2.1 Visualising the relationships of the independent variables\nBefore building a multiple regression model, it is important to ensure that the indepdent variables used are not highly correlated to each other. If these highly correlated independent variables are used in building a regression model by mistake, the quality of the model will be compromised. This phenomenon is known as multicollinearity in statistics.\nCorrelation matrix is commonly used to visualise the relationships between the independent variables. Beside the pairs() of R, there are many packages support the display of a correlation matrix. In this section, the corrplot package will be used.\nThe code chunk below is used to plot a scatterplot matrix of the relationship between the independent variables in condo_resale data.frame.\n\ncorrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = \"AOE\",\n         tl.pos = \"td\", tl.cex = 0.5, method = \"number\", type = \"upper\")\n\n\n\n\n\n\n\n\nMatrix reorder is very important for mining the hiden structure and patter in the matrix. There are four methods in corrplot (parameter order), named “AOE”, “FPC”, “hclust”, “alphabet”. In the code chunk above, AOE order is used. It orders the variables by using the angular order of the eigenvectors method suggested by Michael Friendly.\nFrom the scatterplot matrix, it is clear that Freehold is highly correlated to LEASE_99YEAR. In view of this, it is wiser to only include either one of them in the subsequent model building. As a result, LEASE_99YEAR is excluded in the subsequent model building.\n\n\n\n10.8.3 Building a hedonic pricing model using multiple linear regression method\nThe code chunk below using lm() to calibrate the multiple linear regression model.\n\ncondo.mlr &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + \n                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + \n                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + \n                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                data=condo_resale.sf)\nsummary(condo.mlr)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + \n    PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + \n    PROX_KINDERGARTEN + PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + \n    PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3475964  -293923   -23069   241043 12260381 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           481728.40  121441.01   3.967 7.65e-05 ***\nAREA_SQM               12708.32     369.59  34.385  &lt; 2e-16 ***\nAGE                   -24440.82    2763.16  -8.845  &lt; 2e-16 ***\nPROX_CBD              -78669.78    6768.97 -11.622  &lt; 2e-16 ***\nPROX_CHILDCARE       -351617.91  109467.25  -3.212  0.00135 ** \nPROX_ELDERLYCARE      171029.42   42110.51   4.061 5.14e-05 ***\nPROX_URA_GROWTH_AREA   38474.53   12523.57   3.072  0.00217 ** \nPROX_HAWKER_MARKET     23746.10   29299.76   0.810  0.41782    \nPROX_KINDERGARTEN     147468.99   82668.87   1.784  0.07466 .  \nPROX_MRT             -314599.68   57947.44  -5.429 6.66e-08 ***\nPROX_PARK             563280.50   66551.68   8.464  &lt; 2e-16 ***\nPROX_PRIMARY_SCH      180186.08   65237.95   2.762  0.00582 ** \nPROX_TOP_PRIMARY_SCH    2280.04   20410.43   0.112  0.91107    \nPROX_SHOPPING_MALL   -206604.06   42840.60  -4.823 1.57e-06 ***\nPROX_SUPERMARKET      -44991.80   77082.64  -0.584  0.55953    \nPROX_BUS_STOP         683121.35  138353.28   4.938 8.85e-07 ***\nNO_Of_UNITS             -231.18      89.03  -2.597  0.00951 ** \nFAMILY_FRIENDLY       140340.77   47020.55   2.985  0.00289 ** \nFREEHOLD              359913.01   49220.22   7.312 4.38e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 755800 on 1417 degrees of freedom\nMultiple R-squared:  0.6518,    Adjusted R-squared:  0.6474 \nF-statistic: 147.4 on 18 and 1417 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n10.8.4 Preparing Publication Quality Table: olsrr method\nWith reference to the report above, it is clear that not all the independent variables are statistically significant. We will revised the model by removing those variables which are not statistically significant.\nNow, we are ready to calibrate the revised model by using the code chunk below.\n\ncondo.mlr1 &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + \n                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP + \n                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n                 data=condo_resale.sf)\nols_regress(condo.mlr1)\n\n                                Model Summary                                 \n-----------------------------------------------------------------------------\nR                            0.807       RMSE                     751998.679 \nR-Squared                    0.651       MSE                571471422208.592 \nAdj. R-Squared               0.647       Coef. Var                    43.168 \nPred R-Squared               0.638       AIC                       42966.758 \nMAE                     414819.628       SBC                       43051.072 \n-----------------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                     ANOVA                                       \n--------------------------------------------------------------------------------\n                    Sum of                                                      \n                   Squares          DF         Mean Square       F         Sig. \n--------------------------------------------------------------------------------\nRegression    1.512586e+15          14        1.080418e+14    189.059    0.0000 \nResidual      8.120609e+14        1421    571471422208.592                      \nTotal         2.324647e+15        1435                                          \n--------------------------------------------------------------------------------\n\n                                               Parameter Estimates                                                \n-----------------------------------------------------------------------------------------------------------------\n               model           Beta    Std. Error    Std. Beta       t        Sig           lower          upper \n-----------------------------------------------------------------------------------------------------------------\n         (Intercept)     527633.222    108183.223                   4.877    0.000     315417.244     739849.200 \n            AREA_SQM      12777.523       367.479        0.584     34.771    0.000      12056.663      13498.382 \n                 AGE     -24687.739      2754.845       -0.167     -8.962    0.000     -30091.739     -19283.740 \n            PROX_CBD     -77131.323      5763.125       -0.263    -13.384    0.000     -88436.469     -65826.176 \n      PROX_CHILDCARE    -318472.751    107959.512       -0.084     -2.950    0.003    -530249.889    -106695.613 \n    PROX_ELDERLYCARE     185575.623     39901.864        0.090      4.651    0.000     107302.737     263848.510 \nPROX_URA_GROWTH_AREA      39163.254     11754.829        0.060      3.332    0.001      16104.571      62221.936 \n            PROX_MRT    -294745.107     56916.367       -0.112     -5.179    0.000    -406394.234    -183095.980 \n           PROX_PARK     570504.807     65507.029        0.150      8.709    0.000     442003.938     699005.677 \n    PROX_PRIMARY_SCH     159856.136     60234.599        0.062      2.654    0.008      41697.849     278014.424 \n  PROX_SHOPPING_MALL    -220947.251     36561.832       -0.115     -6.043    0.000    -292668.213    -149226.288 \n       PROX_BUS_STOP     682482.221    134513.243        0.134      5.074    0.000     418616.359     946348.082 \n         NO_Of_UNITS       -245.480        87.947       -0.053     -2.791    0.005       -418.000        -72.961 \n     FAMILY_FRIENDLY     146307.576     46893.021        0.057      3.120    0.002      54320.593     238294.560 \n            FREEHOLD     350599.812     48506.485        0.136      7.228    0.000     255447.802     445751.821 \n-----------------------------------------------------------------------------------------------------------------\n\n\n\n\n10.8.5 Preparing Publication Quality Table: gtsummary method\nThe gtsummary package provides an elegant and flexible way to create publication-ready summary tables in R.\nIn the code chunk below, tbl_regression() is used to create a well formatted regression report.\n\ntbl_regression(condo.mlr1, intercept = TRUE)\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n527,633\n315,417, 739,849\n&lt;0.001\n    AREA_SQM\n12,778\n12,057, 13,498\n&lt;0.001\n    AGE\n-24,688\n-30,092, -19,284\n&lt;0.001\n    PROX_CBD\n-77,131\n-88,436, -65,826\n&lt;0.001\n    PROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n    PROX_ELDERLYCARE\n185,576\n107,303, 263,849\n&lt;0.001\n    PROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n&lt;0.001\n    PROX_MRT\n-294,745\n-406,394, -183,096\n&lt;0.001\n    PROX_PARK\n570,505\n442,004, 699,006\n&lt;0.001\n    PROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n    PROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n&lt;0.001\n    PROX_BUS_STOP\n682,482\n418,616, 946,348\n&lt;0.001\n    NO_Of_UNITS\n-245\n-418, -73\n0.005\n    FAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n    FREEHOLD\n350,600\n255,448, 445,752\n&lt;0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nWith gtsummary package, model statistics can be included in the report by either appending them to the report table by using add_glance_table() or adding as a table source note by using add_glance_source_note() as shown in the code chunk below.\n\ntbl_regression(condo.mlr1, \n               intercept = TRUE) %&gt;% \n  add_glance_source_note(\n    label = list(sigma ~ \"\\U03C3\"),\n    include = c(r.squared, adj.r.squared, \n                AIC, statistic,\n                p.value, sigma))\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n527,633\n315,417, 739,849\n&lt;0.001\n    AREA_SQM\n12,778\n12,057, 13,498\n&lt;0.001\n    AGE\n-24,688\n-30,092, -19,284\n&lt;0.001\n    PROX_CBD\n-77,131\n-88,436, -65,826\n&lt;0.001\n    PROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n    PROX_ELDERLYCARE\n185,576\n107,303, 263,849\n&lt;0.001\n    PROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n&lt;0.001\n    PROX_MRT\n-294,745\n-406,394, -183,096\n&lt;0.001\n    PROX_PARK\n570,505\n442,004, 699,006\n&lt;0.001\n    PROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n    PROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n&lt;0.001\n    PROX_BUS_STOP\n682,482\n418,616, 946,348\n&lt;0.001\n    NO_Of_UNITS\n-245\n-418, -73\n0.005\n    FAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n    FREEHOLD\n350,600\n255,448, 445,752\n&lt;0.001\n  \n  \n    \n      R² = 0.651; Adjusted R² = 0.647; AIC = 42,967; Statistic = 189; p-value = &lt;0.001; σ = 755,957\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nFor more customisation options, refer to Tutorial: tbl_regression\n\n10.8.5.1 Checking for multicolinearity\nIn this section, we would like to introduce you a fantastic R package specially programmed for performing OLS regression. It is called olsrr. It provides a collection of very useful methods for building better multiple linear regression models:\n\ncomprehensive regression output\nresidual diagnostics\nmeasures of influence\nheteroskedasticity tests\ncollinearity diagnostics\nmodel fit assessment\nvariable contribution assessment\nvariable selection procedures\n\nIn the code chunk below, the ols_vif_tol() of olsrr package is used to test if there are sign of multicollinearity.\n\nols_vif_tol(condo.mlr1)\n\n              Variables Tolerance      VIF\n1              AREA_SQM 0.8728554 1.145665\n2                   AGE 0.7071275 1.414172\n3              PROX_CBD 0.6356147 1.573280\n4        PROX_CHILDCARE 0.3066019 3.261559\n5      PROX_ELDERLYCARE 0.6598479 1.515501\n6  PROX_URA_GROWTH_AREA 0.7510311 1.331503\n7              PROX_MRT 0.5236090 1.909822\n8             PROX_PARK 0.8279261 1.207837\n9      PROX_PRIMARY_SCH 0.4524628 2.210126\n10   PROX_SHOPPING_MALL 0.6738795 1.483945\n11        PROX_BUS_STOP 0.3514118 2.845664\n12          NO_Of_UNITS 0.6901036 1.449058\n13      FAMILY_FRIENDLY 0.7244157 1.380423\n14             FREEHOLD 0.6931163 1.442759\n\n\nSince the VIF of the independent variables are less than 10. We can safely conclude that there are no sign of multicollinearity among the independent variables.\n\n\n10.8.5.2 Test for Non-Linearity\nIn multiple linear regression, it is important for us to test the assumption that linearity and additivity of the relationship between dependent and independent variables.\nIn the code chunk below, the ols_plot_resid_fit() of olsrr package is used to perform linearity assumption test.\n\nols_plot_resid_fit(condo.mlr1)\n\n\n\n\n\n\n\n\nThe figure above reveals that most of the data poitns are scattered around the 0 line, hence we can safely conclude that the relationships between the dependent variable and independent variables are linear.\n\n\n10.8.5.3 Test for Normality Assumption\nLastly, the code chunk below uses ols_plot_resid_hist() of olsrr package to perform normality assumption test.\n\nols_plot_resid_hist(condo.mlr1)\n\n\n\n\n\n\n\n\nThe figure reveals that the residual of the multiple linear regression model (i.e. condo.mlr1) is resemble normal distribution.\nIf you prefer formal statistical test methods, the ols_test_normality() of olsrr package can be used as shown in the code chun below.\n\nols_test_normality(condo.mlr1)\n\n-----------------------------------------------\n       Test             Statistic       pvalue  \n-----------------------------------------------\nShapiro-Wilk              0.6856         0.0000 \nKolmogorov-Smirnov        0.1366         0.0000 \nCramer-von Mises         121.0768        0.0000 \nAnderson-Darling         67.9551         0.0000 \n-----------------------------------------------\n\n\nThe summary table above reveals that the p-values of the four tests are way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is statistical evidence that the residual are not normally distributed.\n\n\n10.8.5.4 Testing for Spatial Autocorrelation\nThe hedonic model we try to build are using geographically referenced attributes, hence it is also important for us to visual the residual of the hedonic pricing model.\nIn order to perform spatial autocorrelation test, we need to convert condo_resale.sf from sf data frame into a SpatialPointsDataFrame.\nFirst, we will export the residual of the hedonic pricing model and save it as a data frame.\n\nmlr.output &lt;- as.data.frame(condo.mlr1$residuals)\n\nNext, we will join the newly created data frame with condo_resale.sf object.\n\ncondo_resale.res.sf &lt;- cbind(condo_resale.sf, \n                        condo.mlr1$residuals) %&gt;%\nrename(`MLR_RES` = `condo.mlr1.residuals`)\n\nNext, we will convert condo_resale.res.sf from simple feature object into a SpatialPointsDataFrame because spdep package can only process sp conformed spatial data objects.\nThe code chunk below will be used to perform the data conversion process.\n\ncondo_resale.sp &lt;- as_Spatial(condo_resale.res.sf)\ncondo_resale.sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1436 \nextent      : 14940.85, 43352.45, 24765.67, 48382.81  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 23\nnames       : POSTCODE, SELLING_PRICE, AREA_SQM, AGE,    PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN,    PROX_MRT,   PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH, PROX_SHOPPING_MALL, ... \nmin values  :    18965,        540000,       34,   0, 0.386916393,    0.004927023,      0.054508623,          0.214539508,        0.051817113,       0.004927023, 0.052779424, 0.029064164,      0.077106132,          0.077106132,                  0, ... \nmax values  :   828833,       1.8e+07,      619,  37, 19.18042832,     3.46572633,      3.949157205,           9.15540001,        5.374348075,       2.229045366,  3.48037319,  2.16104919,      3.928989144,          6.748192062,        3.477433767, ... \n\n\nNext, we will use tmap package to display the distribution of the residuals on an interactive map.\nThe code churn below will turn on the interactive mode of tmap.\n\ntmap_mode(\"view\")\n\nThe code chunks below is used to create an interactive point symbol map.\n\ntm_shape(mpsz_svy21)+\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale.res.sf) +  \n  tm_dots(col = \"MLR_RES\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\nRemember to switch back to “plot” mode before continue.\n\ntmap_mode(\"plot\")\n\nThe figure above reveal that there is sign of spatial autocorrelation.\nTo proof that our observation is indeed true, the Moran’s I test will be performed\nFirst, we will compute the distance-based weight matrix by using dnearneigh() function of spdep.\n\nnb &lt;- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)\nsummary(nb)\n\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \n10 disjoint connected subgraphs\nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\n\nNext, nb2listw() of spdep packge will be used to convert the output neighbours lists (i.e. nb) into a spatial weights.\n\nnb_lw &lt;- nb2listw(nb, style = 'W')\nsummary(nb_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \n10 disjoint connected subgraphs\nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\nWeights style: W \nWeights constants summary:\n     n      nn   S0       S1       S2\nW 1436 2062096 1436 94.81916 5798.341\n\n\nNext, lm.morantest() of spdep package will be used to perform Moran’s I test for residual spatial autocorrelation\n\nlm.morantest(condo.mlr1, nb_lw)\n\n\n    Global Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD +\nPROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_MRT +\nPROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP +\nNO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data = condo_resale.sf)\nweights: nb_lw\n\nMoran I statistic standard deviate = 24.366, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nObserved Moran I      Expectation         Variance \n    1.438876e-01    -5.487594e-03     3.758259e-05 \n\n\nThe Global Moran’s I test for residual spatial autocorrelation shows that it’s p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed.\nSince the Observed Global Moran I = 0.1424418 which is greater than 0, we can infer than the residuals resemble cluster distribution."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#building-hedonic-pricing-models-using-gwmodel",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#building-hedonic-pricing-models-using-gwmodel",
    "title": "Hands-On Exercise 10: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "10.9 Building Hedonic Pricing Models using GWmodel",
    "text": "10.9 Building Hedonic Pricing Models using GWmodel\nIn this section, you are going to learn how to modelling hedonic pricing using both the fixed and adaptive bandwidth schemes\n\n10.9.1 Building Fixed Bandwidth GWR Model\n\n10.9.1.1 Computing fixed bandwith\nIn the code chunk below bw.gwr() of GWModel package is used to determine the optimal fixed bandwidth to use in the model. Notice that the argument adaptive is set to FALSE indicates that we are interested to compute the fixed bandwidth.\nThere are two possible approaches can be uused to determine the stopping rule, they are: CV cross-validation approach and AIC corrected (AICc) approach. We define the stopping rule using approach argeement.\n\nbw.fixed &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                     FAMILY_FRIENDLY + FREEHOLD, \n                   data=condo_resale.sp, \n                   approach=\"CV\", \n                   kernel=\"gaussian\", \n                   adaptive=FALSE, \n                   longlat=FALSE)\n\nFixed bandwidth: 17660.96 CV score: 8.259118e+14 \nFixed bandwidth: 10917.26 CV score: 7.970454e+14 \nFixed bandwidth: 6749.419 CV score: 7.273273e+14 \nFixed bandwidth: 4173.553 CV score: 6.300006e+14 \nFixed bandwidth: 2581.58 CV score: 5.404958e+14 \nFixed bandwidth: 1597.687 CV score: 4.857515e+14 \nFixed bandwidth: 989.6077 CV score: 4.722431e+14 \nFixed bandwidth: 613.7939 CV score: 1.379526e+16 \nFixed bandwidth: 1221.873 CV score: 4.778717e+14 \nFixed bandwidth: 846.0596 CV score: 4.791629e+14 \nFixed bandwidth: 1078.325 CV score: 4.751406e+14 \nFixed bandwidth: 934.7772 CV score: 4.72518e+14 \nFixed bandwidth: 1023.495 CV score: 4.730305e+14 \nFixed bandwidth: 968.6643 CV score: 4.721317e+14 \nFixed bandwidth: 955.7206 CV score: 4.722072e+14 \nFixed bandwidth: 976.6639 CV score: 4.721387e+14 \nFixed bandwidth: 963.7202 CV score: 4.721484e+14 \nFixed bandwidth: 971.7199 CV score: 4.721293e+14 \nFixed bandwidth: 973.6083 CV score: 4.721309e+14 \nFixed bandwidth: 970.5527 CV score: 4.721295e+14 \nFixed bandwidth: 972.4412 CV score: 4.721296e+14 \nFixed bandwidth: 971.2741 CV score: 4.721292e+14 \nFixed bandwidth: 970.9985 CV score: 4.721293e+14 \nFixed bandwidth: 971.4443 CV score: 4.721292e+14 \nFixed bandwidth: 971.5496 CV score: 4.721293e+14 \nFixed bandwidth: 971.3793 CV score: 4.721292e+14 \nFixed bandwidth: 971.3391 CV score: 4.721292e+14 \nFixed bandwidth: 971.3143 CV score: 4.721292e+14 \nFixed bandwidth: 971.3545 CV score: 4.721292e+14 \nFixed bandwidth: 971.3296 CV score: 4.721292e+14 \nFixed bandwidth: 971.345 CV score: 4.721292e+14 \nFixed bandwidth: 971.3355 CV score: 4.721292e+14 \nFixed bandwidth: 971.3413 CV score: 4.721292e+14 \nFixed bandwidth: 971.3377 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3396 CV score: 4.721292e+14 \nFixed bandwidth: 971.3402 CV score: 4.721292e+14 \nFixed bandwidth: 971.3398 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3399 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \n\n\nThe result shows that the recommended bandwidth is 971.3405 metres. (Quiz: Do you know why it is in metre?)\n\n\n10.9.1.2 GWModel method - fixed bandwith\nNow we can use the code chunk below to calibrate the gwr model using fixed bandwidth and gaussian kernel.\n\ngwr.fixed &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                         PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                         PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                         PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                         FAMILY_FRIENDLY + FREEHOLD, \n                       data=condo_resale.sp, \n                       bw=bw.fixed, \n                       kernel = 'gaussian', \n                       longlat = FALSE)\n\nThe output is saved in a list of class “gwrm”. The code below can be used to display the model output.\n\ngwr.fixed\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-10-26 18:31:11.979187 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.fixed, kernel = \"gaussian\", \n    longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 971.34 \n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -3.5988e+07 -5.1998e+05  7.6780e+05  1.7412e+06\n   AREA_SQM              1.0003e+03  5.2758e+03  7.4740e+03  1.2301e+04\n   AGE                  -1.3475e+05 -2.0813e+04 -8.6260e+03 -3.7784e+03\n   PROX_CBD             -7.7047e+07 -2.3608e+05 -8.3599e+04  3.4646e+04\n   PROX_CHILDCARE       -6.0097e+06 -3.3667e+05 -9.7426e+04  2.9007e+05\n   PROX_ELDERLYCARE     -3.5001e+06 -1.5970e+05  3.1970e+04  1.9577e+05\n   PROX_URA_GROWTH_AREA -3.0170e+06 -8.2013e+04  7.0749e+04  2.2612e+05\n   PROX_MRT             -3.5282e+06 -6.5836e+05 -1.8833e+05  3.6922e+04\n   PROX_PARK            -1.2062e+06 -2.1732e+05  3.5383e+04  4.1335e+05\n   PROX_PRIMARY_SCH     -2.2695e+07 -1.7066e+05  4.8472e+04  5.1555e+05\n   PROX_SHOPPING_MALL   -7.2585e+06 -1.6684e+05 -1.0517e+04  1.5923e+05\n   PROX_BUS_STOP        -1.4676e+06 -4.5207e+04  3.7601e+05  1.1664e+06\n   NO_Of_UNITS          -1.3170e+03 -2.4822e+02 -3.0846e+01  2.5496e+02\n   FAMILY_FRIENDLY      -2.2749e+06 -1.1140e+05  7.6214e+03  1.6107e+05\n   FREEHOLD             -9.2067e+06  3.8074e+04  1.5169e+05  3.7528e+05\n                             Max.\n   Intercept            112794435\n   AREA_SQM                 21575\n   AGE                     434203\n   PROX_CBD               2704604\n   PROX_CHILDCARE         1654086\n   PROX_ELDERLYCARE      38867861\n   PROX_URA_GROWTH_AREA  78515805\n   PROX_MRT               3124325\n   PROX_PARK             18122439\n   PROX_PRIMARY_SCH       4637517\n   PROX_SHOPPING_MALL     1529953\n   PROX_BUS_STOP         11342209\n   NO_Of_UNITS              12907\n   FAMILY_FRIENDLY        1720745\n   FREEHOLD               6073642\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 438.3807 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 997.6193 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 42263.61 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41632.36 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 42515.71 \n   Residual sum of squares: 2.534069e+14 \n   R-square value:  0.8909912 \n   Adjusted R-square value:  0.8430418 \n\n   ***********************************************************************\n   Program stops at: 2024-10-26 18:31:12.777562 \n\n\nThe report shows that the AICc of the gwr is 42263.61 which is significantly smaller than the globel multiple linear regression model of 42967.1.\n\n\n\n10.9.2 Building Adaptive Bandwidth GWR Model\nIn this section, we will calibrate the gwr-based hedonic pricing model by using adaptive bandwidth approach.\n\n10.9.2.1 Computing the adaptive bandwidth\nSimilar to the earlier section, we will first use bw.gwr() to determine the recommended data point to use.\nThe code chunk used look very similar to the one used to compute the fixed bandwidth except the adaptive argument has changed to TRUE.\n\nbw.adaptive &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE  + \n                        PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE    + \n                        PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                        PROX_PRIMARY_SCH + PROX_SHOPPING_MALL   + PROX_BUS_STOP + \n                        NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                      data=condo_resale.sp, \n                      approach=\"CV\", \n                      kernel=\"gaussian\", \n                      adaptive=TRUE, \n                      longlat=FALSE)\n\nAdaptive bandwidth: 895 CV score: 7.952401e+14 \nAdaptive bandwidth: 561 CV score: 7.667364e+14 \nAdaptive bandwidth: 354 CV score: 6.953454e+14 \nAdaptive bandwidth: 226 CV score: 6.15223e+14 \nAdaptive bandwidth: 147 CV score: 5.674373e+14 \nAdaptive bandwidth: 98 CV score: 5.426745e+14 \nAdaptive bandwidth: 68 CV score: 5.168117e+14 \nAdaptive bandwidth: 49 CV score: 4.859631e+14 \nAdaptive bandwidth: 37 CV score: 4.646518e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \nAdaptive bandwidth: 25 CV score: 4.430816e+14 \nAdaptive bandwidth: 32 CV score: 4.505602e+14 \nAdaptive bandwidth: 27 CV score: 4.462172e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \n\n\nThe result shows that the 30 is the recommended data points to be used.\n\n\n10.9.2.2 Constructing the adaptive bandwidth gwr model\nNow, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and gaussian kernel as shown in the code chunk below.\n\ngwr.adaptive &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + \n                            PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + \n                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                          data=condo_resale.sp, bw=bw.adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE, \n                          longlat = FALSE)\n\nThe code below can be used to display the model output.\n\ngwr.adaptive\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-10-26 18:31:19.215585 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.adaptive, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 30 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -1.3487e+08 -2.4669e+05  7.7928e+05  1.6194e+06\n   AREA_SQM              3.3188e+03  5.6285e+03  7.7825e+03  1.2738e+04\n   AGE                  -9.6746e+04 -2.9288e+04 -1.4043e+04 -5.6119e+03\n   PROX_CBD             -2.5330e+06 -1.6256e+05 -7.7242e+04  2.6624e+03\n   PROX_CHILDCARE       -1.2790e+06 -2.0175e+05  8.7158e+03  3.7778e+05\n   PROX_ELDERLYCARE     -1.6212e+06 -9.2050e+04  6.1029e+04  2.8184e+05\n   PROX_URA_GROWTH_AREA -7.2686e+06 -3.0350e+04  4.5869e+04  2.4613e+05\n   PROX_MRT             -4.3781e+07 -6.7282e+05 -2.2115e+05 -7.4593e+04\n   PROX_PARK            -2.9020e+06 -1.6782e+05  1.1601e+05  4.6572e+05\n   PROX_PRIMARY_SCH     -8.6418e+05 -1.6627e+05 -7.7853e+03  4.3222e+05\n   PROX_SHOPPING_MALL   -1.8272e+06 -1.3175e+05 -1.4049e+04  1.3799e+05\n   PROX_BUS_STOP        -2.0579e+06 -7.1461e+04  4.1104e+05  1.2071e+06\n   NO_Of_UNITS          -2.1993e+03 -2.3685e+02 -3.4699e+01  1.1657e+02\n   FAMILY_FRIENDLY      -5.9879e+05 -5.0927e+04  2.6173e+04  2.2481e+05\n   FREEHOLD             -1.6340e+05  4.0765e+04  1.9023e+05  3.7960e+05\n                            Max.\n   Intercept            18758355\n   AREA_SQM                23064\n   AGE                     13303\n   PROX_CBD             11346650\n   PROX_CHILDCARE        2892127\n   PROX_ELDERLYCARE      2465671\n   PROX_URA_GROWTH_AREA  7384059\n   PROX_MRT              1186242\n   PROX_PARK             2588497\n   PROX_PRIMARY_SCH      3381462\n   PROX_SHOPPING_MALL   38038564\n   PROX_BUS_STOP        12081592\n   NO_Of_UNITS              1010\n   FAMILY_FRIENDLY       2072414\n   FREEHOLD              1813995\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 350.3088 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 1085.691 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 41982.22 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41546.74 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 41914.08 \n   Residual sum of squares: 2.528227e+14 \n   R-square value:  0.8912425 \n   Adjusted R-square value:  0.8561185 \n\n   ***********************************************************************\n   Program stops at: 2024-10-26 18:31:20.11984 \n\n\nThe report shows that the AICc the adaptive distance gwr is 41982.22 which is even smaller than the AICc of the fixed distance gwr of 42263.61.\n\n\n\n10.9.3 Visualising GWR Output\nIn addition to regression residuals, the output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors:\n\nCondition Number: this diagnostic evaluates local collinearity. In the presence of strong local collinearity, results become unstable. Results associated with condition numbers larger than 30, may be unreliable.\nLocal R2: these values range between 0.0 and 1.0 and indicate how well the local regression model fits observed y values. Very low values indicate the local model is performing poorly. Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing from the regression model.\nPredicted: these are the estimated (or fitted) y values 3. computed by GWR.\nResiduals: to obtain the residual values, the fitted y values are subtracted from the observed y values. Standardized residuals have a mean of zero and a standard deviation of 1. A cold-to-hot rendered map of standardized residuals can be produce by using these values.\nCoefficient Standard Error: these values measure the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.\n\nThey are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its “data” slot in an object called SDF of the output list.\n\n\n10.9.4 Converting SDF into sf data.frame\nTo visualise the fields in SDF, we need to first covert it into sf data.frame by using the code chunk below.\n\ncondo_resale.sf.adaptive &lt;- st_as_sf(gwr.adaptive$SDF) %&gt;%\n  st_transform(crs=3414)\n\n\ncondo_resale.sf.adaptive.svy21 &lt;- st_transform(condo_resale.sf.adaptive, 3414)\ncondo_resale.sf.adaptive.svy21  \n\n\ngwr.adaptive.output &lt;- as.data.frame(gwr.adaptive$SDF)\ncondo_resale.sf.adaptive &lt;- cbind(condo_resale.res.sf, as.matrix(gwr.adaptive.output))\n\nNext, glimpse() is used to display the content of condo_resale.sf.adaptive sf data frame.\n\nglimpse(condo_resale.sf.adaptive)\n\nRows: 1,436\nColumns: 52\n$ Intercept               &lt;dbl&gt; 2050011.67, 1633128.24, 3433608.17, 234358.91,…\n$ AREA_SQM                &lt;dbl&gt; 9561.892, 16576.853, 13091.861, 20730.601, 672…\n$ AGE                     &lt;dbl&gt; -9514.634, -58185.479, -26707.386, -93308.988,…\n$ PROX_CBD                &lt;dbl&gt; -120681.94, -149434.22, -259397.77, 2426853.66…\n$ PROX_CHILDCARE          &lt;dbl&gt; 319266.925, 441102.177, -120116.816, 480825.28…\n$ PROX_ELDERLYCARE        &lt;dbl&gt; -393417.795, 325188.741, 535855.806, 314783.72…\n$ PROX_URA_GROWTH_AREA    &lt;dbl&gt; -159980.203, -142290.389, -253621.206, -267929…\n$ PROX_MRT                &lt;dbl&gt; -299742.96, -2510522.23, -936853.28, -2039479.…\n$ PROX_PARK               &lt;dbl&gt; -172104.47, 523379.72, 209099.85, -759153.26, …\n$ PROX_PRIMARY_SCH        &lt;dbl&gt; 242668.03, 1106830.66, 571462.33, 3127477.21, …\n$ PROX_SHOPPING_MALL      &lt;dbl&gt; 300881.390, -87693.378, -126732.712, -29593.34…\n$ PROX_BUS_STOP           &lt;dbl&gt; 1210615.44, 1843587.22, 1411924.90, 7225577.51…\n$ NO_Of_UNITS             &lt;dbl&gt; 104.8290640, -288.3441183, -9.5532945, -161.35…\n$ FAMILY_FRIENDLY         &lt;dbl&gt; -9075.370, 310074.664, 5949.746, 1556178.531, …\n$ FREEHOLD                &lt;dbl&gt; 303955.61, 396221.27, 168821.75, 1212515.58, 3…\n$ y                       &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ yhat                    &lt;dbl&gt; 2886531.8, 3466801.5, 3616527.2, 5435481.6, 13…\n$ residual                &lt;dbl&gt; 113468.16, 413198.52, -291527.20, -1185481.63,…\n$ CV_Score                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Stud_residual           &lt;dbl&gt; 0.38207013, 1.01433140, -0.83780678, -2.846146…\n$ Intercept_SE            &lt;dbl&gt; 516105.5, 488083.5, 963711.4, 444185.5, 211962…\n$ AREA_SQM_SE             &lt;dbl&gt; 823.2860, 825.2380, 988.2240, 617.4007, 1376.2…\n$ AGE_SE                  &lt;dbl&gt; 5889.782, 6226.916, 6510.236, 6010.511, 8180.3…\n$ PROX_CBD_SE             &lt;dbl&gt; 37411.22, 23615.06, 56103.77, 469337.41, 41064…\n$ PROX_CHILDCARE_SE       &lt;dbl&gt; 319111.1, 299705.3, 349128.5, 304965.2, 698720…\n$ PROX_ELDERLYCARE_SE     &lt;dbl&gt; 120633.34, 84546.69, 129687.07, 127150.69, 327…\n$ PROX_URA_GROWTH_AREA_SE &lt;dbl&gt; 56207.39, 76956.50, 95774.60, 470762.12, 47433…\n$ PROX_MRT_SE             &lt;dbl&gt; 185181.3, 281133.9, 275483.7, 279877.1, 363830…\n$ PROX_PARK_SE            &lt;dbl&gt; 205499.6, 229358.7, 314124.3, 227249.4, 364580…\n$ PROX_PRIMARY_SCH_SE     &lt;dbl&gt; 152400.7, 165150.7, 196662.6, 240878.9, 249087…\n$ PROX_SHOPPING_MALL_SE   &lt;dbl&gt; 109268.8, 98906.8, 119913.3, 177104.1, 301032.…\n$ PROX_BUS_STOP_SE        &lt;dbl&gt; 600668.6, 410222.1, 464156.7, 562810.8, 740922…\n$ NO_Of_UNITS_SE          &lt;dbl&gt; 218.1258, 208.9410, 210.9828, 361.7767, 299.50…\n$ FAMILY_FRIENDLY_SE      &lt;dbl&gt; 131474.73, 114989.07, 146607.22, 108726.62, 16…\n$ FREEHOLD_SE             &lt;dbl&gt; 115954.0, 130110.0, 141031.5, 138239.1, 210641…\n$ Intercept_TV            &lt;dbl&gt; 3.9720784, 3.3460017, 3.5629010, 0.5276150, 1.…\n$ AREA_SQM_TV             &lt;dbl&gt; 11.614302, 20.087361, 13.247868, 33.577223, 4.…\n$ AGE_TV                  &lt;dbl&gt; -1.6154474, -9.3441881, -4.1023685, -15.524301…\n$ PROX_CBD_TV             &lt;dbl&gt; -3.22582173, -6.32792021, -4.62353528, 5.17080…\n$ PROX_CHILDCARE_TV       &lt;dbl&gt; 1.000488185, 1.471786337, -0.344047555, 1.5766…\n$ PROX_ELDERLYCARE_TV     &lt;dbl&gt; -3.26126929, 3.84626245, 4.13191383, 2.4756745…\n$ PROX_URA_GROWTH_AREA_TV &lt;dbl&gt; -2.846248368, -1.848971738, -2.648105057, -5.6…\n$ PROX_MRT_TV             &lt;dbl&gt; -1.61864578, -8.92998600, -3.40075727, -7.2870…\n$ PROX_PARK_TV            &lt;dbl&gt; -0.83749312, 2.28192684, 0.66565951, -3.340617…\n$ PROX_PRIMARY_SCH_TV     &lt;dbl&gt; 1.59230221, 6.70194543, 2.90580089, 12.9836104…\n$ PROX_SHOPPING_MALL_TV   &lt;dbl&gt; 2.753588422, -0.886626400, -1.056869486, -0.16…\n$ PROX_BUS_STOP_TV        &lt;dbl&gt; 2.0154464, 4.4941192, 3.0419145, 12.8383775, 0…\n$ NO_Of_UNITS_TV          &lt;dbl&gt; 0.480589953, -1.380026395, -0.045279967, -0.44…\n$ FAMILY_FRIENDLY_TV      &lt;dbl&gt; -0.06902748, 2.69655779, 0.04058290, 14.312764…\n$ FREEHOLD_TV             &lt;dbl&gt; 2.6213469, 3.0452799, 1.1970499, 8.7711485, 1.…\n$ Local_R2                &lt;dbl&gt; 0.8846744, 0.8899773, 0.8947007, 0.9073605, 0.…\n$ geometry                &lt;POINT [m]&gt; POINT (22085.12 29951.54), POINT (25656.…\n\n\n\nsummary(gwr.adaptive$SDF$yhat)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  171347  1102001  1385528  1751842  1982307 13887901 \n\n\n\n\n10.9.5 Visualising local R2\nThe code chunks below is used to create an interactive point symbol map.\n\ntmap_mode(\"view\")\ntm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"Local_R2\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\n\n\n10.9.6 Visualising coefficient estimates\nThe code chunks below is used to create an interactive point symbol map.\n\ntmap_mode(\"view\")\nAREA_SQM_SE &lt;- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_SE\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\nAREA_SQM_TV &lt;- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_TV\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\ntmap_arrange(AREA_SQM_SE, AREA_SQM_TV, \n             asp=1, ncol=2,\n             sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\n\n10.9.6.1 By URA Plannign Region\n\ntm_shape(mpsz_svy21[mpsz_svy21$REGION_N==\"CENTRAL REGION\", ])+\n  tm_polygons()+\ntm_shape(condo_resale.sf.adaptive) + \n  tm_bubbles(col = \"Local_R2\",\n           size = 0.15,\n           border.col = \"gray60\",\n           border.lwd = 1)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html",
    "title": "Hands On Exercise 4: Spatio-Temporal Point Patterns Analysis",
    "section": "",
    "text": "A spatio-temporal point process (also called space-time or spatial-temporal point process) is a random collection of points, where each point represents the time and location of an event. Examples of events include incidence of disease, sightings or births of a species, or the occurrences of fires, earthquakes, lightning strikes, tsunamis, or volcanic eruptions. In this lesson, you will learn the basic concepts and methods of Spatio-temporal Point Patterns Analysis. You will also gain hands-on experience on using these methods to discover real-world point processes.\nThe specific questions we would like to answer are:\n\nare the locations of forest fire in Kepulauan Bangka Belitung spatial and spatio-temporally independent?\nif the answer is NO, where and when the observed forest fire locations tend to cluster?"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#overview",
    "title": "Hands On Exercise 4: Spatio-Temporal Point Patterns Analysis",
    "section": "",
    "text": "A spatio-temporal point process (also called space-time or spatial-temporal point process) is a random collection of points, where each point represents the time and location of an event. Examples of events include incidence of disease, sightings or births of a species, or the occurrences of fires, earthquakes, lightning strikes, tsunamis, or volcanic eruptions. In this lesson, you will learn the basic concepts and methods of Spatio-temporal Point Patterns Analysis. You will also gain hands-on experience on using these methods to discover real-world point processes.\nThe specific questions we would like to answer are:\n\nare the locations of forest fire in Kepulauan Bangka Belitung spatial and spatio-temporally independent?\nif the answer is NO, where and when the observed forest fire locations tend to cluster?"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#importing-the-packages",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#importing-the-packages",
    "title": "Hands On Exercise 4: Spatio-Temporal Point Patterns Analysis",
    "section": "4.1 Importing the Packages",
    "text": "4.1 Importing the Packages\nFor the purpose of this study, five R packages will be used. They are:\n\nrgdal for importing geospatial data in GIS file format such as shapefile into R and save them as Spatial*DataFrame,\nmaptools for converting Spatial* object into ppp object,\nraster for handling raster data in R,\nsparr provides functions to estimate fixed and adoptive kernel-smoothed spatial relative risk surfaces via the density-ratio method and perform subsequent inference. Fixed-bandwidth spatiotemporal density and relative risk estimation is also supported.\nspatstat for performing Spatial Point Patterns Analysis such as kcross, Lcross, etc., and\ntmap for producing cartographic quality thematic maps.\n\n\npacman::p_load(sf, raster, spatstat, sparr, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#the-data",
    "title": "Hands On Exercise 4: Spatio-Temporal Point Patterns Analysis",
    "section": "4.2 The Data",
    "text": "4.2 The Data\nFor the purpose of this exercise, two data sets will be used, they are:\n\nforestfires, a csv file provides locations of forest fire detected from the Moderate Resolution Imaging Spectroradiometer (MODIS) sensor data. The data are downloaded from Fire Information for Resource Management System. For the purpose of this exercise, only forest fires within Kepulauan Bangka Belitung will be used.\nKepulauan_Bangka_Belitung, an ESRI shapefile showing the sub-district (i.e. kelurahan) boundary of Kepulauan Bangka Belitung. The data set was downloaded from Indonesia Geospatial portal. The original data covers the whole Indonesia. For the purpose of this exercise, only sub-districts within Kepulauan Bangka Belitung are extracted.\n\n\n4.2.1 Importing Study Area\n\nkbb &lt;- st_read(dsn=\"data/rawdata/\", \n                   layer=\"Kepulauan_Bangka_Belitung\")\n\nReading layer `Kepulauan_Bangka_Belitung' from data source \n  `/Users/georgiaxng/georgiaxng/is415-handson/Hands-on_Ex/Hands-on_Ex04/data/rawdata' \n  using driver `ESRI Shapefile'\nSimple feature collection with 298 features and 27 fields\nGeometry type: POLYGON\nDimension:     XYZ\nBounding box:  xmin: 105.1085 ymin: -3.116593 xmax: 106.8488 ymax: -1.501603\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that uniquely the polygon in the geometry column when imported is of a Polygon Z type. This means that each polygon not only defines a 2D shape but also includes elevation data with a z-coordinate. This additional z-dimension allows for a more detailed representation of the polygon’s geometry, incorporating vertical information such as elevation or depth.\n\n\nThe below revised code chunk serves to do the following:\n\nGroup the boundaries up\nDrop the Z values\nTransform the coordinate system\n\n\nkbb_sf &lt;- st_read(dsn=\"data/rawdata/\", \n                   layer=\"Kepulauan_Bangka_Belitung\") %&gt;%\n  st_union()%&gt;%\n  st_zm(drop = TRUE, what = \"ZM\")%&gt;%\n  st_transform(32748)\n\nReading layer `Kepulauan_Bangka_Belitung' from data source \n  `/Users/georgiaxng/georgiaxng/is415-handson/Hands-on_Ex/Hands-on_Ex04/data/rawdata' \n  using driver `ESRI Shapefile'\nSimple feature collection with 298 features and 27 fields\nGeometry type: POLYGON\nDimension:     XYZ\nBounding box:  xmin: 105.1085 ymin: -3.116593 xmax: 106.8488 ymax: -1.501603\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\n\n4.2.2 Converting OWIN\nNext, as.owin() is used to convert kbb into an owin object, which is a spatial window or region of interest for point pattern analysis. Once converted to an owin object, we can use it with functions from spatial point pattern analysis packages, such as spatstat, to analyze point patterns within the defined boundary. It helps in setting up the spatial context for further analysis.\n\nkbb_owin &lt;- as.owin(kbb_sf)\nkbb_owin\n\nwindow: polygonal boundary\nenclosing rectangle: [512066.8, 705559.4] x [9655398, 9834006] units\n\n\nNext, class() is used to confirm if the output is indeed an owin object.\n\nclass(kbb_owin)\n\n[1] \"owin\"\n\n\n\n\n4.2.3 Importing and Preparing Forest Fire Data\nNext, we will import the forest fire data set into the R environment. The code reads forest fire data from a CSV file, converts it into an sf object using longitude and latitude coordinates, and then reprojects the spatial data from WGS84 to the UTM zone 48S coordinate system. This prepares the data for further spatial analysis in a projection appropriate for the region of interest.\n\nfire_sf &lt;- read_csv(\"data/rawdata/forestfires.csv\") %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"),\n           crs=4326)%&gt;%\n  st_transform(crs = 32748)\n\nBecause ppp object only accept numeric or character as mark. The code chunk below is used to convert data type of acq_date to numeric.\n\nfire_sf &lt;- fire_sf %&gt;%\n  mutate(DayofYear = yday(acq_date)) %&gt;%\n  mutate(Month_num = month(acq_date)) %&gt;%\n  mutate(Month_fac = month(acq_date, label= TRUE, abbr = FALSE))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualising-the-fire-points",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualising-the-fire-points",
    "title": "Hands On Exercise 4: Spatio-Temporal Point Patterns Analysis",
    "section": "4.3 Visualising The Fire Points",
    "text": "4.3 Visualising The Fire Points\n\n4.3.1 Overall Plot\n\ntm_shape(kbb_sf)+\n  tm_polygons() +\ntm_shape(fire_sf) +\n  tm_dots()\n\n\n\n\n\n\n\n\n\n\n4.3.2 Visualising Geographic Distribution Of Forest Fires By Month\n\ntm_shape(kbb_sf) +\n  tm_polygons()+\n  tm_shape(fire_sf)+\n  tm_dots(size= 0.1)+\n  tm_facets(by=\"Month_fac\", free.coords = FALSE, drop.units= TRUE)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#computing-stkde-by-month",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#computing-stkde-by-month",
    "title": "Hands On Exercise 4: Spatio-Temporal Point Patterns Analysis",
    "section": "4.4 Computing STKDE by Month",
    "text": "4.4 Computing STKDE by Month\n\n4.4.1 Extracting forest fires by month\nThe code chunk below is used to remove the unwanted fields from fire_sf sf data.frame. This is because as.ppp() only need the mark field and geometry field from the input sf data frame.\n\nfire_month &lt;- fire_sf %&gt;% \n  select(Month_num)\n\n\n\n4.4.2 Creating ppp\nThe code chunk below is used to derive a ppp object called fire_month from fire_month of data frame.\n\nfire_month_ppp &lt;- as.ppp(fire_month)\nfire_month_ppp\n\nMarked planar point pattern: 741 points\nmarks are numeric, of storage type  'double'\nwindow: rectangle = [521564.1, 695791] x [9658137, 9828767] units\n\n\nThe code chunk below is used to check the output is in the correct object class.\n\nsummary(fire_month_ppp)\n\nMarked planar point pattern:  741 points\nAverage intensity 2.49258e-08 points per square unit\n\nCoordinates are given to 10 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   8.000   9.000   8.579  10.000  12.000 \n\nWindow: rectangle = [521564.1, 695791] x [9658137, 9828767] units\n                    (174200 x 170600 units)\nWindow area = 29728200000 square units\n\n\nNext, we will check if there are duplicated point events by using the code chunk below.\n\nany(duplicated(fire_month_ppp))\n\n[1] FALSE\n\n\n\n\n4.4.3 Including Owin Object\nThe code chunk below is used to combine origin_am_ppp and am_owin objects into one.\n\nfire_month_owin &lt;- fire_month_ppp[kbb_owin]\nsummary(fire_month_owin)\n\nMarked planar point pattern:  741 points\nAverage intensity 6.424519e-08 points per square unit\n\nCoordinates are given to 10 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   8.000   9.000   8.579  10.000  12.000 \n\nWindow: polygonal boundary\n2 separate polygons (no holes)\n           vertices        area relative.area\npolygon 1     47493 11533600000      1.00e+00\npolygon 2       256      306427      2.66e-05\nenclosing rectangle: [512066.8, 705559.4] x [9655398, 9834006] units\n                     (193500 x 178600 units)\nWindow area = 11533900000 square units\nFraction of frame area: 0.334\n\n\n\nplot(fire_month_owin)\n\n\n\n\n\n\n\n\n\n\n4.4.4 Computing Spatio-Temporal KDE\nNext, spattemp.density() of sparr package is used to compute the STKDE.\n\nst_kde &lt;- spattemp.density(fire_month_owin)\nsummary(st_kde)\n\nSpatiotemporal Kernel Density Estimate\n\nBandwidths\n  h = 15102.47 (spatial)\n  lambda = 0.0304 (temporal)\n\nNo. of observations\n  741 \n\nSpatial bound\n  Type: polygonal\n  2D enclosure: [512066.8, 705559.4] x [9655398, 9834006]\n\nTemporal bound\n  [1, 12]\n\nEvaluation\n  128 x 128 x 12 trivariate lattice\n  Density range: [1.233458e-27, 8.202976e-10]\n\n\n\n\n4.4.5 Plotting the spatio-temporal KDE object\nIn the code chunk below, plot() of R base is used to the KDE for between July 2023 - December 2023.\n\ntims &lt;- c(7,8,9,10,11,12)\npar(mfcol=c(2,3))\nfor(i in tims){ \n  plot(st_kde, i, \n       override.par=FALSE, \n       fix.range=TRUE, \n       main=paste(\"KDE at month\",i))\n}"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#computing-stkde-by-day-of-year",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#computing-stkde-by-day-of-year",
    "title": "Hands On Exercise 4: Spatio-Temporal Point Patterns Analysis",
    "section": "4.5 Computing STKDE by Day of Year",
    "text": "4.5 Computing STKDE by Day of Year\nIn this section, you will learn how to computer the STKDE of forest fires by day of year.\n\n4.5.1 Creating ppp object\nIn the code chunk below, DayofYear field is included in the output ppp object.\n\nfire_yday_ppp &lt;- fire_sf %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\n\n\n4.5.2 Including Owin object\nNext, code chunk below is used to combine the ppp object and the owin object.\n\nfire_yday_owin &lt;- fire_yday_ppp[kbb_owin]\nsummary(fire_yday_owin)\n\nMarked planar point pattern:  741 points\nAverage intensity 6.424519e-08 points per square unit\n\nCoordinates are given to 10 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   10.0   213.0   258.0   245.9   287.0   352.0 \n\nWindow: polygonal boundary\n2 separate polygons (no holes)\n           vertices        area relative.area\npolygon 1     47493 11533600000      1.00e+00\npolygon 2       256      306427      2.66e-05\nenclosing rectangle: [512066.8, 705559.4] x [9655398, 9834006] units\n                     (193500 x 178600 units)\nWindow area = 11533900000 square units\nFraction of frame area: 0.334"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#section",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#section",
    "title": "Hands On Exercise 4: Spatio-Temporal Point Patterns Analysis",
    "section": "4.5.3",
    "text": "4.5.3\n\nkde_yday &lt;- spattemp.density(\n  fire_yday_owin)\nsummary(kde_yday)\n\nSpatiotemporal Kernel Density Estimate\n\nBandwidths\n  h = 15102.47 (spatial)\n  lambda = 6.3198 (temporal)\n\nNo. of observations\n  741 \n\nSpatial bound\n  Type: polygonal\n  2D enclosure: [512066.8, 705559.4] x [9655398, 9834006]\n\nTemporal bound\n  [10, 352]\n\nEvaluation\n  128 x 128 x 343 trivariate lattice\n  Density range: [3.959516e-27, 2.751287e-12]\n\n\n\nplot(kde_yday)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#computing-stkde-by-day-of-year-improved-method",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#computing-stkde-by-day-of-year-improved-method",
    "title": "Hands On Exercise 4: Spatio-Temporal Point Patterns Analysis",
    "section": "4.6 Computing STKDE by Day of Year: Improved method",
    "text": "4.6 Computing STKDE by Day of Year: Improved method\nOne of the nice function provides in sparr package is BOOT.spattemp(). It support bandwidth selection for standalone spatiotemporal density/intensity based on bootstrap estimation of the MISE, providing an isotropic scalar spatial bandwidth and a scalar temporal bandwidth.\nCode chunk below uses BOOT.spattemp() to determine both the spatial bandwidth and the scalar temporal bandwidth.\n\nset.seed(1234)\nBOOT.spattemp(fire_yday_owin) \n\nInitialising...Done.\nOptimising...\nh = 15102.47 \b; lambda = 16.84806 \nh = 16612.72 \b; lambda = 16.84806 \nh = 15102.47 \b; lambda = 1527.095 \nh = 15480.03 \b; lambda = 771.9715 \nh = 15668.81 \b; lambda = 394.4098 \nh = 15763.2 \b; lambda = 205.6289 \nh = 15810.4 \b; lambda = 111.2385 \nh = 15833.99 \b; lambda = 64.04328 \nh = 15845.79 \b; lambda = 40.44567 \nh = 15851.69 \b; lambda = 28.64687 \nh = 15863.49 \b; lambda = 5.049258 \nh = 15854.64 \b; lambda = 22.74746 \nh = 15860.54 \b; lambda = 10.94866 \nh = 15859.07 \b; lambda = 13.89836 \nh = 14348.82 \b; lambda = 13.89836 \nh = 13216.87 \b; lambda = 12.42351 \nh = 12460.27 \b; lambda = 15.37321 \nh = 10760.88 \b; lambda = 16.11064 \nh = 8875.282 \b; lambda = 11.68608 \nh = 10432.08 \b; lambda = 12.97658 \nh = 7976.084 \b; lambda = 16.66371 \nh = 9286.281 \b; lambda = 15.60366 \nh = 9615.08 \b; lambda = 18.73771 \nh = 9206.581 \b; lambda = 21.61828 \nh = 8140.483 \b; lambda = 18.23073 \nh = 8795.582 \b; lambda = 17.70071 \nh = 9124.381 \b; lambda = 20.83477 \nh = 9164.856 \b; lambda = 19.52699 \nh = 8345.358 \b; lambda = 18.48998 \nh = 9297.65 \b; lambda = 18.67578 \nh = 8928.375 \b; lambda = 16.8495 \nh = 9105.736 \b; lambda = 18.85762 \nDone.\n\n\n         h     lambda \n9105.73611   18.85762 \n\n\n\n4.6.1 Computing spatio-temporal KDE\nNow, the STKDE will be derived by using h and lambda values derive in previous step.\n\nkde_yday &lt;- spattemp.density(\n  fire_yday_owin,\n  h = 9000,\n  lambda = 19)\nsummary(kde_yday)\n\nSpatiotemporal Kernel Density Estimate\n\nBandwidths\n  h = 9000 (spatial)\n  lambda = 19 (temporal)\n\nNo. of observations\n  741 \n\nSpatial bound\n  Type: polygonal\n  2D enclosure: [512066.8, 705559.4] x [9655398, 9834006]\n\nTemporal bound\n  [10, 352]\n\nEvaluation\n  128 x 128 x 343 trivariate lattice\n  Density range: [2.001642e-19, 2.445724e-12]\n\n\n\n\n4.6.2 Plotting the output spatio-temporal KDE\nLast, plot() of sparr package is used to plot the output as shown below.\n\nplot(kde_yday)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "title": "Hands-On Exercise 2: Thematic Mapping and GeoVisualisation with R",
    "section": "",
    "text": "The main focus of the topic for these hands-on exercises are thematic/choropleth mapping and other geospatial visualization techniques.\nIn general, thematic mapping involves the use of map symbols to visualize selected properties of geographic features that are not naturally visible, such as population, temperature, crime rate, and property prices, just to mention a few of them.\nGeovisualisation, on the other hand, works by providing graphical ideation to render a place, a phenomenon or a process visible, enabling human’s most powerful information-processing abilities – those of spatial cognition associated with our eye–brain vision system – to be directly brought to bear.\nIn this exercise, we will explore how to plot functional and truthful choropleth maps by using tmap package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#overview",
    "title": "Hands-On Exercise 2: Thematic Mapping and GeoVisualisation with R",
    "section": "",
    "text": "The main focus of the topic for these hands-on exercises are thematic/choropleth mapping and other geospatial visualization techniques.\nIn general, thematic mapping involves the use of map symbols to visualize selected properties of geographic features that are not naturally visible, such as population, temperature, crime rate, and property prices, just to mention a few of them.\nGeovisualisation, on the other hand, works by providing graphical ideation to render a place, a phenomenon or a process visible, enabling human’s most powerful information-processing abilities – those of spatial cognition associated with our eye–brain vision system – to be directly brought to bear.\nIn this exercise, we will explore how to plot functional and truthful choropleth maps by using tmap package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-packages",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-packages",
    "title": "Hands-On Exercise 2: Thematic Mapping and GeoVisualisation with R",
    "section": "2.2 Importing Packages",
    "text": "2.2 Importing Packages\nBefore we start the exercise, we will need to import necessary R packages first. We will use the following packages:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\n\nAmong the four packages, readr, tidyr and dplyr are part of tidyverse package.\nThe code chunk below will be used to install and load these packages in RStudio.\n\npacman::p_load(sf, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-data-into-r",
    "title": "Hands-On Exercise 2: Thematic Mapping and GeoVisualisation with R",
    "section": "2.3 Importing Data into R",
    "text": "2.3 Importing Data into R\n\n2.3.1 Datasets\nTwo data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\n\n2.3.2 Importing Geospatial Data into R\nTo import the geospatial data, we will use st_read() function of sf package to import the MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\nmpsz &lt;- st_read(dsn = \"data/geospatial/MasterPlan2014SubzoneBoundaryWebSHP\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/georgiaxng/georgiaxng/is415-handson/Hands-on_Ex/Hands-on_Ex02/data/geospatial/MasterPlan2014SubzoneBoundaryWebSHP' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nTo inspect the content of mpsz, we can utilise the following code:\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\n2.3.3 Importing Attribute Data into R\nNext, we will import the respopagsex2011to2020.csv file into RStudio and save it into an R dataframe called popdata.\nThe task will be performed by using read_csv() function of readr package as shown in the code chunk below.\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\nWe can use the list() function that we have previously learnt in the previous data to examine if the data file has been imported correctly.\n\nlist(popdata)\n\n[[1]]\n# A tibble: 984,656 × 7\n   PA         SZ                     AG     Sex     TOD                Pop  Time\n   &lt;chr&gt;      &lt;chr&gt;                  &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;\n 1 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   HDB 1- and 2-Ro…     0  2011\n 2 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   HDB 3-Room Flats    10  2011\n 3 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   HDB 4-Room Flats    30  2011\n 4 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   HDB 5-Room and …    50  2011\n 5 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   HUDC Flats (exc…     0  2011\n 6 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   Landed Properti…     0  2011\n 7 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   Condominiums an…    40  2011\n 8 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Males   Others               0  2011\n 9 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Females HDB 1- and 2-Ro…     0  2011\n10 Ang Mo Kio Ang Mo Kio Town Centre 0_to_4 Females HDB 3-Room Flats    10  2011\n# ℹ 984,646 more rows\n\n\n\n\n2.3.4 Data Preparation\nBefore a thematic map can be prepared, we will need to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age group 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n2.3.4.1 Data Preprocessing\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup()%&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[14])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:13])+\nrowSums(.[15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\nThe above code does the following:\n\n\n\n\nFilter Data: Select rows for the year 2020.\nGroup Data: Group by PA, SZ, and AG.\nSummarize Population: Sum population (Pop) within each group.\nUngroup: Remove grouping structure.\nPivot to Wide Format: Transform age groups into separate columns.\nCalculate Age Group Totals:\n\nYOUNG: Sum selected columns for young age groups (columns 3-6 and 14).\nECONOMY ACTIVE: Sum columns for economically active groups (columns 7-13 and 15).\nAGED: Sum columns for older age groups (columns 16-21).\n\nCalculate Total Population: Sum all columns (3-21).\nCalculate Dependency Ratio: Ratio of non-working-age (YOUNG + AGED) to working-age (ECONOMY ACTIVE).\nSelect Final Columns: Output PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, and DEPENDENCY.\n\n\n\n\n\n\n2.3.4.2 Joining the attribute data and geospatial data\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = list(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\n\nConverting the values in PA and SZ fields to uppercase\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\nThing to learn from the code chunk above:\n\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame.\n\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-On Exercise 2: Thematic Mapping and GeoVisualisation with R",
    "section": "2.4 Choropleth Mapping Geospatial Data Using tmap",
    "text": "2.4 Choropleth Mapping Geospatial Data Using tmap\nChoropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n2.4.1 Plotting a choropleth map quickly by using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code chunk below will draw a cartographic standard choropleth map as shown below.\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n\n2.4.2 Creating a choropleth map by using tmap’s elements\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n2.4.2.1 Drawing a base map\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\n\n\n\n2.4.2.2 Drawing a choropleth map using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\nThings to learn from tm_polygons():\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3.\nThe default colour scheme used is YlOrRd of ColorBrewer. You will learn more about the color scheme in sub-section 4.4.\nBy default, Missing value will be shaded in grey.\n\n\n\n2.4.2.3 Drawing a choropleth map using tm_fill() and *tm_border()**\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependency values\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”.\n\n\n\n\n2.4.3 Data classification methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n2.4.3.1 Plotting choropleth maps with built-in classification methods\nThe code chunk below shows a quantile data classification that used 5 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nIn the code chunk below, equal data classification method is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nNotice that the distribution of quantile data classification method are more evenly distributed then equal data classification method.\n\nWarning: Maps Lie!\n\n\nDIY: Using what you had learned, prepare choropleth maps by using different classification methods supported by tmap and compare their differences.\n\n\nDIY: Preparing choropleth maps by using similar classification method but with different numbers of classes (i.e. 2, 6, 10, 20). Compare the output maps, what observation can you draw?\n\n\n\n2.4.3.2 Plotting choropleth map with custome break\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.6519  0.7025  0.7742  0.7645 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\nNow, we will plot the choropleth map by using the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n2.4.4 Colour Scheme\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n2.4.4.1 Using ColourBrewer palette\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nNotice that the choropleth map is shaded in green.\nTo reverse the colour shading, add a “-” prefix.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nNotice that the colour scheme has been reversed.\n\n\n\n2.4.5 Map Layouts\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n2.4.5.1 Map Legend\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n2.4.5.2 Map style\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\nThe code chunk below shows the classic style is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n\n\n\n\n2.4.5.3 Cartographic Furniture\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nTo reset the default style, refer to the code chunk below.\n\ntmap_style(\"white\")\n\n\n\n\n2.4.6 Drawing Small Multiple Choropleth Maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n2.4.6.1 By assigning multiple values to at least one of the aesthetic arguments\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\n2.4.6.2 By defining a group-by variable in tm_facets()\nIn this example, multiple small choropleth maps are created by using tm_facets()\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n2.4.6.3 By creating multiple stand-alone maps with tmap_arrange()\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n2.4.7 Mapping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#reference",
    "title": "Hands-On Exercise 2: Thematic Mapping and GeoVisualisation with R",
    "section": "2.5 Reference",
    "text": "2.5 Reference\n\n2.5.1 All about tmap package\n\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\n2.5.2 Geospatial data wrangling\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\n2.5.3 Data wrangling\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html",
    "title": "Hands-On Exercise 9: Geographical Segmentation with Spatially Constrained Clustering Techniques Part 2",
    "section": "",
    "text": "In this hands-on exercise, you will gain hands-on experience on how to delineate homogeneous region by using geographically referenced multivariate data. There are two major analysis, namely:\n\nhierarchical cluster analysis; and\nspatially constrained cluster analysis.\n\n\n\nBy the end of this hands-on exercise, you will able:\n\nto convert GIS polygon data into R’s simple feature data.frame by using appropriate functions of sf package of R;\nto convert simple feature data.frame into R’s SpatialPolygonDataFrame object by using appropriate sf of package of R;\nto perform custer analysis by using hclust() of Base R;\nto perform spatially constrained cluster analysis using skater() of Base R; and\nto visualise the analysis output by using ggplot2 and tmap package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#overview",
    "title": "Hands-On Exercise 9: Geographical Segmentation with Spatially Constrained Clustering Techniques Part 2",
    "section": "",
    "text": "In this hands-on exercise, you will gain hands-on experience on how to delineate homogeneous region by using geographically referenced multivariate data. There are two major analysis, namely:\n\nhierarchical cluster analysis; and\nspatially constrained cluster analysis.\n\n\n\nBy the end of this hands-on exercise, you will able:\n\nto convert GIS polygon data into R’s simple feature data.frame by using appropriate functions of sf package of R;\nto convert simple feature data.frame into R’s SpatialPolygonDataFrame object by using appropriate sf of package of R;\nto perform custer analysis by using hclust() of Base R;\nto perform spatially constrained cluster analysis using skater() of Base R; and\nto visualise the analysis output by using ggplot2 and tmap package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#getting-started",
    "title": "Hands-On Exercise 9: Geographical Segmentation with Spatially Constrained Clustering Techniques Part 2",
    "section": "9.2 Getting Started",
    "text": "9.2 Getting Started\n\n9.2.1 The analytical question\nIn geobusiness and spatial policy, it is a common practice to delineate the market or planning area into homogeneous regions by using multivariate data. In this hands-on exercise, we are interested to delineate Shan State, Myanmar into homogeneous regions by using multiple Information and Communication technology (ICT) measures, namely: Radio, Television, Land line phone, Mobile phone, Computer, and Internet at home."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#the-data",
    "title": "Hands-On Exercise 9: Geographical Segmentation with Spatially Constrained Clustering Techniques Part 2",
    "section": "9.3 The data",
    "text": "9.3 The data\nTwo data sets will be used in this study. They are:\n\nMyanmar Township Boundary Data (i.e. myanmar_township_boundaries) : This is a GIS data in ESRI shapefile format. It consists of township boundary information of Myanmar. The spatial data are captured in polygon features.\nShan-ICT.csv: This is an extract of The 2014 Myanmar Population and Housing Census Myanmar at the township level.\n\nBoth data sets are download from Myanmar Information Management Unit (MIMU)\n\n9.3.1 Installing and loading R packages\nBefore we get started, it is important for us to install the necessary R packages into R and launch these R packages into R environment.\nThe R packages needed for this exercise are as follows:\n\nSpatial data handling\n\nsf, rgdal and spdep\n\nAttribute data handling\n\ntidyverse, especially readr, ggplot2 and dplyr\n\nChoropleth mapping\n\ntmap\n\nMultivariate data visualisation and analysis\n\ncoorplot, ggpubr, and heatmaply\n\nCluster analysis\n\ncluster\nClustGeo\n\n\nThe code chunks below installs and launches these R packages into R environment.\n\npacman::p_load(spdep, tmap, sf, ClustGeo, \n               ggpubr, cluster, factoextra, NbClust,\n               heatmaply, corrplot, psych, tidyverse, GGally)\n\nNote: With tidyverse, we do not have to install readr, ggplot2 and dplyr packages separately. In fact, tidyverse also installs other very useful R packages such as tidyr."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#data-import-and-prepatation",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#data-import-and-prepatation",
    "title": "Hands-On Exercise 9: Geographical Segmentation with Spatially Constrained Clustering Techniques Part 2",
    "section": "9.4 Data Import and Prepatation",
    "text": "9.4 Data Import and Prepatation\n\n9.4.1 Loading from RDS from Previous Hands On\n\nshan_sf &lt;- read_rds('data/rds/shan_sf.rds')\nshan_ict &lt;- read_rds('data/rds/shan_ict.rds')\nshan_sf_cluster &lt;- (read_rds('data/rds/shan_sf_cluster.rds'))\nproxmat&lt;-read_rds('data/rds/proxmat.rds')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#spatially-constrained-clustering-skater-approach",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#spatially-constrained-clustering-skater-approach",
    "title": "Hands-On Exercise 9: Geographical Segmentation with Spatially Constrained Clustering Techniques Part 2",
    "section": "9.5 Spatially Constrained Clustering: SKATER approach",
    "text": "9.5 Spatially Constrained Clustering: SKATER approach\nIn this section, you will learn how to derive spatially constrained cluster by using skater() method of spdep package.\n\n9.5.1 Converting into SpatialPolygonsDataFrame\nFirst, we need to convert shan_sf into SpatialPolygonsDataFrame. This is because SKATER function only support sp objects such as SpatialPolygonDataFrame.\nThe code chunk below uses as_Spatial() of sf package to convert shan_sf into a SpatialPolygonDataFrame called shan_sp.\n\nshan_sp &lt;- as_Spatial(shan_sf)\n\n\n\n9.5.2 Computing Neighbour List\nNext, poly2nd() of spdep package will be used to compute the neighbours list from polygon list.\n\nshan.nb &lt;- poly2nb(shan_sp)\nsummary(shan.nb)\n\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\n\nWe can plot the neighbours list on shan_sp by using the code chunk below. Since we now can plot the community area boundaries as well, we plot this graph on top of the map. The first plot command gives the boundaries. This is followed by the plot of the neighbor list object, with coordinates applied to the original SpatialPolygonDataFrame (Shan state township boundaries) to extract the centroids of the polygons. These are used as the nodes for the graph representation. We also set the color to blue and specify add=TRUE to plot the network on top of the boundaries.\n\ncoords &lt;- st_coordinates(\n  st_centroid(st_geometry(shan_sf)))\n\n\nplot(st_geometry(shan_sf), \n     border=grey(.5))\nplot(shan.nb,\n     coords, \n     col=\"blue\", \n     add=TRUE)\n\n\n\n\n\n\n\n\nNote that if you plot the network first and then the boundaries, some of the areas will be clipped. This is because the plotting area is determined by the characteristics of the first plot. In this example, because the boundary map extends further than the graph, we plot it first.\n\n\n9.5.3 Computing minimum spanning tree\n\n9.5.3.1 Calculating edge costs\nNext, nbcosts() of spdep package is used to compute the cost of each edge. It is the distance between it nodes. This function compute this distance using a data.frame with observations vector in each node.\nThe code chunk below is used to compute the cost of each edge.\n\nlcosts &lt;- nbcosts(shan.nb, shan_ict)\n\nFor each observation, this gives the pairwise dissimilarity between its values on the five variables and the values for the neighbouring observation (from the neighbour list). Basically, this is the notion of a generalised weight for a spatial weights matrix.\nNext, We will incorporate these costs into a weights object in the same way as we did in the calculation of inverse of distance weights. In other words, we convert the neighbour list to a list weights object by specifying the just computed lcosts as the weights.\nIn order to achieve this, nb2listw() of spdep package is used as shown in the code chunk below.\nNote that we specify the style as B to make sure the cost values are not row-standardised.\n\nshan.w &lt;- nb2listw(shan.nb, \n                   lcosts, \n                   style=\"B\")\nsummary(shan.w)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\nWeights style: B \nWeights constants summary:\n   n   nn       S0       S1        S2\nB 55 3025 76267.65 58260785 522016004\n\n\n\n\n\n9.5.4 Computing minimum spanning tree\nThe minimum spanning tree is computed by mean of the mstree() of spdep package as shown in the code chunk below.\n\nshan.mst &lt;- mstree(shan.w)\n\nAfter computing the MST, we can check its class and dimension by using the code chunk below.\n\nclass(shan.mst)\n\n[1] \"mst\"    \"matrix\"\n\ndim(shan.mst)\n\n[1] 54  3\n\n\nNote that the dimension is 54 and not 55. This is because the minimum spanning tree consists on n-1 edges (links) in order to traverse all the nodes.\nWe can display the content of shan.mst by using head() as shown in the code chunk below.\n\nhead(shan.mst)\n\n     [,1] [,2]      [,3]\n[1,]   30   27  57.60801\n[2,]   27   41  78.29342\n[3,]   30   51 108.37735\n[4,]   51   38 146.66661\n[5,]   41   39 162.80878\n[6,]   39   19  79.41836\n\n\nThe plot method for the MST include a way to show the observation numbers of the nodes in addition to the edge. As before, we plot this together with the township boundaries. We can see how the initial neighbour list is simplified to just one edge connecting each of the nodes, while passing through all the nodes.\n\nplot(st_geometry(shan_sf), \n                 border=gray(.5))\nplot.mst(shan.mst, \n         coords, \n         col=\"blue\", \n         cex.lab=0.7, \n         cex.circles=0.005, \n         add=TRUE)\n\n\n\n\n\n\n\n\n\n\n9.5.5 Computing spatially constrained clusters using SKATER method\nThe code chunk below compute the spatially constrained cluster using skater() of spdep package.\n\nclust6 &lt;- spdep::skater(edges = shan.mst[,1:2], \n                 data = shan_ict, \n                 method = \"euclidean\", \n                 ncuts = 5)\n\nThe skater() takes three mandatory arguments: - the first two columns of the MST matrix (i.e. not the cost), - the data matrix (to update the costs as units are being grouped), and - the number of cuts. Note: It is set to one less than the number of clusters. So, the value specified is not the number of clusters, but the number of cuts in the graph, one less than the number of clusters.\nThe result of the skater() is an object of class skater. We can examine its contents by using the code chunk below.\n\nstr(clust6)\n\nList of 8\n $ groups      : num [1:55] 3 3 6 3 3 3 3 3 3 3 ...\n $ edges.groups:List of 6\n  ..$ :List of 3\n  .. ..$ node: num [1:18] 47 27 53 38 42 15 41 51 43 32 ...\n  .. ..$ edge: num [1:17, 1:3] 53 15 42 38 41 51 15 27 15 43 ...\n  .. ..$ ssw : num 3759\n  ..$ :List of 3\n  .. ..$ node: num [1:22] 13 48 54 55 45 37 34 16 25 52 ...\n  .. ..$ edge: num [1:21, 1:3] 48 55 54 37 34 16 45 25 13 13 ...\n  .. ..$ ssw : num 3423\n  ..$ :List of 3\n  .. ..$ node: num [1:11] 2 6 8 1 36 4 10 9 46 5 ...\n  .. ..$ edge: num [1:10, 1:3] 6 1 8 36 4 6 8 10 10 9 ...\n  .. ..$ ssw : num 1458\n  ..$ :List of 3\n  .. ..$ node: num [1:2] 44 20\n  .. ..$ edge: num [1, 1:3] 44 20 95\n  .. ..$ ssw : num 95\n  ..$ :List of 3\n  .. ..$ node: num 23\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n  ..$ :List of 3\n  .. ..$ node: num 3\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n $ not.prune   : NULL\n $ candidates  : int [1:6] 1 2 3 4 5 6\n $ ssto        : num 12613\n $ ssw         : num [1:6] 12613 10977 9962 9540 9123 ...\n $ crit        : num [1:2] 1 Inf\n $ vec.crit    : num [1:55] 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"class\")= chr \"skater\"\n\n\nThe most interesting component of this list structure is the groups vector containing the labels of the cluster to which each observation belongs (as before, the label itself is arbitary). This is followed by a detailed summary for each of the clusters in the edges.groups list. Sum of squares measures are given as ssto for the total and ssw to show the effect of each of the cuts on the overall criterion.\nWe can check the cluster assignment by using the conde chunk below.\n\nccs6 &lt;- clust6$groups\nccs6\n\n [1] 3 3 6 3 3 3 3 3 3 3 1 2 2 2 1 2 2 2 1 4 2 1 5 2 2 2 1 2 1 1 2 1 1 2 2 3 2 1\n[39] 1 1 1 1 1 4 2 3 1 2 2 2 1 2 1 2 2\n\n\nWe can find out how many observations are in each cluster by means of the table command. Parenthetially, we can also find this as the dimension of each vector in the lists contained in edges.groups. For example, the first list has node with dimension 12, which is also the number of observations in the first cluster.\n\ntable(ccs6)\n\nccs6\n 1  2  3  4  5  6 \n18 22 11  2  1  1 \n\n\nLastly, we can also plot the pruned tree that shows the five clusters on top of the townshop area.\n\nplot(st_geometry(shan_sf), \n     border=gray(.5))\nplot(clust6, \n     coords, \n     cex.lab=.7,\n     groups.colors=c(\"red\",\"green\",\"blue\", \"brown\", \"pink\"),\n     cex.circles=0.005, \n     add=TRUE)\n\n\n\n\n\n\n\n\n\n\n9.5.6 Visualising the clusters in choropleth map\nThe code chunk below is used to plot the newly derived clusters by using SKATER method.\n\ngroups_mat &lt;- as.matrix(clust6$groups)\nshan_sf_spatialcluster &lt;- cbind(shan_sf_cluster, as.factor(groups_mat)) %&gt;%\n  rename(`SP_CLUSTER`=`as.factor.groups_mat.`)\nqtm(shan_sf_spatialcluster, \"SP_CLUSTER\")\n\n\n\n\n\n\n\n\nFor easy comparison, it will be better to place both the hierarchical clustering and spatially constrained hierarchical clustering maps next to each other.\n\nhclust.map &lt;- qtm(shan_sf_cluster,\n                  \"CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\nshclust.map &lt;- qtm(shan_sf_spatialcluster,\n                   \"SP_CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\ntmap_arrange(hclust.map, shclust.map,\n             asp=NA, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#spatially-constrained-clustering-clustgeo-method",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#spatially-constrained-clustering-clustgeo-method",
    "title": "Hands-On Exercise 9: Geographical Segmentation with Spatially Constrained Clustering Techniques Part 2",
    "section": "9.6 Spatially Constrained Clustering: ClustGeo Method",
    "text": "9.6 Spatially Constrained Clustering: ClustGeo Method\nIn this section, you will gain hands-on experience on using functions provided by ClustGeo package to perform non-spatially constrained hierarchical cluster analysis and spatially constrained cluster analysis.\n\n9.6.1 A short note about ClustGeo package\nClustGeo package is an R package specially designed to support the need of performing spatially constrained cluster analysis. More specifically, it provides a Ward-like hierarchical clustering algorithm called hclustgeo() including spatial/geographical constraints.\nIn the nutshell, the algorithm uses two dissimilarity matrices D0 and D1 along with a mixing parameter alpha, whereby the value of alpha must be a real number between [0, 1]. D0 can be non-Euclidean and the weights of the observations can be non-uniform. It gives the dissimilarities in the attribute/clustering variable space. D1, on the other hand, gives the dissimilarities in the constraint space. The criterion minimised at each stage is a convex combination of the homogeneity criterion calculated with D0 and the homogeneity criterion calculated with D1.\nThe idea is then to determine a value of alpha which increases the spatial contiguity without deteriorating too much the quality of the solution based on the variables of interest. This need is supported by a function called choicealpha().\n\n\n9.6.2 Ward-like hierarchical clustering: ClustGeo\nClustGeo package provides function called hclustgeo() to perform a typical Ward-like hierarchical clustering just like hclust() you learned in previous section.\nTo perform non-spatially constrained hierarchical clustering, we only need to provide the function a dissimilarity matrix as shown in the code chunk below.\n\nnongeo_cluster &lt;- hclustgeo(proxmat)\nplot(nongeo_cluster, cex = 0.5)\nrect.hclust(nongeo_cluster, \n            k = 6, \n            border = 2:5)\n\n\n\n\n\n\n\n\nNote that the dissimilarity matrix must be an object of class dist, i.e. an object obtained with the function dist(). For sample code chunk, please refer to 5.7.6 Computing proximity matrix\n\n9.6.2.1 Mapping the clusters formed\nSimilarly, we can plot the clusters on a categorical area shaded map by using the steps we learned in 5.7.12 Mapping the clusters formed.\n\ngroups &lt;- as.factor(cutree(nongeo_cluster, k=6))\n\n\nshan_sf_ngeo_cluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\n\nqtm(shan_sf_ngeo_cluster, \"CLUSTER\")\n\n\n\n\n\n\n\n\n\n\n\n9.6.3 Spatially Constrained Hierarchical Clustering\nBefore we can performed spatially constrained hierarchical clustering, a spatial distance matrix will be derived by using st_distance() of sf package.\n\ndist &lt;- st_distance(shan_sf, shan_sf)\ndistmat &lt;- as.dist(dist)\n\nNotice that as.dist() is used to convert the data frame into matrix.\nNext, choicealpha() will be used to determine a suitable value for the mixing parameter alpha as shown in the code chunk below.\n\ncr &lt;- choicealpha(proxmat, distmat, range.alpha = seq(0, 1, 0.1), K=6, graph = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith reference to the graphs above, alpha = 0.2 will be used as shown in the code chunk below.\n\nclustG &lt;- hclustgeo(proxmat, distmat, alpha = 0.2)\n\nNext, cutree() is used to derive the cluster objecct.\n\ngroups &lt;- as.factor(cutree(clustG, k=6))\n\nWe will then join back the group list with shan_sf polygon feature data frame by using the code chunk below.\n\nshan_sf_Gcluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\nWe can now plot the map of the newly delineated spatially constrained clusters.\n\nqtm(shan_sf_Gcluster, \"CLUSTER\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#visual-interpretation-of-clusters",
    "href": "Hands-on_Ex/Hands-on_Ex09/Hands-on_Ex09.html#visual-interpretation-of-clusters",
    "title": "Hands-On Exercise 9: Geographical Segmentation with Spatially Constrained Clustering Techniques Part 2",
    "section": "9.7 Visual Interpretation of Clusters",
    "text": "9.7 Visual Interpretation of Clusters\n\n9.7.1 Visualising individual clustering variable\nCode chunk below is used to reveal the distribution of a clustering variable (i.e RADIO_PR) by cluster.\n\nggplot(data = shan_sf_ngeo_cluster,\n       aes(x = CLUSTER, y = RADIO_PR)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nThe boxplot reveals Cluster 3 displays the highest mean Radio Ownership Per Thousand Household. This is followed by Cluster 2, 1, 4, 6 and 5.\n\n\n9.7.2 Multivariate Visualisation\nPast studies shown that parallel coordinate plot can be used to reveal clustering variables by cluster very effectively. In the code chunk below, ggparcoord() of GGally package\n\nggparcoord(data = shan_sf_ngeo_cluster, \n           columns = c(17:21), \n           scale = \"globalminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of ICT Variables by Cluster\") +\n  facet_grid(~ CLUSTER) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n\nThe parallel coordinate plot above reveals that households in Cluster 4 townships tend to own the highest number of TV and mobile-phone. On the other hand, households in Cluster 5 tends to own the lowest of all the five ICT.\nNote that the scale argument of ggparcoor() provide several methods to scale the clustering variables. They are:\n\nstd: univariately, subtract mean and divide by standard deviation.\nrobust: univariately, subtract median and divide by median absolute deviation.\nuniminmax: univariately, scale so the minimum of the variable is zero, and the maximum is one.\nglobalminmax: no scaling is done; the range of the graphs is defined by the global minimum and the global maximum.\ncenter: use uniminmax to standardize vertical height, then center each variable at a value specified by the scaleSummary param.\ncenterObs: use uniminmax to standardize vertical height, then center each variable at the value of the observation specified by the centerObsID param\n\nThere is no one best scaling method to use. You should explore them and select the one that best meet your analysis need.\nLast but not least, we can also compute the summary statistics such as mean, median, sd, etc to complement the visual interpretation.\nIn the code chunk below, group_by() and summarise() of dplyr are used to derive mean values of the clustering variables.\n\nshan_sf_ngeo_cluster %&gt;% \n  st_set_geometry(NULL) %&gt;%\n  group_by(CLUSTER) %&gt;%\n  summarise(mean_RADIO_PR = mean(RADIO_PR),\n            mean_TV_PR = mean(TV_PR),\n            mean_LLPHONE_PR = mean(LLPHONE_PR),\n            mean_MPHONE_PR = mean(MPHONE_PR),\n            mean_COMPUTER_PR = mean(COMPUTER_PR))\n\n# A tibble: 6 × 6\n  CLUSTER mean_RADIO_PR mean_TV_PR mean_LLPHONE_PR mean_MPHONE_PR\n  &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n1 1               221.        521.            44.2           246.\n2 2               237.        402.            23.9           134.\n3 3               300.        611.            52.2           392.\n4 4               196.        744.            99.0           651.\n5 5               124.        224.            38.0           132.\n6 6                98.6       499.            74.5           468.\n# ℹ 1 more variable: mean_COMPUTER_PR &lt;dbl&gt;"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "title": "Hands-On Exercise 6: Global & Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "In this hands-on exercise, we will learn how to compute Global Measures of Spatial Autocorrelation (GMSA) by using spdep package. By the end to this hands-on exercise, we will be able to:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of spdep package,\n\nplot Moran scatterplot,\ncompute and plot spatial correlogram using appropriate function of spdep package.\n\nprovide statistically correct interpretation of GSA statistics.\n\nAlso, we will learn how to compute Local Measures of Spatial Autocorrelation (LMSA). By the end to this hands-on exercise, we will be able to:\n\ncompute Local Indicator of Spatial Association (LISA) statistics for detecting clusters and outliers by using appropriate functions spdep package;\ncompute Getis-Ord’s Gi-statistics for detecting hot spot or/and cold spot area by using appropriate functions of spdep package; and\nto visualise the analysis output by using tmap package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#overview",
    "title": "Hands-On Exercise 6: Global & Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "In this hands-on exercise, we will learn how to compute Global Measures of Spatial Autocorrelation (GMSA) by using spdep package. By the end to this hands-on exercise, we will be able to:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of spdep package,\n\nplot Moran scatterplot,\ncompute and plot spatial correlogram using appropriate function of spdep package.\n\nprovide statistically correct interpretation of GSA statistics.\n\nAlso, we will learn how to compute Local Measures of Spatial Autocorrelation (LMSA). By the end to this hands-on exercise, we will be able to:\n\ncompute Local Indicator of Spatial Association (LISA) statistics for detecting clusters and outliers by using appropriate functions spdep package;\ncompute Getis-Ord’s Gi-statistics for detecting hot spot or/and cold spot area by using appropriate functions of spdep package; and\nto visualise the analysis output by using tmap package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#getting-started",
    "title": "Hands-On Exercise 6: Global & Local Measures of Spatial Autocorrelation",
    "section": "6.2 Getting Started",
    "text": "6.2 Getting Started\n\n6.2.1 The analytical question\nIn spatial policy, one of the main development objective of the local government and planners is to ensure equal distribution of development in the province. Our task in this study, hence, is to apply appropriate spatial statistical methods to discover if development are even distributed geographically. If the answer is No. Then, our next question will be “is there sign of spatial clustering?”. And, if the answer for this question is yes, then our next question will be “where are these clusters?”\nIn this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of Hunan Provice, People Republic of China.\n\n\n6.2.2 The Study Area and Data\nTwo data sets will be used in this hands-on exercise, they are:\n\nHunan province administrative boundary layer at county level. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\n6.2.3 Setting the Analytical Toolls\nBefore we get started, we need to ensure that spdep, sf, tmap and tidyverse packages of R are currently installed in your R.\n\nsf is use for importing and handling geospatial data in R,\ntidyverse is mainly use for wrangling attribute data in R,\nspdep will be used to compute spatial weights, global and local spatial autocorrelation statistics, and\ntmap will be used to prepare cartographic quality chropleth map.\n\nThe code chunk below is used to perform the following tasks:\n\ncreating a package list containing the necessary R packages,\nchecking if the R packages in the package list have been installed in R,\n\nif they have yet to be installed, RStudio will installed the missing packages,\n\nlaunching the packages into R environment.\n\n\npacman::p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#getting-the-data-into-r-environment",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#getting-the-data-into-r-environment",
    "title": "Hands-On Exercise 6: Global & Local Measures of Spatial Autocorrelation",
    "section": "6.3 Getting the Data Into R Environment",
    "text": "6.3 Getting the Data Into R Environment\nIn this section, you will learn how to bring a geospatial data and its associated attribute table into R environment. The geospatial data is in ESRI shapefile format and the attribute table is in csv fomat.\n\n6.3.1 Import shapefile into r environment\nThe code chunk below uses st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/georgiaxng/georgiaxng/is415-handson/Hands-on_Ex/Hands-on_Ex06/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n6.3.2 Import csv file into r environment\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package. The output is R data frame class.\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n6.3.3 Performing relational join\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan,hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\n\n\n6.3.4 Visualising Regional Development Indicator\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#global-measures-of-spatial-autocorrelation",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#global-measures-of-spatial-autocorrelation",
    "title": "Hands-On Exercise 6: Global & Local Measures of Spatial Autocorrelation",
    "section": "6.4 Global Measures of Spatial Autocorrelation",
    "text": "6.4 Global Measures of Spatial Autocorrelation\nIn this section, we will learn how to compute global spatial autocorrelation statistics and to perform spatial complete randomness test for global spatial autocorrelation.\n\n6.4.1 Computing Contiguity Spatial Weights\nBefore we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area.\nIn the code chunk below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. If you look at the documentation you will see that you can pass a “queen” argument that takes TRUE or FALSE as options. If you do not specify this argument the default is set to TRUE, that is, if you don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\nMore specifically, the code chunk below is used to compute Queen contiguity weight matrix.\n\nwm_q &lt;- poly2nb(hunan, \n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one neighbours.\n\n\n6.4.2 Row-standardised weights matrix\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\nrswm_q &lt;- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\n\n\n\n\n\n\nWhat can we learn from the code chunk above?\n\n\n\n\nThe input of nb2listw() must be an object of class nb. The syntax of the function has two major arguments, namely style and zero.poly.\nstyle can take values “W”, “B”, “C”, “U”, “minmax” and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nIf zero policy is set to TRUE, weights vectors of zero length are inserted for regions without neighbour in the neighbours list. These will in turn generate lag values of zero, equivalent to the sum of products of the zero row t(rep(0, length=length(neighbours))) %*% x, for arbitrary numerical vector x of length length(neighbours). The spatially lagged value of x for the zero-neighbour region will then be zero, which may (or may not) be a sensible choice."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#global-measures-of-spatial-autocorrelation-morans-i",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#global-measures-of-spatial-autocorrelation-morans-i",
    "title": "Hands-On Exercise 6: Global & Local Measures of Spatial Autocorrelation",
    "section": "6.5 Global Measures of Spatial Autocorrelation: Moran’s I",
    "text": "6.5 Global Measures of Spatial Autocorrelation: Moran’s I\nIn this section, we will learn how to perform Moran’s I statistics testing by using moran.test() of spdep.\n\n9.5.1 Maron’s I test\nThe code chunk below performs Moran’s I statistical testing using moran.test() of spdep.\n\nmoran.test(hunan$GDPPC, \n           listw=rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\n\n6.5.2 Computing Monte Carlo Moran’s I\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep. A total of 1000 simulation will be performed.\n\nset.seed(1234)\nbperm= moran.mc(hunan$GDPPC, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\n6.5.3 Visualising Monte Carlo Moran’s I\nIt is always a good practice for us the examine the simulated Moran’s I test statistics in greater detail. This can be achieved by plotting the distribution of the statistical values as a histogram by using the code chunk below.\nIn the code chunk below hist() and abline() of R Graphics are used.\n\nmean(bperm$res[1:999])\n\n[1] -0.01504572\n\n\n\nvar(bperm$res[1:999])\n\n[1] 0.004371574\n\n\n\nsummary(bperm$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.18339 -0.06168 -0.02125 -0.01505  0.02611  0.27593 \n\n\n\nhist(bperm$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Simulated Moran's I\")\nabline(v=0, \n       col=\"red\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#global-measures-of-spatial-autocorrelation-gearys-c",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#global-measures-of-spatial-autocorrelation-gearys-c",
    "title": "Hands-On Exercise 6: Global & Local Measures of Spatial Autocorrelation",
    "section": "6.6 Global Measures of Spatial Autocorrelation: Geary’s C",
    "text": "6.6 Global Measures of Spatial Autocorrelation: Geary’s C\nIn this section, we will learn how to perform Geary’s C statistics testing by using appropriate functions of spdep package.\n\n6.6.1 Geary’s C test\nThe code chunk below performs Geary’s C test for spatial autocorrelation by using geary.test() of spdep.\n\ngeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q   \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\n\n\n6.6.2 Computing Monte Carlo Geary’s C\nThe code chunk below performs permutation test for Geary’s C statistic by using geary.mc() of spdep.\n\nset.seed(1234)\nbperm=geary.mc(hunan$GDPPC, \n               listw=rswm_q, \n               nsim=999)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\n6.6.3 Visualising the Monte Carlo Geary’s C\nNext, we will plot a histogram to reveal the distribution of the simulated values by using the code chunk below.\n\nmean(bperm$res[1:999])\n\n[1] 1.004402\n\n\n\nvar(bperm$res[1:999])\n\n[1] 0.007436493\n\n\n\nsummary(bperm$res[1:999])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7142  0.9502  1.0052  1.0044  1.0595  1.2722 \n\n\n\nhist(bperm$res, freq=TRUE, breaks=20, xlab=\"Simulated Geary c\")\nabline(v=1, col=\"red\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#spatial-correlogram",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#spatial-correlogram",
    "title": "Hands-On Exercise 6: Global & Local Measures of Spatial Autocorrelation",
    "section": "6.7 Spatial Correlogram",
    "text": "6.7 Spatial Correlogram\nSpatial correlograms are great to examine patterns of spatial autocorrelation in your data or model residuals. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran’s I or Geary’s c) against distance.Although correlograms are not as fundamental as variograms (a keystone concept of geostatistics), they are very useful as an exploratory and descriptive tool. For this purpose they actually provide richer information than variograms.\n\n6.7.1 Compute Moran’s I correlogram\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Moran’s I. The plot() of base Graph is then used to plot the output.\n\nMI_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\n\n\n\n\n\n\nBy plotting the output might not allow us to provide complete interpretation. This is because not all autocorrelation values are statistically significant. Hence, it is important for us to examine the full analysis report by printing out the analysis results as in the code chunk below.\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n6.7.2 Compute Geary’s C correlogram and plot\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Geary’s C. The plot() of base Graph is then used to plot the output.\n\nGC_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)\n\n\n\n\n\n\n\n\nSimilar to the previous step, we will print out the analysis report by using the code chunk below.\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#local-indicators-of-spatial-associationlisa",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#local-indicators-of-spatial-associationlisa",
    "title": "Hands-On Exercise 6: Global & Local Measures of Spatial Autocorrelation",
    "section": "6.8 Local Indicators of Spatial Association(LISA)",
    "text": "6.8 Local Indicators of Spatial Association(LISA)\nLocal Indicators of Spatial Association or LISA are statistics that evaluate the existence of clusters and/or outliers in the spatial arrangement of a given variable. For instance if we are studying distribution of GDP per capita of Hunan Provice, People Republic of China, local clusters in GDP per capita mean that there are counties that have higher or lower rates than is to be expected by chance alone; that is, the values occurring are above or below those of a random distribution in space.\nIn this section, you will learn how to apply appropriate Local Indicators for Spatial Association (LISA), especially local Moran’I to detect cluster and/or outlier from GDP per capita 2012 of Hunan Province, PRC.\n\n6.8.1 Computing Contiguity Spatial Weights\nBefore we can compute the local spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area.\nIn the code chunk below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. If you look at the documentation you will see that you can pass a “queen” argument that takes TRUE or FALSE as options. If you do not specify this argument the default is set to TRUE, that is, if you don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\nMore specifically, the code chunk below is used to compute Queen contiguity weight matrix.\n\nwm_q &lt;- poly2nb(hunan, \n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one neighbours.\n\n\n6.8.2 Row-standardised weights matrix\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\nrswm_q &lt;- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\n\n\n6.8.3 Computing local Moran’s I\nTo compute local Moran’s I, the localmoran() function of spdep will be used. It computes Ii values, given a set of zi values and a listw object providing neighbour weighting information for the polygon associated with the zi values.\nThe code chunks below are used to compute local Moran’s I of GDPPC2012 at the county level.\n\nfips &lt;- order(hunan$County)\nlocalMI &lt;- localmoran(hunan$GDPPC, rswm_q)\nhead(localMI)\n\n            Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.001468468 -2.815006e-05 4.723841e-04 -0.06626904      0.9471636\n2  0.025878173 -6.061953e-04 1.016664e-02  0.26266425      0.7928094\n3 -0.011987646 -5.366648e-03 1.133362e-01 -0.01966705      0.9843090\n4  0.001022468 -2.404783e-07 5.105969e-06  0.45259801      0.6508382\n5  0.014814881 -6.829362e-05 1.449949e-03  0.39085814      0.6959021\n6 -0.038793829 -3.860263e-04 6.475559e-03 -0.47728835      0.6331568\n\n\nlocalmoran() function returns a matrix of values whose columns are:\n\nIi: the local Moran’s I statistics\nE.Ii: the expectation of local moran statistic under the randomisation hypothesis\nVar.Ii: the variance of local moran statistic under the randomisation hypothesis\nZ.Ii:the standard deviate of local moran statistic\nPr(): the p-value of local moran statistic\n\nThe code chunk below list the content of the local Moran matrix derived by using printCoefmat().\n\nprintCoefmat(data.frame(\n  localMI[fips,], \n  row.names=hunan$County[fips]),\n  check.names=FALSE)\n\n                       Ii        E.Ii      Var.Ii        Z.Ii Pr.z....E.Ii..\nAnhua         -2.2493e-02 -5.0048e-03  5.8235e-02 -7.2467e-02         0.9422\nAnren         -3.9932e-01 -7.0111e-03  7.0348e-02 -1.4791e+00         0.1391\nAnxiang       -1.4685e-03 -2.8150e-05  4.7238e-04 -6.6269e-02         0.9472\nBaojing        3.4737e-01 -5.0089e-03  8.3636e-02  1.2185e+00         0.2230\nChaling        2.0559e-02 -9.6812e-04  2.7711e-02  1.2932e-01         0.8971\nChangning     -2.9868e-05 -9.0010e-09  1.5105e-07 -7.6828e-02         0.9388\nChangsha       4.9022e+00 -2.1348e-01  2.3194e+00  3.3590e+00         0.0008\nChengbu        7.3725e-01 -1.0534e-02  2.2132e-01  1.5895e+00         0.1119\nChenxi         1.4544e-01 -2.8156e-03  4.7116e-02  6.8299e-01         0.4946\nCili           7.3176e-02 -1.6747e-03  4.7902e-02  3.4200e-01         0.7324\nDao            2.1420e-01 -2.0824e-03  4.4123e-02  1.0297e+00         0.3032\nDongan         1.5210e-01 -6.3485e-04  1.3471e-02  1.3159e+00         0.1882\nDongkou        5.2918e-01 -6.4461e-03  1.0748e-01  1.6338e+00         0.1023\nFenghuang      1.8013e-01 -6.2832e-03  1.3257e-01  5.1198e-01         0.6087\nGuidong       -5.9160e-01 -1.3086e-02  3.7003e-01 -9.5104e-01         0.3416\nGuiyang        1.8240e-01 -3.6908e-03  3.2610e-02  1.0305e+00         0.3028\nGuzhang        2.8466e-01 -8.5054e-03  1.4152e-01  7.7931e-01         0.4358\nHanshou        2.5878e-02 -6.0620e-04  1.0167e-02  2.6266e-01         0.7928\nHengdong       9.9964e-03 -4.9063e-04  6.7742e-03  1.2742e-01         0.8986\nHengnan        2.8064e-02 -3.2160e-04  3.7597e-03  4.6294e-01         0.6434\nHengshan      -5.8201e-03 -3.0437e-05  5.1076e-04 -2.5618e-01         0.7978\nHengyang       6.2997e-02 -1.3046e-03  2.1865e-02  4.3486e-01         0.6637\nHongjiang      1.8790e-01 -2.3019e-03  3.1725e-02  1.0678e+00         0.2856\nHuarong       -1.5389e-02 -1.8667e-03  8.1030e-02 -4.7503e-02         0.9621\nHuayuan        8.3772e-02 -8.5569e-04  2.4495e-02  5.4072e-01         0.5887\nHuitong        2.5997e-01 -5.2447e-03  1.1077e-01  7.9685e-01         0.4255\nJiahe         -1.2431e-01 -3.0550e-03  5.1111e-02 -5.3633e-01         0.5917\nJianghua       2.8651e-01 -3.8280e-03  8.0968e-02  1.0204e+00         0.3076\nJiangyong      2.4337e-01 -2.7082e-03  1.1746e-01  7.1800e-01         0.4728\nJingzhou       1.8270e-01 -8.5106e-04  2.4363e-02  1.1759e+00         0.2396\nJinshi        -1.1988e-02 -5.3666e-03  1.1334e-01 -1.9667e-02         0.9843\nJishou        -2.8680e-01 -2.6305e-03  4.4028e-02 -1.3543e+00         0.1756\nLanshan        6.3334e-02 -9.6365e-04  2.0441e-02  4.4972e-01         0.6529\nLeiyang        1.1581e-02 -1.4948e-04  2.5082e-03  2.3422e-01         0.8148\nLengshuijiang -1.7903e+00 -8.2129e-02  2.1598e+00 -1.1623e+00         0.2451\nLi             1.0225e-03 -2.4048e-07  5.1060e-06  4.5260e-01         0.6508\nLianyuan      -1.4672e-01 -1.8983e-03  1.9145e-02 -1.0467e+00         0.2952\nLiling         1.3774e+00 -1.5097e-02  4.2601e-01  2.1335e+00         0.0329\nLinli          1.4815e-02 -6.8294e-05  1.4499e-03  3.9086e-01         0.6959\nLinwu         -2.4621e-03 -9.0703e-06  1.9258e-04 -1.7676e-01         0.8597\nLinxiang       6.5904e-02 -2.9028e-03  2.5470e-01  1.3634e-01         0.8916\nLiuyang        3.3688e+00 -7.7502e-02  1.5180e+00  2.7972e+00         0.0052\nLonghui        8.0801e-01 -1.1377e-02  1.5538e-01  2.0787e+00         0.0376\nLongshan       7.5663e-01 -1.1100e-02  3.1449e-01  1.3690e+00         0.1710\nLuxi           1.8177e-01 -2.4855e-03  3.4249e-02  9.9561e-01         0.3194\nMayang         2.1852e-01 -5.8773e-03  9.8049e-02  7.1663e-01         0.4736\nMiluo          1.8704e+00 -1.6927e-02  2.7925e-01  3.5715e+00         0.0004\nNan           -9.5789e-03 -4.9497e-04  6.8341e-03 -1.0988e-01         0.9125\nNingxiang      1.5607e+00 -7.3878e-02  8.0012e-01  1.8274e+00         0.0676\nNingyuan       2.0910e-01 -7.0884e-03  8.2306e-02  7.5356e-01         0.4511\nPingjiang     -9.8964e-01 -2.6457e-03  5.6027e-02 -4.1698e+00         0.0000\nQidong         1.1806e-01 -2.1207e-03  2.4747e-02  7.6396e-01         0.4449\nQiyang         6.1966e-02 -7.3374e-04  8.5743e-03  6.7712e-01         0.4983\nRucheng       -3.6992e-01 -8.8999e-03  2.5272e-01 -7.1814e-01         0.4727\nSangzhi        2.5053e-01 -4.9470e-03  6.8000e-02  9.7972e-01         0.3272\nShaodong      -3.2659e-02 -3.6592e-05  5.0546e-04 -1.4510e+00         0.1468\nShaoshan       2.1223e+00 -5.0227e-02  1.3668e+00  1.8583e+00         0.0631\nShaoyang       5.9499e-01 -1.1253e-02  1.3012e-01  1.6807e+00         0.0928\nShimen        -3.8794e-02 -3.8603e-04  6.4756e-03 -4.7729e-01         0.6332\nShuangfeng     9.2835e-03 -2.2867e-03  3.1516e-02  6.5174e-02         0.9480\nShuangpai      8.0591e-02 -3.1366e-04  8.9838e-03  8.5358e-01         0.3933\nSuining        3.7585e-01 -3.5933e-03  4.1870e-02  1.8544e+00         0.0637\nTaojiang      -2.5394e-01 -1.2395e-03  1.4477e-02 -2.1002e+00         0.0357\nTaoyuan        1.4729e-02 -1.2039e-04  8.5103e-04  5.0903e-01         0.6107\nTongdao        4.6482e-01 -6.9870e-03  1.9879e-01  1.0582e+00         0.2900\nWangcheng      4.4220e+00 -1.1067e-01  1.3596e+00  3.8873e+00         0.0001\nWugang         7.1003e-01 -7.8144e-03  1.0710e-01  2.1935e+00         0.0283\nXiangtan       2.4530e-01 -3.6457e-04  3.2319e-03  4.3213e+00         0.0000\nXiangxiang     2.6271e-01 -1.2703e-03  2.1290e-02  1.8092e+00         0.0704\nXiangyin       5.4525e-01 -4.7442e-03  7.9236e-02  1.9539e+00         0.0507\nXinhua         1.1810e-01 -6.2649e-03  8.6001e-02  4.2409e-01         0.6715\nXinhuang       1.5725e-01 -4.1820e-03  3.6648e-01  2.6667e-01         0.7897\nXinning        6.8928e-01 -9.6674e-03  2.0328e-01  1.5502e+00         0.1211\nXinshao        5.7578e-02 -8.5932e-03  1.1769e-01  1.9289e-01         0.8470\nXintian       -7.4050e-03 -5.1493e-03  1.0877e-01 -6.8395e-03         0.9945\nXupu           3.2406e-01 -5.7468e-03  5.7735e-02  1.3726e+00         0.1699\nYanling       -6.9021e-02 -5.9211e-04  9.9306e-03 -6.8667e-01         0.4923\nYizhang       -2.6844e-01 -2.2463e-03  4.7588e-02 -1.2202e+00         0.2224\nYongshun       6.3064e-01 -1.1350e-02  1.8830e-01  1.4795e+00         0.1390\nYongxing       4.3411e-01 -9.0735e-03  1.5088e-01  1.1409e+00         0.2539\nYou            7.8750e-02 -7.2728e-03  1.2116e-01  2.4714e-01         0.8048\nYuanjiang      2.0004e-04 -1.7760e-04  2.9798e-03  6.9181e-03         0.9945\nYuanling       8.7298e-03 -2.2981e-06  2.3221e-05  1.8121e+00         0.0700\nYueyang        4.1189e-02 -1.9768e-04  2.3113e-03  8.6085e-01         0.3893\nZhijiang       1.0476e-01 -7.8123e-04  1.3100e-02  9.2214e-01         0.3565\nZhongfang     -2.2685e-01 -2.1455e-03  3.5927e-02 -1.1855e+00         0.2358\nZhuzhou        3.2864e-01 -5.2432e-04  7.2391e-03  3.8688e+00         0.0001\nZixing        -7.6849e-01 -8.8210e-02  9.4057e-01 -7.0144e-01         0.4830\n\n\n\n6.8.3.1 Mapping the local Moran’s I\nBefore mapping the local Moran’s I map, it is wise to append the local Moran’s I dataframe (i.e. localMI) onto hunan SpatialPolygonDataFrame. The code chunks below can be used to perform the task. The out SpatialPolygonDataFrame is called hunan.localMI.\n\nhunan.localMI &lt;- cbind(hunan,localMI) %&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\n\n\n6.8.3.2 Mapping local Moran’s I values\nUsing choropleth mapping functions of tmap package, we can plot the local Moran’s I values by using the code chinks below.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n6.8.3.3 Mapping local Moran’s I p-values\nThe choropleth shows there is evidence for both positive and negative Ii values. However, it is useful to consider the p-values for each of these values, as consider above.\nThe code chunks below produce a choropleth map of Moran’s I p-values by using functions of tmap package.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n6.8.3.4 Mapping both local Moran’s I values and p-values\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation.\n\nlocalMI.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\", \n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#creating-a-lisa-cluster-map",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#creating-a-lisa-cluster-map",
    "title": "Hands-On Exercise 6: Global & Local Measures of Spatial Autocorrelation",
    "section": "6.9 Creating a LISA Cluster Map",
    "text": "6.9 Creating a LISA Cluster Map\nThe LISA Cluster Map shows the significant locations color coded by type of spatial autocorrelation. The first step before we can generate the LISA cluster map is to plot the Moran scatterplot.\n\n6.9.1 Plotting Moran scatterplot\nThe Moran scatterplot is an illustration of the relationship between the values of the chosen attribute at each location and the average value of the same attribute at neighboring locations.\nThe code chunk below plots the Moran scatterplot of GDPPC 2012 by using moran.plot() of spdep.\n\nnci &lt;- moran.plot(hunan$GDPPC, rswm_q,\n                  labels=as.character(hunan$County), \n                  xlab=\"GDPPC 2012\", \n                  ylab=\"Spatially Lag GDPPC 2012\")\n\n\n\n\n\n\n\n\nNotice that the plot is split in 4 quadrants. The top right corner belongs to areas that have high GDPPC and are surrounded by other areas that have the average level of GDPPC. This are the high-high locations in the lesson slide.\n\n\n6.9.2 Plotting Moran scatterplot with standardised variable\nFirst we will use scale() to centers and scales the variable. Here centering is done by subtracting the mean (omitting NAs) the corresponding columns, and scaling is done by dividing the (centered) variable by their standard deviations.\n\nhunan$Z.GDPPC &lt;- scale(hunan$GDPPC) %&gt;% \n  as.vector \n\nThe as.vector() added to the end is to make sure that the data type we get out of this is a vector, that map neatly into out dataframe.\nNow, we are ready to plot the Moran scatterplot again by using the code chunk below.\n\nnci2 &lt;- moran.plot(hunan$Z.GDPPC, rswm_q,\n                   labels=as.character(hunan$County),\n                   xlab=\"z-GDPPC 2012\", \n                   ylab=\"Spatially Lag z-GDPPC 2012\")\n\n\n\n\n\n\n\n\n\n\n6.9.3 Preparing LISA map classes\nThe code chunks below show the steps to prepare a LISA cluster map.\n\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\n\nNext, derives the spatially lagged variable of interest (i.e. GDPPC) and centers the spatially lagged variable around its mean.\n\nhunan$lag_GDPPC &lt;- lag.listw(rswm_q, hunan$GDPPC)\nDV &lt;- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)     \n\nThis is follow by centering the local Moran’s around the mean.\n\nLM_I &lt;- localMI[,1] - mean(localMI[,1])    \n\nNext, we will set a statistical significance level for the local Moran.\n\nsignif &lt;- 0.05       \n\nThese four command lines define the low-low (1), low-high (2), high-low (3) and high-high (4) categories.\n\nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4      \n\nLastly, places non-significant Moran in the category 0.\n\nquadrant[localMI[,5]&gt;signif] &lt;- 0\n\nIn fact, we can combined all the steps into one single code chunk as shown below:\n\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\nhunan$lag_GDPPC &lt;- lag.listw(rswm_q, hunan$GDPPC)\nDV &lt;- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)     \nLM_I &lt;- localMI[,1]   \nsignif &lt;- 0.05       \nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4    \nquadrant[localMI[,5]&gt;signif] &lt;- 0\n\n\n\n6.9.4 Plotting LISA map\nNow, we can build the LISA map by using the code chunks below.\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n\n\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\ntmap_arrange(gdppc, LISAmap, \n             asp=1, ncol=2)\n\n\n\n\n\n\n\n\nWe can also include the local Moran’s I map and p-value map as shown below for easy comparison."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#hot-spot-and-cold-spot-area-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#hot-spot-and-cold-spot-area-analysis",
    "title": "Hands-On Exercise 6: Global & Local Measures of Spatial Autocorrelation",
    "section": "6.10 Hot Spot and Cold Spot Area Analysis",
    "text": "6.10 Hot Spot and Cold Spot Area Analysis\nBeside detecting cluster and outliers, localised spatial statistics can be also used to detect hot spot and/or cold spot areas.\nThe term ‘hot spot’ has been used generically across disciplines to describe a region or value that is higher relative to its surroundings (Lepers et al 2005, Aben et al 2012, Isobe et al 2015).\n\n6.10.1 Getis and Ord’s G-Statistics\nAn alternative spatial statistics to detect spatial anomalies is the Getis and Ord’s G-statistics (Getis and Ord, 1972; Ord and Getis, 1995). It looks at neighbours within a defined proximity to identify where either high or low values clutser spatially. Here, statistically significant hot-spots are recognised as areas of high values where other areas within a neighbourhood range also share high values too.\nThe analysis consists of three steps:\n\nDeriving spatial weight matrix\nComputing Gi statistics\nMapping Gi statistics\n\n\n\n6.10.2 Deriving distance-based weight matrix\nFirst, we need to define a new set of neighbours. Whist the spatial autocorrelation considered units which shared borders, for Getis-Ord we are defining neighbours based on distance.\nThere are two type of distance-based proximity matrix, they are:\n\nfixed distance weight matrix; and\nadaptive distance weight matrix.\n\n\n6.10.2.1 Deriving the centroid\nWe will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid() on the sf object: us.bound. We need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of us.bound. Our function will be st_centroid(). We will be using map_dbl variation of map from the purrr package. For more documentation, check out map documentation\nTo get our longitude values we map the st_centroid() function over the geometry column of us.bound and access the longitude value through double bracket notation [[]] and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\ncoords &lt;- cbind(longitude, latitude)\n\n\n\n6.10.2.2 Determine the cut-off distance\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\n#coords &lt;- coordinates(hunan)\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n6.10.2.3 Computing fixed distance weight matrix\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\nwm62_lw &lt;- nb2listw(wm_d62, style = 'B')\nsummary(wm62_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \nLink number distribution:\n\n 1  2  3  4  5  6 \n 6 15 14 26 20  7 \n6 least connected regions:\n6 15 30 32 56 65 with 1 link\n7 most connected regions:\n21 28 35 45 50 52 82 with 6 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1   S2\nB 88 7744 324 648 5440\n\n\nThe output spatial weights object is called wm62_lw.\n\n\n\n6.10.3 Computing adaptive distance weight matrix\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\nknn &lt;- knn2nb(knearneigh(coords, k=8))\nknn\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\nknn_lw &lt;- nb2listw(knn, style = 'B')\nsummary(knn_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n 8 \n88 \n88 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n88 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 704 1300 23014"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#computing-gi-statistics",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#computing-gi-statistics",
    "title": "Hands-On Exercise 6: Global & Local Measures of Spatial Autocorrelation",
    "section": "6.11 Computing Gi statistics",
    "text": "6.11 Computing Gi statistics\n\n6.11.1 Gi statistics using fixed distance\n\nfips &lt;- order(hunan$County)\ngi.fixed &lt;- localG(hunan$GDPPC, wm62_lw)\ngi.fixed\n\n [1]  0.436075843 -0.265505650 -0.073033665  0.413017033  0.273070579\n [6] -0.377510776  2.863898821  2.794350420  5.216125401  0.228236603\n[11]  0.951035346 -0.536334231  0.176761556  1.195564020 -0.033020610\n[16]  1.378081093 -0.585756761 -0.419680565  0.258805141  0.012056111\n[21] -0.145716531 -0.027158687 -0.318615290 -0.748946051 -0.961700582\n[26] -0.796851342 -1.033949773 -0.460979158 -0.885240161 -0.266671512\n[31] -0.886168613 -0.855476971 -0.922143185 -1.162328599  0.735582222\n[36] -0.003358489 -0.967459309 -1.259299080 -1.452256513 -1.540671121\n[41] -1.395011407 -1.681505286 -1.314110709 -0.767944457 -0.192889342\n[46]  2.720804542  1.809191360 -1.218469473 -0.511984469 -0.834546363\n[51] -0.908179070 -1.541081516 -1.192199867 -1.075080164 -1.631075961\n[56] -0.743472246  0.418842387  0.832943753 -0.710289083 -0.449718820\n[61] -0.493238743 -1.083386776  0.042979051  0.008596093  0.136337469\n[66]  2.203411744  2.690329952  4.453703219 -0.340842743 -0.129318589\n[71]  0.737806634 -1.246912658  0.666667559  1.088613505 -0.985792573\n[76]  1.233609606 -0.487196415  1.626174042 -1.060416797  0.425361422\n[81] -0.837897118 -0.314565243  0.371456331  4.424392623 -0.109566928\n[86]  1.364597995 -1.029658605 -0.718000620\nattr(,\"internals\")\n               Gi      E(Gi)        V(Gi)        Z(Gi) Pr(z != E(Gi))\n [1,] 0.064192949 0.05747126 2.375922e-04  0.436075843   6.627817e-01\n [2,] 0.042300020 0.04597701 1.917951e-04 -0.265505650   7.906200e-01\n [3,] 0.044961480 0.04597701 1.933486e-04 -0.073033665   9.417793e-01\n [4,] 0.039475779 0.03448276 1.461473e-04  0.413017033   6.795941e-01\n [5,] 0.049767939 0.04597701 1.927263e-04  0.273070579   7.847990e-01\n [6,] 0.008825335 0.01149425 4.998177e-05 -0.377510776   7.057941e-01\n [7,] 0.050807266 0.02298851 9.435398e-05  2.863898821   4.184617e-03\n [8,] 0.083966739 0.04597701 1.848292e-04  2.794350420   5.200409e-03\n [9,] 0.115751554 0.04597701 1.789361e-04  5.216125401   1.827045e-07\n[10,] 0.049115587 0.04597701 1.891013e-04  0.228236603   8.194623e-01\n[11,] 0.045819180 0.03448276 1.420884e-04  0.951035346   3.415864e-01\n[12,] 0.049183846 0.05747126 2.387633e-04 -0.536334231   5.917276e-01\n[13,] 0.048429181 0.04597701 1.924532e-04  0.176761556   8.596957e-01\n[14,] 0.034733752 0.02298851 9.651140e-05  1.195564020   2.318667e-01\n[15,] 0.011262043 0.01149425 4.945294e-05 -0.033020610   9.736582e-01\n[16,] 0.065131196 0.04597701 1.931870e-04  1.378081093   1.681783e-01\n[17,] 0.027587075 0.03448276 1.385862e-04 -0.585756761   5.580390e-01\n[18,] 0.029409313 0.03448276 1.461397e-04 -0.419680565   6.747188e-01\n[19,] 0.061466754 0.05747126 2.383385e-04  0.258805141   7.957856e-01\n[20,] 0.057656917 0.05747126 2.371303e-04  0.012056111   9.903808e-01\n[21,] 0.066518379 0.06896552 2.820326e-04 -0.145716531   8.841452e-01\n[22,] 0.045599896 0.04597701 1.928108e-04 -0.027158687   9.783332e-01\n[23,] 0.030646753 0.03448276 1.449523e-04 -0.318615290   7.500183e-01\n[24,] 0.035635552 0.04597701 1.906613e-04 -0.748946051   4.538897e-01\n[25,] 0.032606647 0.04597701 1.932888e-04 -0.961700582   3.362000e-01\n[26,] 0.035001352 0.04597701 1.897172e-04 -0.796851342   4.255374e-01\n[27,] 0.012746354 0.02298851 9.812587e-05 -1.033949773   3.011596e-01\n[28,] 0.061287917 0.06896552 2.773884e-04 -0.460979158   6.448136e-01\n[29,] 0.014277403 0.02298851 9.683314e-05 -0.885240161   3.760271e-01\n[30,] 0.009622875 0.01149425 4.924586e-05 -0.266671512   7.897221e-01\n[31,] 0.014258398 0.02298851 9.705244e-05 -0.886168613   3.755267e-01\n[32,] 0.005453443 0.01149425 4.986245e-05 -0.855476971   3.922871e-01\n[33,] 0.043283712 0.05747126 2.367109e-04 -0.922143185   3.564539e-01\n[34,] 0.020763514 0.03448276 1.393165e-04 -1.162328599   2.451020e-01\n[35,] 0.081261843 0.06896552 2.794398e-04  0.735582222   4.619850e-01\n[36,] 0.057419907 0.05747126 2.338437e-04 -0.003358489   9.973203e-01\n[37,] 0.013497133 0.02298851 9.624821e-05 -0.967459309   3.333145e-01\n[38,] 0.019289310 0.03448276 1.455643e-04 -1.259299080   2.079223e-01\n[39,] 0.025996272 0.04597701 1.892938e-04 -1.452256513   1.464303e-01\n[40,] 0.016092694 0.03448276 1.424776e-04 -1.540671121   1.233968e-01\n[41,] 0.035952614 0.05747126 2.379439e-04 -1.395011407   1.630124e-01\n[42,] 0.031690963 0.05747126 2.350604e-04 -1.681505286   9.266481e-02\n[43,] 0.018750079 0.03448276 1.433314e-04 -1.314110709   1.888090e-01\n[44,] 0.015449080 0.02298851 9.638666e-05 -0.767944457   4.425202e-01\n[45,] 0.065760689 0.06896552 2.760533e-04 -0.192889342   8.470456e-01\n[46,] 0.098966900 0.05747126 2.326002e-04  2.720804542   6.512325e-03\n[47,] 0.085415780 0.05747126 2.385746e-04  1.809191360   7.042128e-02\n[48,] 0.038816536 0.05747126 2.343951e-04 -1.218469473   2.230456e-01\n[49,] 0.038931873 0.04597701 1.893501e-04 -0.511984469   6.086619e-01\n[50,] 0.055098610 0.06896552 2.760948e-04 -0.834546363   4.039732e-01\n[51,] 0.033405005 0.04597701 1.916312e-04 -0.908179070   3.637836e-01\n[52,] 0.043040784 0.06896552 2.829941e-04 -1.541081516   1.232969e-01\n[53,] 0.011297699 0.02298851 9.615920e-05 -1.192199867   2.331829e-01\n[54,] 0.040968457 0.05747126 2.356318e-04 -1.075080164   2.823388e-01\n[55,] 0.023629663 0.04597701 1.877170e-04 -1.631075961   1.028743e-01\n[56,] 0.006281129 0.01149425 4.916619e-05 -0.743472246   4.571958e-01\n[57,] 0.063918654 0.05747126 2.369553e-04  0.418842387   6.753313e-01\n[58,] 0.070325003 0.05747126 2.381374e-04  0.832943753   4.048765e-01\n[59,] 0.025947288 0.03448276 1.444058e-04 -0.710289083   4.775249e-01\n[60,] 0.039752578 0.04597701 1.915656e-04 -0.449718820   6.529132e-01\n[61,] 0.049934283 0.05747126 2.334965e-04 -0.493238743   6.218439e-01\n[62,] 0.030964195 0.04597701 1.920248e-04 -1.083386776   2.786368e-01\n[63,] 0.058129184 0.05747126 2.343319e-04  0.042979051   9.657182e-01\n[64,] 0.046096514 0.04597701 1.932637e-04  0.008596093   9.931414e-01\n[65,] 0.012459080 0.01149425 5.008051e-05  0.136337469   8.915545e-01\n[66,] 0.091447733 0.05747126 2.377744e-04  2.203411744   2.756574e-02\n[67,] 0.049575872 0.02298851 9.766513e-05  2.690329952   7.138140e-03\n[68,] 0.107907212 0.04597701 1.933581e-04  4.453703219   8.440175e-06\n[69,] 0.019616151 0.02298851 9.789454e-05 -0.340842743   7.332220e-01\n[70,] 0.032923393 0.03448276 1.454032e-04 -0.129318589   8.971056e-01\n[71,] 0.030317663 0.02298851 9.867859e-05  0.737806634   4.606320e-01\n[72,] 0.019437582 0.03448276 1.455870e-04 -1.246912658   2.124295e-01\n[73,] 0.055245460 0.04597701 1.932838e-04  0.666667559   5.049845e-01\n[74,] 0.074278054 0.05747126 2.383538e-04  1.088613505   2.763244e-01\n[75,] 0.013269580 0.02298851 9.719982e-05 -0.985792573   3.242349e-01\n[76,] 0.049407829 0.03448276 1.463785e-04  1.233609606   2.173484e-01\n[77,] 0.028605749 0.03448276 1.455139e-04 -0.487196415   6.261191e-01\n[78,] 0.039087662 0.02298851 9.801040e-05  1.626174042   1.039126e-01\n[79,] 0.031447120 0.04597701 1.877464e-04 -1.060416797   2.889550e-01\n[80,] 0.064005294 0.05747126 2.359641e-04  0.425361422   6.705732e-01\n[81,] 0.044606529 0.05747126 2.357330e-04 -0.837897118   4.020885e-01\n[82,] 0.063700493 0.06896552 2.801427e-04 -0.314565243   7.530918e-01\n[83,] 0.051142205 0.04597701 1.933560e-04  0.371456331   7.102977e-01\n[84,] 0.102121112 0.04597701 1.610278e-04  4.424392623   9.671399e-06\n[85,] 0.021901462 0.02298851 9.843172e-05 -0.109566928   9.127528e-01\n[86,] 0.064931813 0.04597701 1.929430e-04  1.364597995   1.723794e-01\n[87,] 0.031747344 0.04597701 1.909867e-04 -1.029658605   3.031703e-01\n[88,] 0.015893319 0.02298851 9.765131e-05 -0.718000620   4.727569e-01\nattr(,\"cluster\")\n [1] Low  Low  High High High High High High High Low  Low  High Low  Low  Low \n[16] High High High High Low  High High Low  Low  High Low  Low  Low  Low  Low \n[31] Low  Low  Low  High Low  Low  Low  Low  Low  Low  High Low  Low  Low  Low \n[46] High High Low  Low  Low  Low  High Low  Low  Low  Low  Low  High Low  Low \n[61] Low  Low  Low  High High High Low  High Low  Low  High Low  High High Low \n[76] High Low  Low  Low  Low  Low  Low  High High Low  High Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = hunan$GDPPC, listw = wm62_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n\nThe output of localG() is a vector of G or Gstar values, with attributes “gstari” set to TRUE or FALSE, “call” set to the function call, and class “localG”.\nThe Gi statistics is represented as a Z-score. Greater values represent a greater intensity of clustering and the direction (positive or negative) indicates high or low clusters.\nNext, we will join the Gi values to their corresponding hunan sf data frame by using the code chunk below.\n\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.fixed)) %&gt;%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\nIn fact, the code chunk above performs three tasks. First, it convert the output vector (i.e. gi.fixed) into r matrix object by using as.matrix(). Next, cbind() is used to join hunan@data and gi.fixed matrix to produce a new SpatialPolygonDataFrame called hunan.gi. Lastly, the field name of the gi values is renamed to gstat_fixed by using rename().\n\n\n6.11.2 Mapping Gi values with fixed distance weights\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nGimap &lt;-tm_shape(hunan.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"local Gi\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, Gimap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\nQuestion: What statistical observation can you draw from the Gi map above?\n\n\n\n6.11.3 Gi statistics using adaptive distance\nThe code chunk below are used to compute the Gi values for GDPPC2012 by using an adaptive distance weight matrix (i.e knb_lw).\n\nfips &lt;- order(hunan$County)\ngi.adaptive &lt;- localG(hunan$GDPPC, knn_lw)\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.adaptive)) %&gt;%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\n\n\n6.11.4 Mapping Gi values with adaptive distance weights\nIt is time for us to visualise the locations of hot spot and cold spot areas. The choropleth mapping functions of tmap package will be used to map the Gi values.\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\ngdppc&lt;- qtm(hunan, \"GDPPC\")\n\nGimap &lt;- tm_shape(hunan.gi) + \n  tm_fill(col = \"gstat_adaptive\", \n          style = \"pretty\", \n          palette=\"-RdBu\", \n          title = \"local Gi\") + \n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, \n             Gimap, \n             asp=1, \n             ncol=2)\n\n\n\n\n\n\n\n\n\nQuestion: What statistical observation can you draw from the Gi map above?"
  },
  {
    "objectID": "Test/test.html",
    "href": "Test/test.html",
    "title": "Georgia's IS415 Experience",
    "section": "",
    "text": "pacman::p_load(sf, tmap, tidyverse, sfdep, ggplot2, RColorBrewer)\n\n\nkml_data &lt;- st_read(\"data/DengueMosquitoBreedingHabitats/Dengue Mosquito Breeding Habitats.kml\")\n\nReading layer `BREEDINGHABITAT_CENTRAL_AREA' from data source \n  `/Users/georgiaxng/georgiaxng/is415-handson/Test/data/DengueMosquitoBreedingHabitats/Dengue Mosquito Breeding Habitats.kml' \n  using driver `KML'\nSimple feature collection with 25 features and 2 fields\nGeometry type: POLYGON\nDimension:     XYZ\nBounding box:  xmin: 103.797 ymin: 1.270384 xmax: 103.8887 ymax: 1.400613\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(kml_data) +\n  tm_polygons()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "Data Preparation, Preprocessing\n\n\n\n\nHere, we have loaded the following packages:\n\npacman::p_load(sf, sfdep, tmap, tidyverse, RColorBrewer, ggplot2, spatstat, jsonlite, units, matrixStats, httr)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#my-responsibilities",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#my-responsibilities",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "Data Preparation, Preprocessing"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#importing-packages",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#importing-packages",
    "title": "Take Home Exercise 3",
    "section": "",
    "text": "Here, we have loaded the following packages:\n\npacman::p_load(sf, sfdep, tmap, tidyverse, RColorBrewer, ggplot2, spatstat, jsonlite, units, matrixStats, httr)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#importing-geospatial-data",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#importing-geospatial-data",
    "title": "Take Home Exercise 3",
    "section": "2.1 Importing Geospatial Data",
    "text": "2.1 Importing Geospatial Data\n\n2.1.1 Importing Singapore Subzone Boundaries\nThe code chunk below is used to import MP_SUBZONE_WEB_PL shapefile by using st_read() of sfpackages.\n\nmpsz_sf &lt;- st_read(dsn = \"data/geospatial/MasterPlan2014SubzoneBoundaryWebSHP/\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/georgiaxng/georgiaxng/is415-handson/Take-home_Ex/Take-home_Ex03/data/geospatial/MasterPlan2014SubzoneBoundaryWebSHP' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\nwrite_rds(mpsz_sf, 'data/rds/mpsz_sf.rds')\n\nUsing st_crs, we can check the coordinate system.\n\nst_crs(mpsz_sf)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\n\n2.1.1.1 Checking Validity of Geometries\nUsing st_is_valid, we can check to see whether all the polygons are valid or not. From the results, we can see a total of 9 not valid.\n\n# checks for the number of geometries that are invalid\nlength(which(st_is_valid(mpsz_sf) == FALSE))\n\n[1] 9\n\n\nTo rectify this, we can use st_make_valid() to correct these invalid geometries as demonstrated in the code chunk below.\n\nmpsz_sf &lt;- st_make_valid(mpsz_sf)\nlength(which(st_is_valid(mpsz_sf) == FALSE))\n\n[1] 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.1.2 Importing Kindergartens\nThis chunk of code imports the kindergartens data.\n\nkindergarten_json &lt;- fromJSON(\"data/geospatial/kindergartens.json\")\n\nkindergarten_cleaned &lt;- kindergarten_json$SrchResults[-1, ]\n\nkindergarten_df &lt;- data.frame(\n  NAME = kindergarten_cleaned$NAME,\n  latitude = sapply(kindergarten_cleaned$LatLng, function(x) as.numeric(unlist(strsplit(x, \",\"))[1])),\n  longitude = sapply(kindergarten_cleaned$LatLng, function(x) as.numeric(unlist(strsplit(x, \",\"))[2]))\n)\n\nkindergarten_sf &lt;- kindergarten_df %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs=4326) %&gt;%\n  st_transform(crs = 3414)\n\n\n\n2.1.3 Importing Childcare\nThis chunk of code imports the childcare data.\n\nchildcare_json &lt;- fromJSON(\"data/geospatial/childcare.json\")\n\nchildcare_cleaned &lt;- childcare_json$SrchResults[-1, ]\n\nchildcare_df &lt;- data.frame(\n  NAME = childcare_cleaned$NAME,\n  latitude = sapply(childcare_cleaned$LatLng, function(x) as.numeric(unlist(strsplit(x, \",\"))[1])),\n  longitude = sapply(childcare_cleaned$LatLng, function(x) as.numeric(unlist(strsplit(x, \",\"))[2]))\n)\n\nchildcare_sf &lt;- childcare_df %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs=4326) %&gt;%\n  st_transform(crs = 3414)\n\n\n\n2.1.4 Importing Hawker Centre\nSimilarly here, we will use st_read to read the geojson information, however since the columns values are in the format of html code of ’\n\n\n’ etc we will need to use a function to apply a regex expression in order to extract the name of the hawker centres.\n\nhawker_sf &lt;- st_read('data/geospatial/HawkerCentresGEOJSON.geojson')\n\nReading layer `HawkerCentresGEOJSON' from data source \n  `/Users/georgiaxng/georgiaxng/is415-handson/Take-home_Ex/Take-home_Ex03/data/geospatial/HawkerCentresGEOJSON.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 125 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6974 ymin: 1.272716 xmax: 103.9882 ymax: 1.449017\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n# Function to extract name from description\nextract_name &lt;- function(description) {\n  if (!is.na(description)) {\n    # Use regular expression to extract the NAME \n    name &lt;- sub(\".*&lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;(.*?)&lt;/td&gt;.*\", \"\\\\1\", description)\n    if (name == description) {\n      return(NA)  # Return NA if no match is found\n    }\n    return(name)\n  } else {\n    return(NA) \n  }\n}\n\n# Apply the extraction function to every row\nhawker_sf &lt;- hawker_sf %&gt;%\n  mutate(Name = sapply(Description, extract_name)) %&gt;% select (-Description)\n\nHere, we can see that the hawker centres are now appropriately named.\n\n\nSimple feature collection with 6 features and 1 field\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.7798 ymin: 1.284425 xmax: 103.9048 ymax: 1.449017\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n                             Name                      geometry\n1     Market Street Hawker Centre POINT Z (103.8502 1.284425 0)\n2    Marsiling Mall Hawker Centre POINT Z (103.7798 1.433539 0)\n3    Margaret Drive Hawker Centre POINT Z (103.8047 1.297486 0)\n4 Fernvale Hawker Centre & Market POINT Z (103.8771 1.391592 0)\n5       One Punggol Hawker Centre  POINT Z (103.9048 1.40819 0)\n6    Bukit Canberra Hawker Centre POINT Z (103.8225 1.449017 0)\n\n\nAs shown above, we can see that the geographic coordinate system for the hawker dataset is in WGS84 and has XYZ coordinates, among which contains the Z-coordinates we do not need. Thus, we can use st_zm() to remove the Z-coordinate and project it to the SVY21 coordiate system using st_transform().\n\nhawker_sf &lt;- st_zm(hawker_sf) %&gt;%\n  st_transform(crs = 3414)\n\nhead(hawker_sf)\n\nSimple feature collection with 6 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 22042.51 ymin: 29650.7 xmax: 35955.52 ymax: 47850.43\nProjected CRS: SVY21 / Singapore TM\n                             Name                  geometry\n1     Market Street Hawker Centre  POINT (29874.82 29650.7)\n2    Marsiling Mall Hawker Centre POINT (22042.51 46139.03)\n3    Margaret Drive Hawker Centre  POINT (24816.7 31094.91)\n4 Fernvale Hawker Centre & Market  POINT (32867.9 41500.77)\n5       One Punggol Hawker Centre POINT (35955.52 43336.13)\n6    Bukit Canberra Hawker Centre POINT (26794.39 47850.43)\n\n\n\n\n2.1.5 Importing Bus Stops\nHere we are importing the bus stop locations using st_read and also converting it to the SVY21 coordinate system.\n\nbusstop_sf &lt;- st_read(dsn = \"data/geospatial/BusStopLocation_Jul2024/\", layer = \"BusStop\")%&gt;%\n  st_transform(crs = 3414)\n\nReading layer `BusStop' from data source \n  `/Users/georgiaxng/georgiaxng/is415-handson/Take-home_Ex/Take-home_Ex03/data/geospatial/BusStopLocation_Jul2024' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5166 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48285.52 ymax: 52983.82\nProjected CRS: SVY21\n\n\n\n\n2.1.6 Importing Shopping Malls\nHere we are importing the shopping mall locations using read_csv and also converting it to the SVY21 coordinate system.\n\nshoppingmall_sf &lt;- read_csv('data/geospatial/shopping_mall_coordinates.csv') %&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs=4326) %&gt;%\n  st_transform(crs = 3414)\n\n\n\n2.1.7 Importing MRT\nHere we are importing the mrt locations using st_read.\n\nmrt_sf &lt;- st_read(dsn = \"data/geospatial/TrainStation_Jul2024/\", layer = \"RapidTransitSystemStation\")\n\nReading layer `RapidTransitSystemStation' from data source \n  `/Users/georgiaxng/georgiaxng/is415-handson/Take-home_Ex/Take-home_Ex03/data/geospatial/TrainStation_Jul2024' \n  using driver `ESRI Shapefile'\nSimple feature collection with 230 features and 5 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 6068.209 ymin: 27478.44 xmax: 45377.5 ymax: 47913.58\nProjected CRS: SVY21\n\n\nHaving imported the dataset, we will now need to check for both invalid geometries and NA values before proceeding. The chunk of code below detects not only these but also resolves it. The final printed result shows that all geometries are now valid.\n\n# Check for invalid geometries and NA values\nvalidity_checks &lt;- st_is_valid(mrt_sf, reason = TRUE)\n\n# Identify indices with NA\nna_indices &lt;- which(is.na(validity_checks))\n\n# Filter out rows with NA values from the mrt object\nmrt_sf &lt;- mrt_sf[-na_indices, ]\n\n# Verify the mrt object no longer contains invalid geometries\nany(is.na(sf::st_is_valid(mrt_sf)))\n\n[1] FALSE\n\n\nHere we use st_transform() to convert it to the SVY21 Coordinates System of CRS code 3414.\n\nmrt_sf &lt;- mrt_sf %&gt;%\n  st_transform(crs = 3414)\n\n\n\n2.1.8 Importing Primary School\nThis chunk of code imports the primary school dataset from data.gov.sg and uses the select() function to select the relevant columns through the input of the column numbers.\n\nprimarysch_df = read_csv('data/geospatial/Generalinformationofschools.csv') %&gt;% filter(mainlevel_code =='PRIMARY') %&gt;% select(1,3,4)\n\n\n2.1.8.1 Geocoding Primary School Data using OneMap API\nSince this dataset only has the addresses and not the actual coordinates, we will need to use the OneMapAPI to geocode these addresses. This chunk of code contains a function whereby the OneMapApi is called upon and returns the actual latitude and longitude of the addresses inputted.\n\n\nClick to view the code\ngeocode &lt;- function(address, postal) {\n  base_url &lt;- \"https://www.onemap.gov.sg/api/common/elastic/search\"\n  query &lt;- list(\"searchVal\" = address,\n                \"postal\" = postal,\n                \"returnGeom\" = \"Y\",\n                \"getAddrDetails\" = \"N\",\n                \"pageNum\" = \"1\")\n  \n  res &lt;- GET(base_url, query = query)\n  restext&lt;-content(res, as=\"text\")\n  \n  output &lt;- fromJSON(restext)  %&gt;% \n    as.data.frame %&gt;%\n    select(results.LATITUDE, results.LONGITUDE)\n\n  return(output)\n}\n\n\nThis chunk of code creates two columns for latitude and longitude and sets the default values to 0. Then it loops through every single row of the primary school dataset and calls upon the above function to populate the respective latitude and longitude values for each row.\n\n\nClick to view the code\nprimarysch_df$LATITUDE &lt;- 0\nprimarysch_df$LONGITUDE &lt;- 0\n\nfor (i in 1:nrow(primarysch_df)){\n  temp_output &lt;- geocode(primarysch_df[i, 2], primarysch_df[i, 3])\n  print(i)\n  \n  primarysch_df$LATITUDE[i] &lt;- temp_output$results.LATITUDE\n  primarysch_df$LONGITUDE[i] &lt;- temp_output$results.LONGITUDE\n}\nwrite_rds(primarysch_df, 'data/rds/geocoded_primarysch.rds')\n\n\nAs shown below, using head() we can see that the new columns for lat and long has been added with the values fetched using the OneMap API.\n\nglimpse(primarysch_df)\n\nRows: 178\nColumns: 3\n$ school_name &lt;chr&gt; \"ADMIRALTY PRIMARY SCHOOL\", \"AHMAD IBRAHIM PRIMARY SCHOOL\"…\n$ address     &lt;chr&gt; \"11   WOODLANDS CIRCLE\", \"10   YISHUN STREET 11\", \"100  Br…\n$ postal_code &lt;dbl&gt; 738907, 768643, 579646, 159016, 544969, 569785, 569920, 22…\n\n\nUsing read_rds, we can access the already processed and geocoded data from rds without needing to run through the geocoding function again. Since the data is in the WGS coordinate system, we can use st_transform() to project it to the SVY21 coordinate system we will be using.\n\nprimarysch_df &lt;- read_rds('data/rds/geocoded_primarysch.rds')\nprimarysch_sf &lt;- primarysch_df %&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs=4326) %&gt;%\n  st_transform(crs = 3414)\n\n\n\n\n2.1.9 Inferring CBD\nFinally, let us factor in the proximity to the Central Business District - in the Downtown Core. For this, let us take the coordinates of Downtown Core to be the coordinates of the CBD:\n\nlat &lt;- 1.287953\nlng &lt;- 103.851784\n\ncbd_sf &lt;- data.frame(lat, lng) %&gt;%\n  st_as_sf(coords = c(\"lng\", \"lat\"), crs=4326) %&gt;%\n  st_transform(crs=3414)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#importing-aspatial-data",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#importing-aspatial-data",
    "title": "Take Home Exercise 3",
    "section": "2.2 Importing Aspatial Data",
    "text": "2.2 Importing Aspatial Data\n\n2.2.1 Importing Rental Flat\nThe code chunk below is used to import the rental data from data.gov.sg.\n\nrental_df = read_csv('data/aspatial/RentingOutofFlats2024CSV.csv')\n\nTo get a brief overview of existing columns of this dataset, we can use colnames() to do so.\n\ncolnames(rental_df)\n\n[1] \"rent_approval_date\" \"town\"               \"block\"             \n[4] \"street_name\"        \"flat_type\"          \"monthly_rent\"      \n\n\n\n2.2.1.1 Converting rent_approval_date to a Valid Date Format\nSince the rent_approval_date is in the chr format, we will want to convert it to the date format so that we can later better access and use this variable. This is done so by the ym() as shown in the chunk of code below.\n\nrental_df$rent_approval_date &lt;- ym(rental_df$rent_approval_date)\n\n\n\n2.2.1.2 Filtering For 2024\nSince the dataset is rather large, we want to size down our scope and instead focus on only the 2024 data, which in this case is from Jan 2024 to Sep 2024.\n\nrental_df &lt;- rental_df %&gt;%\n  filter(year(rent_approval_date) == 2024)\n\n\n\n2.2.1.3 Geocoding Rental Flat Data Using OneMap API\nLike the primary school data, we face the similar problem here thus we will need to go through the geocoding process similarly to what we have done above. The geocoding function:\n\n\nClick to view the code\ngeocode &lt;- function(block, streetname) {\n  base_url &lt;- \"https://www.onemap.gov.sg/api/common/elastic/search\"\n  address &lt;- paste(block, streetname, sep = \" \")\n  query &lt;- list(\"searchVal\" = address, \n                \"returnGeom\" = \"Y\",\n                \"getAddrDetails\" = \"N\",\n                \"pageNum\" = \"1\")\n  \n  res &lt;- GET(base_url, query = query)\n  restext&lt;-content(res, as=\"text\")\n  \n  output &lt;- fromJSON(restext)  %&gt;% \n    as.data.frame %&gt;%\n    select(results.LATITUDE, results.LONGITUDE)\n\n  return(output)\n}\n\n\nThis chunk of code then calls upon the above function for every single row of the rental_df and writes it to the rds.\n\nrental_df$LATITUDE &lt;- 0\nrental_df$LONGITUDE &lt;- 0\n\nfor (i in 1:nrow(rental_df)){\n  temp_output &lt;- geocode(rental_df[i, 3], rental_df[i, 4])\n  print(i)\n  \n  rental_df$LATITUDE[i] &lt;- temp_output$results.LATITUDE\n  rental_df$LONGITUDE[i] &lt;- temp_output$results.LONGITUDE\n}\nwrite_rds(rental_df, 'data/rds/geocoded_rental_2024.rds')\n\nWithout needing to run the above time-consuming method yet again, we can just read the data from the rds here.\n\nrental_df &lt;- read_rds('data/rds/geocoded_rental_2024.rds')\n\n\n\n2.2.1.4 CRS Adjustments\nAnother important step after importing the dataset is checking the coordinate system used, as seen in the result below using st_crs(), we can see that there is no CRS stated for rental_df.\n\nst_crs(rental_df)\n\nCoordinate Reference System: NA\n\n\nTherefore, we need to convert the longitude and latitude columns into a spatial format. Since our dataset is based in Singapore and it uses the SVY21 coordinate reference system (CRS Code: 3414), we will use the st_transform() function to perform the conversion and create the geometry column.\n\nrental_sf &lt;- rental_df %&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs=4326) %&gt;%\n  st_transform(crs = 3414)\n\nUsing st_crs(), we can check and verify that the conversion is successful.\n\nst_crs(rental_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\nSimple feature collection with 6 features and 6 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 15786.61 ymin: 30769.77 xmax: 39668.39 ymax: 45634.94\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 6 × 7\n  rent_approval_date town            block street_name    flat_type monthly_rent\n  &lt;date&gt;             &lt;chr&gt;           &lt;chr&gt; &lt;chr&gt;          &lt;chr&gt;            &lt;dbl&gt;\n1 2024-01-01         YISHUN          386   YISHUN RING RD 4-ROOM            3700\n2 2024-01-01         JURONG WEST     140B  CORPORATION DR 4-ROOM            3900\n3 2024-01-01         SENGKANG        471B  FERNVALE ST    5-ROOM            3700\n4 2024-01-01         KALLANG/WHAMPOA 10    GLOUCESTER RD  3-ROOM            3600\n5 2024-01-01         BEDOK           31    BEDOK STH AVE… 5-ROOM            4350\n6 2024-01-01         QUEENSTOWN      82    STRATHMORE AVE 4-ROOM            3000\n# ℹ 1 more variable: geometry &lt;POINT [m]&gt;\n\n\n\n\n2.2.1.5 Checking for NA values\nThis chunk of code checks the dataset for any na values in all of the columns. As shown below, there is none.\n\nrental_sf %&gt;%\n  summarise(across(everything(), ~ sum(is.na(.)))) -&gt; extra_NA \nextra_NA\n\nSimple feature collection with 1 feature and 6 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: 11519.15 ymin: 28097.64 xmax: 45192.3 ymax: 48741.06\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 1 × 7\n  rent_approval_date  town block street_name flat_type monthly_rent\n               &lt;int&gt; &lt;int&gt; &lt;int&gt;       &lt;int&gt;     &lt;int&gt;        &lt;int&gt;\n1                  0     0     0           0         0            0\n# ℹ 1 more variable: geometry &lt;MULTIPOINT [m]&gt;"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#removal-of-redundant-columns",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#removal-of-redundant-columns",
    "title": "Take Home Exercise 3",
    "section": "3.1 Removal of Redundant Columns",
    "text": "3.1 Removal of Redundant Columns\nTo increase efficiency and reduce the data size, we can remove columns we do not need like the block and street_name in which we have already utilised previously and now have no use for.\n\n# Define columns to be removed\ncolumns_to_remove &lt;- c(\"block\",\"street_name\")\n\n# Remove columns only if they exist in the dataframe\nrental_sf &lt;- rental_sf %&gt;%\n  dplyr::select(-all_of(columns_to_remove[columns_to_remove %in% names(rental_sf)]))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#filter-by-flat-type",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#filter-by-flat-type",
    "title": "Take Home Exercise 3",
    "section": "3.2 Filter By Flat Type",
    "text": "3.2 Filter By Flat Type\nLet us get an overview of the distributions of the housing types. As shown in the histogram, we can see that there is significantly less data for flat types like 1-room, 2-room, and executive housing.\n\n# Create a summary of counts for each remaining lease range\ncount_data &lt;- rental_sf %&gt;%\n  group_by(flat_type) %&gt;%\n  summarise(count = n())\n\n# Create the bar plot with labels\nggplot(count_data, aes(x = flat_type, y = count)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") + \n  geom_text(aes(label = count), vjust = -0.5, size = 4) +  # Add labels on top of the bars\n  labs(title = \"Count of Flat Type\",\n       x = \"Flat Type\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nHence, we will focus on analyzing the 3-room, 4-room, and 5-room flats since they show a more substantial presence in the dataset compared to smaller flat types.\n\nrental_sf &lt;- rental_sf %&gt;% filter (flat_type == '3-ROOM' | flat_type == '4-ROOM' |flat_type == '5-ROOM' )"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#adding-region-to-rental-data",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#adding-region-to-rental-data",
    "title": "Take Home Exercise 3",
    "section": "3.3 Adding Region to Rental Data",
    "text": "3.3 Adding Region to Rental Data\nThis chunk of code performs a left join with mpsz_sfto categorise the different flats into different regions in order to better understand the rental trends.\n\n3.3.1 Left Joining with mpsz_sf\n\n# Perform the left join by dropping the geometry from 'datab' and only bringing in 'region_n'\nrental_sf &lt;- rental_sf %&gt;%\n  left_join(st_drop_geometry(mpsz_sf) %&gt;% select(PLN_AREA_N, REGION_N) %&gt;% distinct(PLN_AREA_N, .keep_all = TRUE), \n            by = c(\"town\" = \"PLN_AREA_N\"))\n\n\n\n3.3.2 Identifying Rows with NA values\nThen, let’s perform a check to see if any of the rows have na values in the newly created column and display it. As shown here we can see that there are multiple rows in which the town column was unable to find a matching value in the mpsz_sf PLN_AREA_N column.\n\nrental_sf_with_na &lt;- rental_sf %&gt;%\n  filter(is.na(REGION_N))\n\nrental_sf_with_na\n\nSimple feature collection with 1471 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 19536.43 ymin: 28634.73 xmax: 33665.81 ymax: 41493.47\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 1,471 × 6\n   rent_approval_date town      flat_type monthly_rent            geometry\n * &lt;date&gt;             &lt;chr&gt;     &lt;chr&gt;            &lt;dbl&gt;         &lt;POINT [m]&gt;\n 1 2024-01-01         KALLANG/… 3-ROOM            3600 (30102.41 32911.75)\n 2 2024-01-01         KALLANG/… 3-ROOM            2300 (19536.43 41493.47)\n 3 2024-01-01         CENTRAL   3-ROOM            3000 (29435.92 29669.46)\n 4 2024-01-01         KALLANG/… 3-ROOM            1850 (19536.43 41493.47)\n 5 2024-01-01         KALLANG/… 4-ROOM            3400 (31184.91 34078.85)\n 6 2024-01-01         KALLANG/… 5-ROOM            4100  (31013.62 33175.3)\n 7 2024-01-01         KALLANG/… 3-ROOM            3200 (31618.57 33708.83)\n 8 2024-01-01         KALLANG/… 3-ROOM            2400 (31228.06 33432.22)\n 9 2024-01-01         CENTRAL   3-ROOM            2850 (29067.24 29360.52)\n10 2024-01-01         KALLANG/… 3-ROOM            2700 (31547.37 31835.93)\n# ℹ 1,461 more rows\n# ℹ 1 more variable: REGION_N &lt;chr&gt;\n\n\nUsing unique, we can identify the town values of these problematic rows and also the available regions in mpsz_sf so that we have a brief idea of what are the possible values we can later use. In particular, the problematic values are ‘Kallang/Whampoa’ and ‘Central’.\n\nunique (rental_sf_with_na$town)\n\n[1] \"KALLANG/WHAMPOA\" \"CENTRAL\"        \n\nunique(mpsz_sf$REGION_N)\n\n[1] \"CENTRAL REGION\"    \"WEST REGION\"       \"EAST REGION\"      \n[4] \"NORTH-EAST REGION\" \"NORTH REGION\"     \n\n\nSince the value is Kallang/Whampoa, let’s try to find the region of either Kallang or Whampoa through the filter()` function.\n\ntest &lt;- mpsz_sf%&gt;% filter(PLN_AREA_N == 'KALLANG' | PLN_AREA_N == 'WHAMPOA') %&gt;% select(PLN_AREA_N,REGION_N)\ntest\n\nSimple feature collection with 9 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 29224.85 ymin: 30694.74 xmax: 33809.38 ymax: 34738.73\nProjected CRS: SVY21\n  PLN_AREA_N       REGION_N                       geometry\n1    KALLANG CENTRAL REGION POLYGON ((31632.98 30741.73...\n2    KALLANG CENTRAL REGION POLYGON ((31915.99 31851.24...\n3    KALLANG CENTRAL REGION POLYGON ((31494.25 32088.46...\n4    KALLANG CENTRAL REGION POLYGON ((32710.73 33608.07...\n5    KALLANG CENTRAL REGION POLYGON ((31277.37 34723.29...\n6    KALLANG CENTRAL REGION POLYGON ((31389.56 32098.17...\n7    KALLANG CENTRAL REGION POLYGON ((30344.12 32879.39...\n8    KALLANG CENTRAL REGION POLYGON ((32160.87 32549.12...\n9    KALLANG CENTRAL REGION POLYGON ((31437.02 32345.27...\n\n\nWhile we can’t find a match for Whampoa, we can see that Kallang falls under the Central Region. From the naming, we can also make the deduction that the town ‘Central’ likely falls under the same region. Thus by using a if_else statement we can assign the region Central Region to these towns.\n\nrental_sf &lt;- rental_sf %&gt;%\n  mutate(REGION_N = if_else(town == 'CENTRAL' | town == 'KALLANG/WHAMPOA', 'CENTRAL REGION', REGION_N))\n\nLet us also rename the column to standardise the namings.\n\nrental_sf &lt;- rental_sf %&gt;% rename(region = REGION_N)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#calculate-number-of-facilities-within-a-certain-distance-proximity-to-nearest-facility",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#calculate-number-of-facilities-within-a-certain-distance-proximity-to-nearest-facility",
    "title": "Take Home Exercise 3",
    "section": "3.4 Calculate Number of Facilities Within A Certain Distance & Proximity To Nearest Facility",
    "text": "3.4 Calculate Number of Facilities Within A Certain Distance & Proximity To Nearest Facility\nSince the number of facilities within range and proximity to certain facilities are some of the most important factors of rental prices, it is important for us to include that in our analysis as well. Thus to do so we have the below function to made these calculations based on the locations of the different facilities’ datasets we have imported compared with the individual rental flats themselves.\n\n\n\n\n\n\nNote\n\n\n\nNote: the calculateNumberOffacilities is a parameter used to indicate if the calculation of facilities for a particular facility is required.\n\n\n\n\nClick to view the code\ncalculate_facilities_and_proximity &lt;- function(dataset1, dataset2, name_of_col_facilities, name_of_col_proximity, radius, calculateNumberOfFacilities) {\n  # Calculate distance matrix\n  dist_matrix &lt;- st_distance(dataset1, dataset2) %&gt;%\n    drop_units()\n  \n  if (calculateNumberOfFacilities){\n  # Calculate the number of facilities within the specified radius\n    dataset1[[name_of_col_facilities]] &lt;- rowSums(dist_matrix &lt;= radius)\n  }\n  # Calculate the proximity to the nearest facility\n  dataset1[[name_of_col_proximity]] &lt;- rowMins(dist_matrix)\n  \n  return(dataset1)\n}\n\n\nThe below chunk of code calls upon the calculate_facilities_and_proximity() based on the different parameters stated for each facility. We indicated for the mrt and primary school to not be included in the calculations for the count within a certain radius as the distance to such facilities has way more importance than the actual count of it which is usually one within a certain range since these facilities are more spread out.\n\n\nClick to view the code\nrental_sf &lt;- \n  calculate_facilities_and_proximity(\n    rental_sf, kindergarten_sf, \"no_of_kindergarten_500m\", \"prox_kindergarten\", 500, TRUE\n  ) %&gt;%\n  calculate_facilities_and_proximity(\n    ., childcare_sf, \"no_of_childcare_500m\", \"prox_childcare\", 500, TRUE\n  ) %&gt;%\n  calculate_facilities_and_proximity(\n    ., hawker_sf, \"no_of_hawker_500m\", \"prox_hawker\", 500, TRUE\n  ) %&gt;%\n  calculate_facilities_and_proximity(\n    ., busstop_sf, \"no_of_busstop_500m\", \"prox_busstop\", 500, TRUE\n  ) %&gt;%\n  calculate_facilities_and_proximity(\n    ., shoppingmall_sf, \"no_of_shoppingmall_1km\", \"prox_shoppingmall\", 1000, TRUE\n  ) %&gt;% \n  calculate_facilities_and_proximity(\n    ., mrt_sf, \"x\", \"prox_mrt\", 1000, FALSE\n  ) %&gt;%\n  calculate_facilities_and_proximity(\n    ., primarysch_sf, \"x\", \"prox_prisch\", 1000, FALSE\n  ) %&gt;%\n  calculate_facilities_and_proximity(\n    ., cbd_sf, \"x\", \"prox_cbd\", 1000, FALSE\n  )\n\n\n# Writing to RDS\nwrite_rds(rental_sf,'data/rds/rental_sf.rds')\n\n\nLikewise, to skip the whole time-consuming process, we can instead read the rds data using the below code.\n\nrental_sf &lt;- read_rds('data/rds/rental_sf.rds')\nglimpse(rental_sf)\n\nRows: 25,713\nColumns: 19\n$ rent_approval_date      &lt;date&gt; 2024-01-01, 2024-01-01, 2024-01-01, 2024-01-0…\n$ town                    &lt;chr&gt; \"YISHUN\", \"JURONG WEST\", \"SENGKANG\", \"KALLANG/…\n$ flat_type               &lt;chr&gt; \"4-ROOM\", \"4-ROOM\", \"5-ROOM\", \"3-ROOM\", \"5-ROO…\n$ monthly_rent            &lt;dbl&gt; 3700, 3900, 3700, 3600, 4350, 3000, 3800, 3600…\n$ geometry                &lt;POINT [m]&gt; POINT (29463.95 45634.94), POINT (15786.…\n$ region                  &lt;chr&gt; \"NORTH REGION\", \"WEST REGION\", \"NORTH-EAST REG…\n$ no_of_kindergarten_500m &lt;dbl&gt; 1, 1, 0, 1, 1, 6, 3, 1, 1, 1, 1, 0, 2, 4, 0, 0…\n$ prox_kindergarten       &lt;dbl&gt; 3.717727e+02, 1.241314e+02, 6.531547e+02, 4.47…\n$ no_of_childcare_500m    &lt;dbl&gt; 10, 9, 4, 3, 6, 11, 9, 9, 7, 6, 4, 1, 8, 10, 2…\n$ prox_childcare          &lt;dbl&gt; 6.318731e+01, 7.642944e+01, 8.264710e+01, 4.47…\n$ no_of_hawker_500m       &lt;dbl&gt; 1, 0, 0, 1, 2, 0, 0, 0, 2, 0, 0, 0, 1, 1, 0, 0…\n$ prox_hawker             &lt;dbl&gt; 478.4537, 840.4254, 660.6058, 332.1417, 354.77…\n$ no_of_busstop_500m      &lt;dbl&gt; 17, 17, 6, 11, 13, 17, 15, 8, 16, 11, 5, 6, 17…\n$ prox_busstop            &lt;dbl&gt; 174.00119, 80.37739, 70.48567, 161.93848, 280.…\n$ no_of_shoppingmall_1km  &lt;dbl&gt; 1, 1, 1, 2, 0, 3, 1, 2, 1, 2, 1, 0, 3, 4, 0, 1…\n$ prox_shoppingmall       &lt;dbl&gt; 704.70468, 905.82230, 735.18898, 565.82028, 10…\n$ prox_mrt                &lt;dbl&gt; 1259.97262, 1869.01210, 197.18773, 175.98753, …\n$ prox_prisch             &lt;dbl&gt; 271.15943, 1353.83517, 86.23193, 234.73109, 57…\n$ prox_cbd                &lt;dbl&gt; 15605.314, 14916.316, 12419.101, 2871.311, 103…"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#distribution",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#distribution",
    "title": "Take Home Exercise 3",
    "section": "3.1 Distribution",
    "text": "3.1 Distribution"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#section",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#section",
    "title": "Take Home Exercise 3",
    "section": "3.2",
    "text": "3.2"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#categorical-variables",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#categorical-variables",
    "title": "Take Home Exercise 3",
    "section": "4.1 Categorical Variables",
    "text": "4.1 Categorical Variables\n\n4.1.1 Housing Type\n\n# Create a summary of counts for each remaining lease range\ncount_data &lt;- rental_sf %&gt;%\n  group_by(flat_type) %&gt;%\n  summarise(count = n())\n\n# Create the bar plot with labels\nggplot(count_data, aes(x = flat_type, y = count)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") + \n  geom_text(aes(label = count), vjust = -0.5, size = 4) +  # Add labels on top of the bars\n  labs(title = \"Count of Flat Type\",\n       x = \"Flat Type\",\n       y = \"Count\") +\n  theme_minimal()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html",
    "title": "In Class Exercise 9",
    "section": "",
    "text": "pacman::p_load(spdep, sp, tmap, sf, ClustGeo, cluster, factoextra, NbClust, tidyverse, GGally)\n\n\nshan_sf &lt;- read_rds(\"data/rds/shan_sf.rds\") \nshan_ict &lt;- read_rds(\"data/rds/shan_ict.rds\")\nshan_sf_cluster &lt;- read_rds(\"data/rds/shan_sf_cluster.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#loading-the-r-packages",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#loading-the-r-packages",
    "title": "In Class Exercise 9",
    "section": "",
    "text": "pacman::p_load(spdep, sp, tmap, sf, ClustGeo, cluster, factoextra, NbClust, tidyverse, GGally)\n\n\nshan_sf &lt;- read_rds(\"data/rds/shan_sf.rds\") \nshan_ict &lt;- read_rds(\"data/rds/shan_ict.rds\")\nshan_sf_cluster &lt;- read_rds(\"data/rds/shan_sf_cluster.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#conventional-hieracrchical-clustering",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#conventional-hieracrchical-clustering",
    "title": "In Class Exercise 9",
    "section": "6.1 Conventional Hieracrchical Clustering",
    "text": "6.1 Conventional Hieracrchical Clustering\nIn R, many packages provide functions to calculate distance matrix. We will compute the proximity matrix by using dist() of R.\ndist() supports six distance proximity calculations, they are: euclidean, maximum, manhattan, canberra, binary and minkowski. The default is euclidean proximity matrix.\nThe code chunk below is used to compute the proximity matrix using euclidean method.\n\nproxmat &lt;- dist(shan_ict, method = \"euclidean\")\nhclust_ward &lt;- hclust(proxmat, method = \"ward.D\") \ngroups &lt;- as.factor(cutree(hclust_ward, k=6))\n\nThen, we append to the geospatial data with the following code\n\nshan_sf_cluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;% rename(CLUSTER=as.matrix.groups.) %&gt;% select(-c(3:4, 7:9)) %&gt;% rename(TS = TS.x)\n\nThe below chunk of code plots the dendrogram.\n\nplot(hclust_ward, cex = 0.6)\nrect.hclust(hclust_ward, k = 6, border = 2:5)\n\n\n\n\n\n\n\n\nAnd below here is the cluster map.\n\n\n\n\n\n\nTip\n\n\n\nqtm is useful for categorical data\n\n\n\nqtm(shan_sf_cluster, \"CLUSTER\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#spatially-constrained-clustering-skater-method",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#spatially-constrained-clustering-skater-method",
    "title": "In Class Exercise 9",
    "section": "6.2 Spatially Constrained Clustering: SKATER method",
    "text": "6.2 Spatially Constrained Clustering: SKATER method\n\n6.2.1 Step 1: Computing nearest neighbours\n\nshan.nb &lt;- poly2nb(shan_sf)\nsummary(shan.nb)\n\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\n\n\n\n\n6.2.2 Step 2: Visualising the neighbours\n\nplot(st_geometry(shan_sf), \n     border=grey(.5))\npts &lt;- st_coordinates(st_centroid(shan_sf))\nplot(shan.nb, \n     pts, \n     col=\"blue\", \n     add=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.2.3 Step 3: Computing minimum spanning tree\n\n6.2.3.1 Calculating edge costs\n\nlcosts &lt;- nbcosts(shan.nb, shan_ict)\n\n\n\n6.2.3.2 Incorporating these costs into a weights object\n\nshan.w &lt;- nb2listw(shan.nb, \n                   lcosts, \n                   style=\"B\")\nsummary(shan.w)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\nWeights style: B \nWeights constants summary:\n   n   nn       S0       S1        S2\nB 55 3025 76267.65 58260785 522016004\n\n\n\n\n6.2.3.3 Computing MST\n\nshan.mst &lt;- mstree(shan.w)\n\n\n\n6.2.3.4 Visualising MST\n\nplot(st_geometry(shan_sf), \n     border=gray(.5))\nplot.mst(shan.mst, \n         pts, \n         col=\"blue\", \n         cex.lab=0.7, \n         cex.circles=0.005, \n         add=TRUE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#computing-spatially-constrained-clusters-using-skater-method",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#computing-spatially-constrained-clusters-using-skater-method",
    "title": "In Class Exercise 9",
    "section": "6.3 Computing spatially constrained clusters using SKATER method",
    "text": "6.3 Computing spatially constrained clusters using SKATER method\n\nThe codeThe skater tree\n\n\n\nskater.clust6 &lt;- skater(edges = shan.mst[,1:2], \n                 data = shan_ict, \n                 method = \"euclidean\", \n                 ncuts = 5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe code to plot skater tree\n\nplot(st_geometry(shan_sf), \n     border=gray(.5))\nplot(skater.clust6, \n     pts, \n     cex.lab=.7,\n     groups.colors=c(\"red\",\"green\",\"blue\", \"brown\", \"pink\"),\n     cex.circles=0.005, \n     add=TRUE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#visualising-the-clusters-in-choropleth-map",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#visualising-the-clusters-in-choropleth-map",
    "title": "In Class Exercise 9",
    "section": "6.4 Visualising the clusters in choropleth map",
    "text": "6.4 Visualising the clusters in choropleth map\n\n6.4.1 The plot\n\n\n\n\n\n\n\n\n\n\n\n6.4.2 The code\n\ngroups_mat &lt;- as.matrix(skater.clust6$groups)\nshan_sf_spatialcluster &lt;- cbind(shan_sf_cluster, as.factor(groups_mat)) %&gt;%\n  rename(`skater_CLUSTER`=`as.factor.groups_mat.`)\nqtm(shan_sf_spatialcluster, \"skater_CLUSTER\")\n\n\n\n6.4.3 Plotting the cluster maps\n\n6.4.3.1 The Code\n\nhclust.map &lt;- qtm(shan_sf_cluster,\n                  \"CLUSTER\") + \n  tm_borders(alpha = 0.5) +\n  tm_layout(legend.position = c(0.8, 0.6))\n\nshclust.map &lt;- qtm(shan_sf_spatialcluster,\n                   \"skater_CLUSTER\") + \n  tm_borders(alpha = 0.5) +\n  tm_layout(legend.position = c(0.7, 0.6))\n\ntmap_arrange(hclust.map, shclust.map,\n             asp=NA, ncol=2)\n\n\n\n6.4.3.2 The Plot"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#spatially-constrained-clustering-clustgeo-method",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#spatially-constrained-clustering-clustgeo-method",
    "title": "In Class Exercise 9",
    "section": "6.5 Spatially Constrained Clustering: ClustGeo Method",
    "text": "6.5 Spatially Constrained Clustering: ClustGeo Method\n\n6.5.1 Computing spatial distance matrix\nIn the code chunk below, st_distance() of sf package is used to compute the distance matrix.\n\ndist &lt;- st_distance(shan_sf, shan_sf)\ndistmat &lt;- as.dist(dist)\n\n\n\n6.5.2 The cluster graphs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.5.3 The code\n\ncr &lt;- choicealpha(proxmat, distmat, \n                  range.alpha = seq(0, 1, 0.1), \n                  K=6, graph = TRUE)\n\n\n\n6.5.4 Saving clustGeo output\n\nclustG &lt;- hclustgeo(proxmat, distmat, alpha = 0.2)\ngroups &lt;- as.factor(cutree(clustG, k=6))\nshan_sf_clustGeo &lt;- cbind(shan_sf, \n                          as.matrix(groups)) %&gt;%\n  rename(`clustGeo` = `as.matrix.groups.`)\n\n\n\n6.5.5 Visualising the clustGeo map\n\nqtm(shan_sf_clustGeo, \"clustGeo\")\n\n\n\n\n\n\n\n\n\n\n6.5.6 Comparing cluster maps\n\n6.5.6.1 The code\n\nhclust.map &lt;- qtm(shan_sf_cluster,\n                  \"CLUSTER\") + \n  tm_borders(alpha = 0.5) +\n  tm_layout(legend.position = c(0.8, 0.6))\n\nshclust.map &lt;- qtm(shan_sf_spatialcluster,\n                   \"skater_CLUSTER\") + \n  tm_borders(alpha = 0.5) +\n  tm_layout(legend.position = c(0.7, 0.6))\n\nclustGeo.map &lt;- qtm(shan_sf_clustGeo,\n                   \"clustGeo\") + \n  tm_borders(alpha = 0.5) +\n  tm_layout(legend.position = c(0.7, 0.6))\n\ntmap_arrange(hclust.map, shclust.map, \n             clustGeo.map,\n             asp=NA, ncol=3)\n\n\n\n6.5.6.2 The plot"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#characterising-the-clusters",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#characterising-the-clusters",
    "title": "In Class Exercise 9",
    "section": "6.6 Characterising the clusters",
    "text": "6.6 Characterising the clusters\n\n6.6.1 The plot\n\n\n\n\n\n\n\n\n\n\n\n6.6.2 The code\n\nggparcoord(data = shan_sf_clustGeo, \n           columns = c(17:21), \n           scale = \"globalminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of ICT Variables by Cluster\") +\n  facet_grid(~ clustGeo) + \n  theme(axis.text.x = element_text(angle = 30))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#converting-to-spatialpolygondataframe",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#converting-to-spatialpolygondataframe",
    "title": "In Class Exercise 5",
    "section": "3.4 Converting to SpatialPolygonDataFrame",
    "text": "3.4 Converting to SpatialPolygonDataFrame\n\n\n\n\n\n\nNote\n\n\n\nGWmodel is built around the older sp and not sf formats for handling spatial data in R.\n\n\n\nhunan_sp &lt;- hunan_sf %&gt;% as_Spatial()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#determine-adaptive-bandwidth",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#determine-adaptive-bandwidth",
    "title": "In Class Exercise 5",
    "section": "4.1 Determine Adaptive Bandwidth",
    "text": "4.1 Determine Adaptive Bandwidth\n\n4.1.1 AIC\n\nbw_AIC &lt;- bw.gwr(GDPPC ~ 1, \n                 data = hunan_sp,\n                 approach = \"AIC\",\n                 adaptive = TRUE,\n                 kernel = \"bisquare\",\n                 longlat = T)\n\nAdaptive bandwidth (number of nearest neighbours): 62 AICc value: 1923.156 \nAdaptive bandwidth (number of nearest neighbours): 46 AICc value: 1920.469 \nAdaptive bandwidth (number of nearest neighbours): 36 AICc value: 1917.324 \nAdaptive bandwidth (number of nearest neighbours): 29 AICc value: 1916.661 \nAdaptive bandwidth (number of nearest neighbours): 26 AICc value: 1914.897 \nAdaptive bandwidth (number of nearest neighbours): 22 AICc value: 1914.045 \nAdaptive bandwidth (number of nearest neighbours): 22 AICc value: 1914.045 \n\n\nGood thing with GWmodel is that automatically determines the bandwidth for you\n\n\n\n\n\n\nNote\n\n\n\nUnit of measurement for bandwidth value shown here is in kilometres.\n\n\n\n\n4.1.2 Cross-validation\n\nbw_AIC &lt;- bw.gwr(GDPPC ~ 1, \n                 data = hunan_sp,\n                 approach = \"CV\",\n                 adaptive = TRUE,\n                 kernel = \"bisquare\",\n                 longlat = T)\n\nAdaptive bandwidth: 62 CV score: 15515442343 \nAdaptive bandwidth: 46 CV score: 14937956887 \nAdaptive bandwidth: 36 CV score: 14408561608 \nAdaptive bandwidth: 29 CV score: 14198527496 \nAdaptive bandwidth: 26 CV score: 13898800611 \nAdaptive bandwidth: 22 CV score: 13662299974 \nAdaptive bandwidth: 22 CV score: 13662299974 \n\n\nIdentical to AIC, same number of results generated."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#determine-fixed-bandwidth",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#determine-fixed-bandwidth",
    "title": "In Class Exercise 5",
    "section": "4.2 Determine Fixed Bandwidth",
    "text": "4.2 Determine Fixed Bandwidth\n\n4.2.1 AIC\n\nbw_AIC &lt;- bw.gwr(GDPPC ~ 1, \n                 data = hunan_sp,\n                 approach = \"AIC\",\n                 kernel = \"bisquare\",\n                 adaptive = FALSE,\n                 longlat = T)\n\nFixed bandwidth: 357.4897 AICc value: 1927.631 \nFixed bandwidth: 220.985 AICc value: 1921.547 \nFixed bandwidth: 136.6204 AICc value: 1919.993 \nFixed bandwidth: 84.48025 AICc value: 1940.603 \nFixed bandwidth: 168.8448 AICc value: 1919.457 \nFixed bandwidth: 188.7606 AICc value: 1920.007 \nFixed bandwidth: 156.5362 AICc value: 1919.41 \nFixed bandwidth: 148.929 AICc value: 1919.527 \nFixed bandwidth: 161.2377 AICc value: 1919.392 \nFixed bandwidth: 164.1433 AICc value: 1919.403 \nFixed bandwidth: 159.4419 AICc value: 1919.393 \nFixed bandwidth: 162.3475 AICc value: 1919.394 \nFixed bandwidth: 160.5517 AICc value: 1919.391 \n\n\n\n\n4.2.2 Cross Validation\n\nbw_AIC &lt;- bw.gwr(GDPPC ~ 1, \n                 data = hunan_sp,\n                 approach = \"CV\",\n                 kernel = \"bisquare\",\n                 adaptive = FALSE,\n                 longlat = T)\n\nFixed bandwidth: 357.4897 CV score: 16265191728 \nFixed bandwidth: 220.985 CV score: 14954930931 \nFixed bandwidth: 136.6204 CV score: 14134185837 \nFixed bandwidth: 84.48025 CV score: 13693362460 \nFixed bandwidth: 52.25585 CV score: Inf \nFixed bandwidth: 104.396 CV score: 13891052305 \nFixed bandwidth: 72.17162 CV score: 13577893677 \nFixed bandwidth: 64.56447 CV score: 14681160609 \nFixed bandwidth: 76.8731 CV score: 13444716890 \nFixed bandwidth: 79.77877 CV score: 13503296834 \nFixed bandwidth: 75.07729 CV score: 13452450771 \nFixed bandwidth: 77.98296 CV score: 13457916138 \nFixed bandwidth: 76.18716 CV score: 13442911302 \nFixed bandwidth: 75.76323 CV score: 13444600639 \nFixed bandwidth: 76.44916 CV score: 13442994078 \nFixed bandwidth: 76.02523 CV score: 13443285248 \nFixed bandwidth: 76.28724 CV score: 13442844774 \nFixed bandwidth: 76.34909 CV score: 13442864995 \nFixed bandwidth: 76.24901 CV score: 13442855596 \nFixed bandwidth: 76.31086 CV score: 13442847019 \nFixed bandwidth: 76.27264 CV score: 13442846793 \nFixed bandwidth: 76.29626 CV score: 13442844829 \nFixed bandwidth: 76.28166 CV score: 13442845238 \nFixed bandwidth: 76.29068 CV score: 13442844678 \nFixed bandwidth: 76.29281 CV score: 13442844691 \nFixed bandwidth: 76.28937 CV score: 13442844698 \nFixed bandwidth: 76.2915 CV score: 13442844676 \nFixed bandwidth: 76.292 CV score: 13442844679 \nFixed bandwidth: 76.29119 CV score: 13442844676 \nFixed bandwidth: 76.29099 CV score: 13442844676 \nFixed bandwidth: 76.29131 CV score: 13442844676 \nFixed bandwidth: 76.29138 CV score: 13442844676 \nFixed bandwidth: 76.29126 CV score: 13442844676 \nFixed bandwidth: 76.29123 CV score: 13442844676 \n\n\n\n\n\n\n\n\nTip\n\n\n\nThe bandwidth calculated here can be used to pass it over to the calculation (in next section). The number of"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#computing-geographically-weighted-summary-statistics",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#computing-geographically-weighted-summary-statistics",
    "title": "In Class Exercise 5",
    "section": "4.3 Computing Geographically Weighted Summary Statistics",
    "text": "4.3 Computing Geographically Weighted Summary Statistics\nSince we are using one variable for two chunks of code above (bw_AIC), need to make sure that the adaptive one is ran before this chunk of code is ran.\n\n\nAdaptive bandwidth: 62 CV score: 15515442343 \nAdaptive bandwidth: 46 CV score: 14937956887 \nAdaptive bandwidth: 36 CV score: 14408561608 \nAdaptive bandwidth: 29 CV score: 14198527496 \nAdaptive bandwidth: 26 CV score: 13898800611 \nAdaptive bandwidth: 22 CV score: 13662299974 \nAdaptive bandwidth: 22 CV score: 13662299974 \n\n\n\ngstat &lt;- gwss( data = hunan_sp,\n                vars = \"GDPPC\",\n                bw = bw_AIC,\n                kernel = \"bisquare\",\n                adaptive = TRUE,\n                longlat = T)\n\nHow to interpret the table of the data: GDPPC_LM –&gt; Average of all the neighbours\n\n4.3.2 Preparing the output data\nCode chunk below is used to extract SDF data table from gwss object output from gwss(). It will be converted into data.frame. It will be converted into data.frame by using as.data.frame().\n\n\n\n\n\n\nNote\n\n\n\nSort or order etc altering functions cannot be applied to the code below, it will mess with the sequence fo the\n\n\n\ngstat_df &lt;- as.data.frame(gstat$SDF)\n\nNext, cbind() is used to append the newly derived data.frame onto hunan_sf sf data.frame.\n\nhunan_gstat &lt;- cbind(hunan_sf, gstat_df)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#visualising-geographically-weighted-summary-statistics",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#visualising-geographically-weighted-summary-statistics",
    "title": "In Class Exercise 5",
    "section": "4.4 Visualising Geographically Weighted Summary Statistics",
    "text": "4.4 Visualising Geographically Weighted Summary Statistics\n\ntm_shape(hunan_gstat)+\n  tm_fill(\"GDPPC_LM\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of geographically weighted mean\",\n            main.title.position = \"center\",\n            main.title.size = 2.0,\n            legend.text.size = 1.2,\n            legend.height = 1.5,\n            legend.width = 1.5,\n            frame = TRUE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/data/MPSZ2019/MPSZ-2019.html",
    "href": "In-class_Ex/In-class_Ex02/data/MPSZ2019/MPSZ-2019.html",
    "title": "Georgia's IS415 Experience",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/data/MPSZ-2019.html",
    "href": "In-class_Ex/In-class_Ex01/data/MPSZ-2019.html",
    "title": "Georgia's IS415 Experience",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  }
]